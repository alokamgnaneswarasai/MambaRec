Arguments:
backbone sasmamba
batch_size 16
dataset ml-1m
device cuda:1
dropout_rate 0.5
eval_neg_sample 100
hidden_units 50
inference_only False
l2_emb 0.0
lr 0.0005
maxlen 260
name mamba
num_blocks 2
num_epochs 5
num_heads 1
state_dict_path None
train_dir tracks256_default/

user_count: 6040
item count: 3416
6040
average sequence length: 163.50
parameters_count: 240601
loss in epoch 1 iteration 376: 1.0138792991638184
loss in epoch 2 iteration 376: 0.8803838491439819
loss in epoch 3 iteration 376: 0.7718309164047241
loss in epoch 4 iteration 376: 0.7111097574234009
loss in epoch 5 iteration 376: 0.6888135075569153
Evaluatingepoch:5, time: 17449.634314(s), valid (NDCG@5: 0.3784, HR@5: 0.5346, NDCG@10: 0.4309, HR@10: 0.6972, NDCG@20: 0.4659, HR@20: 0.8344), test (NDCG@5: 0.3648, HR@5: 0.5167, NDCG@10: 0.4162, HR@10: 0.6745, NDCG@20: 0.4518, HR@20: 0.8154)
Training time : 17449.634313583374
Done
