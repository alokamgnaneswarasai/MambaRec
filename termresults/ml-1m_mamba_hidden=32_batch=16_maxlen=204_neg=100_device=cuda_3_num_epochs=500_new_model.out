Arguments:
backbone mamba
batch_size 16
dataset ml-1m
device cuda:3
dropout_rate 0.5
eval_neg_sample 100
hidden_units 32
inference_only False
l2_emb 0.0
lr 0.0005
maxlen 204
name mamba
num_blocks 2
num_epochs 500
num_heads 1
state_dict_path None
train_dir tracks256_default/

user_count: 6040
item count: 3416
6040
average sequence length: 163.50
parameters_count: 128864
loss in epoch 1 iteration 376: 0.9591059684753418
loss in epoch 2 iteration 376: 0.9791014194488525
