{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users:  6040\n",
      "Number of training samples:  42280\n",
      "torch.Size([32, 25])\n",
      "torch.Size([32, 25])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "def load_data(file_path):\n",
    "    user_dict = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            user, item = map(int, line.strip().split())\n",
    "            if user not in user_dict:\n",
    "                user_dict[user] = []\n",
    "            user_dict[user].append(item)\n",
    "    return user_dict\n",
    "\n",
    "class RecDataset(Dataset):\n",
    "    def __init__(self, user_dict, num_items, max_seq_length, segment_length):\n",
    "        self.data = []\n",
    "        for user, items in user_dict.items():\n",
    "            if len(items) < 2:\n",
    "                continue\n",
    "            \n",
    "            if len(items)<max_seq_length:\n",
    "                items = [0]*(max_seq_length-len(items)) + items\n",
    "                \n",
    "            \n",
    "            seq = items[-max_seq_length:]\n",
    "            for i in range(0, len(seq) - 1, segment_length):\n",
    "                segment = seq[i:i+segment_length]\n",
    "                target = seq[i+1:i+1+segment_length]\n",
    "                if len(segment) == segment_length and len(target) == segment_length:\n",
    "                    self.data.append((segment, target))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx][0], dtype=torch.long), torch.tensor(self.data[idx][1], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embed_dim = 64\n",
    "num_layers = 4\n",
    "num_heads = 8\n",
    "hidden_dim = 64\n",
    "mem_length = 100\n",
    "max_seq_length = 200\n",
    "segment_length = 25\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# Load data\n",
    "\n",
    "file_path = \"data/ml-1m.txt\"\n",
    "user_dict = load_data(file_path)\n",
    "num_items = max(max(items) for items in user_dict.values()) + 1\n",
    "print(\"Number of users: \", len(user_dict))\n",
    "\n",
    "train_dataset = RecDataset(user_dict, num_items, max_seq_length, segment_length)\n",
    "\n",
    "print(\"Number of training samples: \", len(train_dataset))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "for segment, target in train_loader:\n",
    "    print(segment.shape)\n",
    "    print(target.shape)\n",
    "    break\n",
    "    \n",
    "# if the last batch is smaller than batch_size, the last batch will be dropped \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---\n",
    "title: Relative Multi-Headed Attention\n",
    "summary: >\n",
    "  Documented implementation with explanations of\n",
    "  Relative Multi-Headed Attention from paper Transformer-XL.\n",
    "---\n",
    "\n",
    "# Relative Multi-Headed Attention\n",
    "\n",
    "This is an implementation of relative multi-headed attention from paper\n",
    "[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)\n",
    "in [PyTorch](https://pytorch.org).\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from labml.logger import inspect\n",
    "from labml_nn.transformers.mha import MultiHeadAttention\n",
    "\n",
    "\n",
    "from typing import List, Optional\n",
    "def shift_right(x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    This method shifts $i^{th}$ row of a matrix by $i$ columns.\n",
    "\n",
    "    If the input is `[[1, 2 ,3], [4, 5 ,6], [7, 8, 9]]`, the shifted\n",
    "    result would be `[[1, 2 ,3], [0, 4, 5], [6, 0, 7]]`.\n",
    "    *Ideally we should mask out the lower triangle but it's ok for our purpose*.\n",
    "    \"\"\"\n",
    "\n",
    "    # Concatenate a column of zeros\n",
    "    zero_pad = x.new_zeros(x.shape[0], 1, *x.shape[2:])\n",
    "    x_padded = torch.cat([x, zero_pad], dim=1)\n",
    "\n",
    "    # Reshape and remove excess elements from the end\n",
    "    x_padded = x_padded.view(x.shape[1] + 1, x.shape[0], *x.shape[2:])\n",
    "    x = x_padded[:-1].view_as(x)\n",
    "\n",
    "    #\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class RelativeMultiHeadAttention(MultiHeadAttention):\n",
    "    \"\"\"\n",
    "    ## Relative Multi-Head Attention Module\n",
    "\n",
    "    We override [Multi-Head Attention](mha.html) module so we only need to \n",
    "    write the `get_scores` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n",
    "        # The linear transformations do not need a bias since we\n",
    "        # explicitly include it when calculating scores.\n",
    "        # However having a bias for `value` might make sense.\n",
    "        # print(heads, d_model, dropout_prob)\n",
    "        super().__init__(heads, d_model, dropout_prob, bias=False)\n",
    "        \n",
    "        # Number of relative positions\n",
    "        self.P = 2 ** 12\n",
    "\n",
    "        # Relative positional embeddings for key relative to the query.\n",
    "        # We need $2P$ embeddings because the keys can be before or after the query.\n",
    "        self.key_pos_embeddings = nn.Parameter(torch.zeros((self.P * 2, heads, self.d_k)), requires_grad=True)\n",
    "        # Relative positional embedding bias for key relative to the query.\n",
    "        self.key_pos_bias = nn.Parameter(torch.zeros((self.P * 2, heads)), requires_grad=True)\n",
    "        # Positional embeddings for the query is independent of the position of the query\n",
    "        self.query_pos_bias = nn.Parameter(torch.zeros((heads, self.d_k)), requires_grad=True)\n",
    "\n",
    "    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
    "        r\"\"\"\n",
    "        ### Get relative attention scores\n",
    "\n",
    "        With absolute attention\n",
    "\n",
    "        \\begin{align}\n",
    "        A^{abs}_{j} &= lin_q(X^q_i + P_i)^\\top lin_k(X^k_j + P_j) \\\\\n",
    "                      &= \\underset{\\textcolor{lightgreen}{A}}{Q_i^\\top K_j} +\n",
    "                         \\underset{\\textcolor{lightgreen}{B}}{Q_i^\\top U^K_j} +\n",
    "                         \\underset{\\textcolor{lightgreen}{C}}{{U^Q_i}^\\top K_j} +\n",
    "                         \\underset{\\textcolor{lightgreen}{D}}{{U^Q_i}^\\top U^K_j}\n",
    "        \\end{align}\n",
    "\n",
    "        where $Q_i, K_j$, are linear transformations of\n",
    "         original embeddings $X^q_i, X^k_j$\n",
    "         and $U^Q_i, U^K_j$ are linear transformations of\n",
    "         absolute positional encodings $P_i, P_j$.\n",
    "\n",
    "        They reason out that the attention to a given key should be the same regardless of\n",
    "        the position of query.\n",
    "        Hence replace $\\underset{\\textcolor{lightgreen}{C}}{{U^Q_i}^\\top K_j}$\n",
    "        with a constant $\\underset{\\textcolor{lightgreen}{C}}{\\textcolor{orange}{v^\\top} K_j}$.\n",
    "\n",
    "        For the second and third terms relative positional encodings are introduced.\n",
    "        So $\\underset{\\textcolor{lightgreen}{B}}{Q_i^\\top U^K_j}$ is\n",
    "        replaced with $\\underset{\\textcolor{lightgreen}{B}}{Q_i^\\top \\textcolor{orange}{R_{i - j}}}$\n",
    "        and $\\underset{\\textcolor{lightgreen}{D}}{{U^Q_i}^\\top U^K_j}$\n",
    "        with $\\underset{\\textcolor{lightgreen}{D}}{\\textcolor{orange}{S_{i-j}}}$.\n",
    "\n",
    "        \\begin{align}\n",
    "        A^{rel}_{i,j} &= \\underset{\\mathbf{\\textcolor{lightgreen}{A}}}{Q_i^\\top K_j} +\n",
    "                         \\underset{\\mathbf{\\textcolor{lightgreen}{B}}}{Q_i^\\top \\textcolor{orange}{R_{i - j}}} +\n",
    "                         \\underset{\\mathbf{\\textcolor{lightgreen}{C}}}{\\textcolor{orange}{v^\\top} K_j} +\n",
    "                         \\underset{\\mathbf{\\textcolor{lightgreen}{D}}}{\\textcolor{orange}{S_{i-j}}}\n",
    "        \\end{align}\n",
    "        \"\"\"\n",
    "\n",
    "        # $\\textcolor{orange}{R_k}$\n",
    "        key_pos_emb = self.key_pos_embeddings[self.P - key.shape[0]:self.P + query.shape[0]]\n",
    "        # $\\textcolor{orange}{S_k}$\n",
    "        key_pos_bias = self.key_pos_bias[self.P - key.shape[0]:self.P + query.shape[0]]\n",
    "        # $\\textcolor{orange}{v^\\top}$\n",
    "        query_pos_bias = self.query_pos_bias[None, None, :, :]\n",
    "\n",
    "        # ${(\\textcolor{lightgreen}{\\mathbf{A + C}})}_{i,j} =\n",
    "        # Q_i^\\top K_j +\n",
    "        # \\textcolor{orange}{v^\\top} K_j$\n",
    "        ac = torch.einsum('ibhd,jbhd->ijbh', query + query_pos_bias, key)\n",
    "        # $\\textcolor{lightgreen}{\\mathbf{B'}_{i,k}} = Q_i^\\top \\textcolor{orange}{R_k}$\n",
    "        b = torch.einsum('ibhd,jhd->ijbh', query, key_pos_emb)\n",
    "        # $\\textcolor{lightgreen}{\\mathbf{D'}_{i,k}} = \\textcolor{orange}{S_k}$\n",
    "        d = key_pos_bias[None, :, None, :]\n",
    "        # Shift the rows of $\\textcolor{lightgreen}{\\mathbf{(B' + D')}_{i,k}}$\n",
    "        # to get $$\\textcolor{lightgreen}{\\mathbf{(B + D)}_{i,j} = \\mathbf{(B' + D')}_{i,i - j}}$$\n",
    "        bd = shift_right(b + d)\n",
    "        # Remove extra positions\n",
    "        bd = bd[:, -key.shape[0]:]\n",
    "\n",
    "        # Return the sum $$\n",
    "        # \\underset{\\mathbf{\\textcolor{lightgreen}{A}}}{Q_i^\\top K_j} +\n",
    "        # \\underset{\\mathbf{\\textcolor{lightgreen}{B}}}{Q_i^\\top \\textcolor{orange}{R_{i - j}}} +\n",
    "        # \\underset{\\mathbf{\\textcolor{lightgreen}{C}}}{\\textcolor{orange}{v^\\top} K_j} +\n",
    "        # \\underset{\\mathbf{\\textcolor{lightgreen}{D}}}{\\textcolor{orange}{S_{i-j}}}\n",
    "        # $$\n",
    "        return ac + bd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerXLLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, self_attn: RelativeMultiHeadAttention, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.size = d_model\n",
    "        self.self_attn = self_attn\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.norm_self_attn = nn.LayerNorm(d_model)\n",
    "        self.norm_linear = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mem: Optional[torch.Tensor], mask: torch.Tensor):\n",
    "        z = self.norm_self_attn(x)\n",
    "        if mem is not None:\n",
    "            mem = self.norm_self_attn(mem)\n",
    "            \n",
    "            m_z = torch.cat((mem, z), dim=0)\n",
    "        else:\n",
    "            m_z = z\n",
    "        self_attn = self.self_attn(query=z, key=m_z, value=m_z, mask=mask)\n",
    "        x = x + self.dropout(self_attn)\n",
    "        z = self.norm_linear(x)\n",
    "        linear_out = self.linear(z)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        return x\n",
    "\n",
    "class TransformerXL(nn.Module):\n",
    "    def __init__(self, layer: TransformerXLLayer, n_layers: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([layer for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mem: List[torch.Tensor], mask: torch.Tensor):\n",
    "        new_mem = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            new_mem.append(x.detach())\n",
    "            m = mem[i] if mem else None\n",
    "            x = layer(x=x, mem=m, mask=mask)\n",
    "        return self.norm(x), new_mem\n",
    "\n",
    "class TransformerXLEncoder(nn.Module):\n",
    "    def __init__(self, num_items, embed_dim, num_layers, num_heads, hidden_dim, mem_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_items, embed_dim)\n",
    "        self.mem_length = mem_length\n",
    "        # print(embed_dim)\n",
    "        self.transformer = TransformerXL(\n",
    "            TransformerXLLayer(\n",
    "                d_model=embed_dim,\n",
    "                self_attn=RelativeMultiHeadAttention(num_heads,embed_dim, dropout),\n",
    "                dropout_prob=dropout\n",
    "            ),\n",
    "            n_layers=num_layers\n",
    "        )\n",
    "        self.linear = nn.Linear(embed_dim, num_items)\n",
    "    \n",
    "    def forward(self, x, memory=None):\n",
    "        x = self.embedding(x)  # Shape: (B, S, D)\n",
    "        x = x.permute(1, 0, 2)  # Shape: (S, B, D)\n",
    "        mask = None  # Define mask if needed\n",
    "        output, new_memory = self.transformer(x, memory, mask)\n",
    "        logits = self.linear(output)  # Shape: (S, B, num_items)\n",
    "        return logits.permute(1, 2, 0), new_memory  # Shape: (B, num_items, S)\n",
    "    \n",
    "    \n",
    "#example usage\n",
    "# x = torch.randint(0, num_items, (batch_size, segment_length), dtype=torch.long).to(device)\n",
    "# model = TransformerXLEncoder(num_items, embed_dim, num_layers, num_heads, hidden_dim, mem_length).to(device)\n",
    "# logits, memory = model(x)\n",
    "# print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, device, num_epochs=10):\n",
    "    model.train()\n",
    "    memory = None  # Initialize memory\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, memory = model(inputs, memory)  # Pass both sequence & memory\n",
    "            \n",
    "            # print(logits.shape, targets.shape)\n",
    "            loss = criterion(logits, targets)  # Shape: (B, num_items, S)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, user_dict, num_items, max_seq_length, segment_length, device):\n",
    "    model.eval()\n",
    "    NDCG, HR, valid_users = 0.0, 0.0, 0\n",
    "    \n",
    "    for user, items in user_dict.items():\n",
    "        if len(items) < 2:\n",
    "            continue\n",
    "        \n",
    "        seq = items[-max_seq_length:]\n",
    "        input_seq = torch.tensor(seq[:-1], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        target = seq[-1]\n",
    "        candidates = [target] + random.sample(set(range(1, num_items)) - set(items), 99)\n",
    "        \n",
    "        memory = None\n",
    "        for i in range(0, len(input_seq[0]), segment_length):\n",
    "            segment = input_seq[:, i:i+segment_length]\n",
    "            logits, memory = model(segment, memory)\n",
    "            \n",
    "        # logits shape is (1, num_items, segment_length)\n",
    "        scores = logits[0, :, -1]\n",
    "        scores = scores[candidates]\n",
    "        # print(scores)\n",
    "        ranked = torch.argsort(scores, descending=True).cpu().numpy()\n",
    "        rank = np.where(ranked == 0)[0][0] + 1\n",
    "        # print(rank)\n",
    "        valid_users += 1\n",
    "        \n",
    "        \n",
    "        HR += int(rank <= 10)\n",
    "        NDCG += 1 / np.log2(rank + 1) if rank <= 10 else 0\n",
    "        \n",
    "        if valid_users % 10 == 0:\n",
    "            print(f\"Validated users: {valid_users}, HR@10: {HR / valid_users:.4f}, NDCG@10: {NDCG / valid_users:.4f}\")\n",
    "    print(f\"HR@10: {HR / valid_users:.4f}, NDCG@10: {NDCG / valid_users:.4f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerXLEncoder(\n",
      "  (embedding): Embedding(3417, 64)\n",
      "  (transformer): TransformerXL(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerXLLayer(\n",
      "        (self_attn): RelativeMultiHeadAttention(\n",
      "          (query): PrepareForMultiHeadAttention(\n",
      "            (linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "          )\n",
      "          (key): PrepareForMultiHeadAttention(\n",
      "            (linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "          )\n",
      "          (value): PrepareForMultiHeadAttention(\n",
      "            (linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (softmax): Softmax(dim=1)\n",
      "          (output): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (linear): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (norm_self_attn): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_linear): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (linear): Linear(in_features=64, out_features=3417, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = TransformerXLEncoder(num_items, embed_dim, num_layers, num_heads, hidden_dim, mem_length).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(model)\n",
    "num_epochs=100\n",
    "train_model(model, train_loader, optimizer, criterion, device, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated users: 10, HR@10: 0.1000, NDCG@10: 0.0631\n",
      "Validated users: 20, HR@10: 0.4500, NDCG@10: 0.3122\n",
      "Validated users: 30, HR@10: 0.5000, NDCG@10: 0.3479\n",
      "Validated users: 40, HR@10: 0.5500, NDCG@10: 0.3638\n",
      "Validated users: 50, HR@10: 0.5400, NDCG@10: 0.3911\n",
      "Validated users: 60, HR@10: 0.5167, NDCG@10: 0.3831\n",
      "Validated users: 70, HR@10: 0.5143, NDCG@10: 0.3676\n",
      "Validated users: 80, HR@10: 0.5125, NDCG@10: 0.3646\n",
      "Validated users: 90, HR@10: 0.4889, NDCG@10: 0.3432\n",
      "Validated users: 100, HR@10: 0.4900, NDCG@10: 0.3453\n",
      "Validated users: 110, HR@10: 0.5000, NDCG@10: 0.3371\n",
      "Validated users: 120, HR@10: 0.5167, NDCG@10: 0.3320\n",
      "Validated users: 130, HR@10: 0.5077, NDCG@10: 0.3245\n",
      "Validated users: 140, HR@10: 0.5000, NDCG@10: 0.3229\n",
      "Validated users: 150, HR@10: 0.5000, NDCG@10: 0.3309\n",
      "Validated users: 160, HR@10: 0.5062, NDCG@10: 0.3393\n",
      "Validated users: 170, HR@10: 0.4941, NDCG@10: 0.3319\n",
      "Validated users: 180, HR@10: 0.4833, NDCG@10: 0.3265\n",
      "Validated users: 190, HR@10: 0.4895, NDCG@10: 0.3314\n",
      "Validated users: 200, HR@10: 0.4800, NDCG@10: 0.3222\n",
      "Validated users: 210, HR@10: 0.4857, NDCG@10: 0.3237\n",
      "Validated users: 220, HR@10: 0.4864, NDCG@10: 0.3205\n",
      "Validated users: 230, HR@10: 0.4783, NDCG@10: 0.3140\n",
      "Validated users: 240, HR@10: 0.4792, NDCG@10: 0.3132\n",
      "Validated users: 250, HR@10: 0.4880, NDCG@10: 0.3165\n",
      "Validated users: 260, HR@10: 0.4808, NDCG@10: 0.3103\n",
      "Validated users: 270, HR@10: 0.4852, NDCG@10: 0.3098\n",
      "Validated users: 280, HR@10: 0.4893, NDCG@10: 0.3138\n",
      "Validated users: 290, HR@10: 0.4931, NDCG@10: 0.3139\n",
      "Validated users: 300, HR@10: 0.4867, NDCG@10: 0.3092\n",
      "Validated users: 310, HR@10: 0.4935, NDCG@10: 0.3079\n",
      "Validated users: 320, HR@10: 0.5000, NDCG@10: 0.3118\n",
      "Validated users: 330, HR@10: 0.4970, NDCG@10: 0.3099\n",
      "Validated users: 340, HR@10: 0.5000, NDCG@10: 0.3135\n",
      "Validated users: 350, HR@10: 0.4971, NDCG@10: 0.3082\n",
      "Validated users: 360, HR@10: 0.5056, NDCG@10: 0.3135\n",
      "Validated users: 370, HR@10: 0.5000, NDCG@10: 0.3113\n",
      "Validated users: 380, HR@10: 0.4947, NDCG@10: 0.3066\n",
      "Validated users: 390, HR@10: 0.4974, NDCG@10: 0.3124\n",
      "Validated users: 400, HR@10: 0.4950, NDCG@10: 0.3107\n",
      "Validated users: 410, HR@10: 0.4951, NDCG@10: 0.3108\n",
      "Validated users: 420, HR@10: 0.4976, NDCG@10: 0.3107\n",
      "Validated users: 430, HR@10: 0.4953, NDCG@10: 0.3103\n",
      "Validated users: 440, HR@10: 0.4977, NDCG@10: 0.3138\n",
      "Validated users: 450, HR@10: 0.4933, NDCG@10: 0.3116\n",
      "Validated users: 460, HR@10: 0.4870, NDCG@10: 0.3072\n",
      "Validated users: 470, HR@10: 0.4872, NDCG@10: 0.3060\n",
      "Validated users: 480, HR@10: 0.4854, NDCG@10: 0.3024\n",
      "Validated users: 490, HR@10: 0.4898, NDCG@10: 0.3049\n",
      "Validated users: 500, HR@10: 0.4900, NDCG@10: 0.3078\n",
      "Validated users: 510, HR@10: 0.4902, NDCG@10: 0.3092\n",
      "Validated users: 520, HR@10: 0.4865, NDCG@10: 0.3057\n",
      "Validated users: 530, HR@10: 0.4792, NDCG@10: 0.3018\n",
      "Validated users: 540, HR@10: 0.4778, NDCG@10: 0.3027\n",
      "Validated users: 550, HR@10: 0.4745, NDCG@10: 0.3020\n",
      "Validated users: 560, HR@10: 0.4786, NDCG@10: 0.3064\n",
      "Validated users: 570, HR@10: 0.4754, NDCG@10: 0.3027\n",
      "Validated users: 580, HR@10: 0.4724, NDCG@10: 0.2999\n",
      "Validated users: 590, HR@10: 0.4712, NDCG@10: 0.2988\n",
      "Validated users: 600, HR@10: 0.4717, NDCG@10: 0.2975\n",
      "Validated users: 610, HR@10: 0.4721, NDCG@10: 0.2991\n",
      "Validated users: 620, HR@10: 0.4710, NDCG@10: 0.2977\n",
      "Validated users: 630, HR@10: 0.4698, NDCG@10: 0.2955\n",
      "Validated users: 640, HR@10: 0.4703, NDCG@10: 0.2959\n",
      "Validated users: 650, HR@10: 0.4708, NDCG@10: 0.2964\n",
      "Validated users: 660, HR@10: 0.4697, NDCG@10: 0.2965\n",
      "Validated users: 670, HR@10: 0.4701, NDCG@10: 0.2967\n",
      "Validated users: 680, HR@10: 0.4647, NDCG@10: 0.2930\n",
      "Validated users: 690, HR@10: 0.4652, NDCG@10: 0.2938\n",
      "Validated users: 700, HR@10: 0.4671, NDCG@10: 0.2939\n",
      "Validated users: 710, HR@10: 0.4676, NDCG@10: 0.2958\n",
      "Validated users: 720, HR@10: 0.4653, NDCG@10: 0.2934\n",
      "Validated users: 730, HR@10: 0.4658, NDCG@10: 0.2923\n",
      "Validated users: 740, HR@10: 0.4662, NDCG@10: 0.2938\n",
      "Validated users: 750, HR@10: 0.4680, NDCG@10: 0.2934\n",
      "Validated users: 760, HR@10: 0.4711, NDCG@10: 0.2948\n",
      "Validated users: 770, HR@10: 0.4714, NDCG@10: 0.2950\n",
      "Validated users: 780, HR@10: 0.4731, NDCG@10: 0.2963\n",
      "Validated users: 790, HR@10: 0.4722, NDCG@10: 0.2954\n",
      "Validated users: 800, HR@10: 0.4688, NDCG@10: 0.2938\n",
      "Validated users: 810, HR@10: 0.4679, NDCG@10: 0.2918\n",
      "Validated users: 820, HR@10: 0.4683, NDCG@10: 0.2924\n",
      "Validated users: 830, HR@10: 0.4699, NDCG@10: 0.2946\n",
      "Validated users: 840, HR@10: 0.4702, NDCG@10: 0.2964\n",
      "Validated users: 850, HR@10: 0.4718, NDCG@10: 0.2981\n",
      "Validated users: 860, HR@10: 0.4709, NDCG@10: 0.2985\n",
      "Validated users: 870, HR@10: 0.4713, NDCG@10: 0.2981\n",
      "Validated users: 880, HR@10: 0.4682, NDCG@10: 0.2964\n",
      "Validated users: 890, HR@10: 0.4719, NDCG@10: 0.2979\n",
      "Validated users: 900, HR@10: 0.4744, NDCG@10: 0.2995\n",
      "Validated users: 910, HR@10: 0.4758, NDCG@10: 0.2999\n",
      "Validated users: 920, HR@10: 0.4772, NDCG@10: 0.3008\n",
      "Validated users: 930, HR@10: 0.4806, NDCG@10: 0.3028\n",
      "Validated users: 940, HR@10: 0.4830, NDCG@10: 0.3041\n",
      "Validated users: 950, HR@10: 0.4853, NDCG@10: 0.3063\n",
      "Validated users: 960, HR@10: 0.4865, NDCG@10: 0.3060\n",
      "Validated users: 970, HR@10: 0.4856, NDCG@10: 0.3052\n",
      "Validated users: 980, HR@10: 0.4857, NDCG@10: 0.3055\n",
      "Validated users: 990, HR@10: 0.4869, NDCG@10: 0.3058\n",
      "Validated users: 1000, HR@10: 0.4860, NDCG@10: 0.3054\n",
      "Validated users: 1010, HR@10: 0.4871, NDCG@10: 0.3056\n",
      "Validated users: 1020, HR@10: 0.4863, NDCG@10: 0.3052\n",
      "Validated users: 1030, HR@10: 0.4845, NDCG@10: 0.3047\n",
      "Validated users: 1040, HR@10: 0.4865, NDCG@10: 0.3063\n",
      "Validated users: 1050, HR@10: 0.4829, NDCG@10: 0.3043\n",
      "Validated users: 1060, HR@10: 0.4849, NDCG@10: 0.3055\n",
      "Validated users: 1070, HR@10: 0.4850, NDCG@10: 0.3057\n",
      "Validated users: 1080, HR@10: 0.4843, NDCG@10: 0.3054\n",
      "Validated users: 1090, HR@10: 0.4844, NDCG@10: 0.3051\n",
      "Validated users: 1100, HR@10: 0.4864, NDCG@10: 0.3062\n",
      "Validated users: 1110, HR@10: 0.4883, NDCG@10: 0.3070\n",
      "Validated users: 1120, HR@10: 0.4884, NDCG@10: 0.3076\n",
      "Validated users: 1130, HR@10: 0.4903, NDCG@10: 0.3087\n",
      "Validated users: 1140, HR@10: 0.4904, NDCG@10: 0.3079\n",
      "Validated users: 1150, HR@10: 0.4904, NDCG@10: 0.3080\n",
      "Validated users: 1160, HR@10: 0.4922, NDCG@10: 0.3096\n",
      "Validated users: 1170, HR@10: 0.4940, NDCG@10: 0.3113\n",
      "Validated users: 1180, HR@10: 0.4932, NDCG@10: 0.3102\n",
      "Validated users: 1190, HR@10: 0.4941, NDCG@10: 0.3103\n",
      "Validated users: 1200, HR@10: 0.4958, NDCG@10: 0.3111\n",
      "Validated users: 1210, HR@10: 0.4959, NDCG@10: 0.3118\n",
      "Validated users: 1220, HR@10: 0.4951, NDCG@10: 0.3111\n",
      "Validated users: 1230, HR@10: 0.4935, NDCG@10: 0.3099\n",
      "Validated users: 1240, HR@10: 0.4927, NDCG@10: 0.3094\n",
      "Validated users: 1250, HR@10: 0.4928, NDCG@10: 0.3090\n",
      "Validated users: 1260, HR@10: 0.4937, NDCG@10: 0.3093\n",
      "Validated users: 1270, HR@10: 0.4929, NDCG@10: 0.3092\n",
      "Validated users: 1280, HR@10: 0.4938, NDCG@10: 0.3102\n",
      "Validated users: 1290, HR@10: 0.4953, NDCG@10: 0.3109\n",
      "Validated users: 1300, HR@10: 0.4969, NDCG@10: 0.3118\n",
      "Validated users: 1310, HR@10: 0.4977, NDCG@10: 0.3122\n",
      "Validated users: 1320, HR@10: 0.4970, NDCG@10: 0.3118\n",
      "Validated users: 1330, HR@10: 0.4970, NDCG@10: 0.3111\n",
      "Validated users: 1340, HR@10: 0.4948, NDCG@10: 0.3099\n",
      "Validated users: 1350, HR@10: 0.4948, NDCG@10: 0.3103\n",
      "Validated users: 1360, HR@10: 0.4963, NDCG@10: 0.3113\n",
      "Validated users: 1370, HR@10: 0.4964, NDCG@10: 0.3114\n",
      "Validated users: 1380, HR@10: 0.4978, NDCG@10: 0.3130\n",
      "Validated users: 1390, HR@10: 0.4964, NDCG@10: 0.3116\n",
      "Validated users: 1400, HR@10: 0.4936, NDCG@10: 0.3096\n",
      "Validated users: 1410, HR@10: 0.4915, NDCG@10: 0.3079\n",
      "Validated users: 1420, HR@10: 0.4930, NDCG@10: 0.3096\n",
      "Validated users: 1430, HR@10: 0.4944, NDCG@10: 0.3109\n",
      "Validated users: 1440, HR@10: 0.4965, NDCG@10: 0.3118\n",
      "Validated users: 1450, HR@10: 0.4952, NDCG@10: 0.3104\n",
      "Validated users: 1460, HR@10: 0.4932, NDCG@10: 0.3092\n",
      "Validated users: 1470, HR@10: 0.4952, NDCG@10: 0.3108\n",
      "Validated users: 1480, HR@10: 0.4939, NDCG@10: 0.3096\n",
      "Validated users: 1490, HR@10: 0.4933, NDCG@10: 0.3087\n",
      "Validated users: 1500, HR@10: 0.4920, NDCG@10: 0.3081\n",
      "Validated users: 1510, HR@10: 0.4927, NDCG@10: 0.3078\n",
      "Validated users: 1520, HR@10: 0.4947, NDCG@10: 0.3084\n",
      "Validated users: 1530, HR@10: 0.4941, NDCG@10: 0.3080\n",
      "Validated users: 1540, HR@10: 0.4942, NDCG@10: 0.3079\n",
      "Validated users: 1550, HR@10: 0.4948, NDCG@10: 0.3079\n",
      "Validated users: 1560, HR@10: 0.4942, NDCG@10: 0.3075\n",
      "Validated users: 1570, HR@10: 0.4943, NDCG@10: 0.3073\n",
      "Validated users: 1580, HR@10: 0.4937, NDCG@10: 0.3065\n",
      "Validated users: 1590, HR@10: 0.4943, NDCG@10: 0.3065\n",
      "Validated users: 1600, HR@10: 0.4950, NDCG@10: 0.3074\n",
      "Validated users: 1610, HR@10: 0.4950, NDCG@10: 0.3079\n",
      "Validated users: 1620, HR@10: 0.4944, NDCG@10: 0.3077\n",
      "Validated users: 1630, HR@10: 0.4951, NDCG@10: 0.3082\n",
      "Validated users: 1640, HR@10: 0.4951, NDCG@10: 0.3085\n",
      "Validated users: 1650, HR@10: 0.4952, NDCG@10: 0.3082\n",
      "Validated users: 1660, HR@10: 0.4958, NDCG@10: 0.3094\n",
      "Validated users: 1670, HR@10: 0.4958, NDCG@10: 0.3096\n",
      "Validated users: 1680, HR@10: 0.4970, NDCG@10: 0.3101\n",
      "Validated users: 1690, HR@10: 0.4959, NDCG@10: 0.3097\n",
      "Validated users: 1700, HR@10: 0.4971, NDCG@10: 0.3097\n",
      "Validated users: 1710, HR@10: 0.4965, NDCG@10: 0.3092\n",
      "Validated users: 1720, HR@10: 0.4965, NDCG@10: 0.3093\n",
      "Validated users: 1730, HR@10: 0.4971, NDCG@10: 0.3093\n",
      "Validated users: 1740, HR@10: 0.4977, NDCG@10: 0.3100\n",
      "Validated users: 1750, HR@10: 0.4983, NDCG@10: 0.3105\n",
      "Validated users: 1760, HR@10: 0.4994, NDCG@10: 0.3110\n",
      "Validated users: 1770, HR@10: 0.4989, NDCG@10: 0.3107\n",
      "Validated users: 1780, HR@10: 0.4983, NDCG@10: 0.3101\n",
      "Validated users: 1790, HR@10: 0.4978, NDCG@10: 0.3099\n",
      "Validated users: 1800, HR@10: 0.4972, NDCG@10: 0.3091\n",
      "Validated users: 1810, HR@10: 0.4967, NDCG@10: 0.3089\n",
      "Validated users: 1820, HR@10: 0.4978, NDCG@10: 0.3096\n",
      "Validated users: 1830, HR@10: 0.4978, NDCG@10: 0.3095\n",
      "Validated users: 1840, HR@10: 0.4978, NDCG@10: 0.3095\n",
      "Validated users: 1850, HR@10: 0.4978, NDCG@10: 0.3089\n",
      "Validated users: 1860, HR@10: 0.4973, NDCG@10: 0.3092\n",
      "Validated users: 1870, HR@10: 0.4963, NDCG@10: 0.3088\n",
      "Validated users: 1880, HR@10: 0.4952, NDCG@10: 0.3079\n",
      "Validated users: 1890, HR@10: 0.4963, NDCG@10: 0.3080\n",
      "Validated users: 1900, HR@10: 0.4968, NDCG@10: 0.3090\n",
      "Validated users: 1910, HR@10: 0.4974, NDCG@10: 0.3097\n",
      "Validated users: 1920, HR@10: 0.4969, NDCG@10: 0.3099\n",
      "Validated users: 1930, HR@10: 0.4964, NDCG@10: 0.3093\n",
      "Validated users: 1940, HR@10: 0.4954, NDCG@10: 0.3085\n",
      "Validated users: 1950, HR@10: 0.4944, NDCG@10: 0.3079\n",
      "Validated users: 1960, HR@10: 0.4949, NDCG@10: 0.3076\n",
      "Validated users: 1970, HR@10: 0.4959, NDCG@10: 0.3087\n",
      "Validated users: 1980, HR@10: 0.4970, NDCG@10: 0.3091\n",
      "Validated users: 1990, HR@10: 0.4965, NDCG@10: 0.3090\n",
      "Validated users: 2000, HR@10: 0.4980, NDCG@10: 0.3099\n",
      "Validated users: 2010, HR@10: 0.4975, NDCG@10: 0.3094\n",
      "Validated users: 2020, HR@10: 0.4965, NDCG@10: 0.3088\n",
      "Validated users: 2030, HR@10: 0.4970, NDCG@10: 0.3089\n",
      "Validated users: 2040, HR@10: 0.4951, NDCG@10: 0.3079\n",
      "Validated users: 2050, HR@10: 0.4951, NDCG@10: 0.3079\n",
      "Validated users: 2060, HR@10: 0.4937, NDCG@10: 0.3072\n",
      "Validated users: 2070, HR@10: 0.4952, NDCG@10: 0.3077\n",
      "Validated users: 2080, HR@10: 0.4952, NDCG@10: 0.3072\n",
      "Validated users: 2090, HR@10: 0.4952, NDCG@10: 0.3076\n",
      "Validated users: 2100, HR@10: 0.4962, NDCG@10: 0.3085\n",
      "Validated users: 2110, HR@10: 0.4967, NDCG@10: 0.3086\n",
      "Validated users: 2120, HR@10: 0.4972, NDCG@10: 0.3089\n",
      "Validated users: 2130, HR@10: 0.4972, NDCG@10: 0.3088\n",
      "Validated users: 2140, HR@10: 0.4977, NDCG@10: 0.3085\n",
      "Validated users: 2150, HR@10: 0.4977, NDCG@10: 0.3086\n",
      "Validated users: 2160, HR@10: 0.4977, NDCG@10: 0.3085\n",
      "Validated users: 2170, HR@10: 0.4982, NDCG@10: 0.3092\n",
      "Validated users: 2180, HR@10: 0.4982, NDCG@10: 0.3087\n",
      "Validated users: 2190, HR@10: 0.4991, NDCG@10: 0.3090\n",
      "Validated users: 2200, HR@10: 0.4995, NDCG@10: 0.3095\n",
      "Validated users: 2210, HR@10: 0.5005, NDCG@10: 0.3097\n",
      "Validated users: 2220, HR@10: 0.5009, NDCG@10: 0.3100\n",
      "Validated users: 2230, HR@10: 0.5013, NDCG@10: 0.3103\n",
      "Validated users: 2240, HR@10: 0.5027, NDCG@10: 0.3113\n",
      "Validated users: 2250, HR@10: 0.5036, NDCG@10: 0.3112\n",
      "Validated users: 2260, HR@10: 0.5031, NDCG@10: 0.3110\n",
      "Validated users: 2270, HR@10: 0.5022, NDCG@10: 0.3101\n",
      "Validated users: 2280, HR@10: 0.5026, NDCG@10: 0.3106\n",
      "Validated users: 2290, HR@10: 0.5026, NDCG@10: 0.3108\n",
      "Validated users: 2300, HR@10: 0.5048, NDCG@10: 0.3126\n",
      "Validated users: 2310, HR@10: 0.5052, NDCG@10: 0.3129\n",
      "Validated users: 2320, HR@10: 0.5052, NDCG@10: 0.3130\n",
      "Validated users: 2330, HR@10: 0.5039, NDCG@10: 0.3121\n",
      "Validated users: 2340, HR@10: 0.5030, NDCG@10: 0.3115\n",
      "Validated users: 2350, HR@10: 0.5038, NDCG@10: 0.3129\n",
      "Validated users: 2360, HR@10: 0.5038, NDCG@10: 0.3128\n",
      "Validated users: 2370, HR@10: 0.5025, NDCG@10: 0.3119\n",
      "Validated users: 2380, HR@10: 0.5021, NDCG@10: 0.3118\n",
      "Validated users: 2390, HR@10: 0.5025, NDCG@10: 0.3121\n",
      "Validated users: 2400, HR@10: 0.5021, NDCG@10: 0.3116\n",
      "Validated users: 2410, HR@10: 0.5017, NDCG@10: 0.3112\n",
      "Validated users: 2420, HR@10: 0.5017, NDCG@10: 0.3111\n",
      "Validated users: 2430, HR@10: 0.5016, NDCG@10: 0.3112\n",
      "Validated users: 2440, HR@10: 0.5008, NDCG@10: 0.3105\n",
      "Validated users: 2450, HR@10: 0.5000, NDCG@10: 0.3100\n",
      "Validated users: 2460, HR@10: 0.5004, NDCG@10: 0.3105\n",
      "Validated users: 2470, HR@10: 0.5012, NDCG@10: 0.3109\n",
      "Validated users: 2480, HR@10: 0.5008, NDCG@10: 0.3106\n",
      "Validated users: 2490, HR@10: 0.5020, NDCG@10: 0.3115\n",
      "Validated users: 2500, HR@10: 0.5012, NDCG@10: 0.3111\n",
      "Validated users: 2510, HR@10: 0.5020, NDCG@10: 0.3111\n",
      "Validated users: 2520, HR@10: 0.5024, NDCG@10: 0.3112\n",
      "Validated users: 2530, HR@10: 0.5028, NDCG@10: 0.3111\n",
      "Validated users: 2540, HR@10: 0.5035, NDCG@10: 0.3120\n",
      "Validated users: 2550, HR@10: 0.5047, NDCG@10: 0.3126\n",
      "Validated users: 2560, HR@10: 0.5035, NDCG@10: 0.3119\n",
      "Validated users: 2570, HR@10: 0.5035, NDCG@10: 0.3119\n",
      "Validated users: 2580, HR@10: 0.5023, NDCG@10: 0.3111\n",
      "Validated users: 2590, HR@10: 0.5019, NDCG@10: 0.3105\n",
      "Validated users: 2600, HR@10: 0.5015, NDCG@10: 0.3106\n",
      "Validated users: 2610, HR@10: 0.5011, NDCG@10: 0.3101\n",
      "Validated users: 2620, HR@10: 0.5015, NDCG@10: 0.3104\n",
      "Validated users: 2630, HR@10: 0.5023, NDCG@10: 0.3107\n",
      "Validated users: 2640, HR@10: 0.5019, NDCG@10: 0.3108\n",
      "Validated users: 2650, HR@10: 0.5023, NDCG@10: 0.3112\n",
      "Validated users: 2660, HR@10: 0.5023, NDCG@10: 0.3118\n",
      "Validated users: 2670, HR@10: 0.5015, NDCG@10: 0.3115\n",
      "Validated users: 2680, HR@10: 0.5011, NDCG@10: 0.3112\n",
      "Validated users: 2690, HR@10: 0.4996, NDCG@10: 0.3104\n",
      "Validated users: 2700, HR@10: 0.5004, NDCG@10: 0.3110\n",
      "Validated users: 2710, HR@10: 0.5000, NDCG@10: 0.3103\n",
      "Validated users: 2720, HR@10: 0.4993, NDCG@10: 0.3099\n",
      "Validated users: 2730, HR@10: 0.4989, NDCG@10: 0.3098\n",
      "Validated users: 2740, HR@10: 0.4989, NDCG@10: 0.3101\n",
      "Validated users: 2750, HR@10: 0.4985, NDCG@10: 0.3100\n",
      "Validated users: 2760, HR@10: 0.4978, NDCG@10: 0.3095\n",
      "Validated users: 2770, HR@10: 0.4971, NDCG@10: 0.3095\n",
      "Validated users: 2780, HR@10: 0.4968, NDCG@10: 0.3094\n",
      "Validated users: 2790, HR@10: 0.4961, NDCG@10: 0.3088\n",
      "Validated users: 2800, HR@10: 0.4957, NDCG@10: 0.3086\n",
      "Validated users: 2810, HR@10: 0.4957, NDCG@10: 0.3085\n",
      "Validated users: 2820, HR@10: 0.4957, NDCG@10: 0.3081\n",
      "Validated users: 2830, HR@10: 0.4958, NDCG@10: 0.3080\n",
      "Validated users: 2840, HR@10: 0.4947, NDCG@10: 0.3075\n",
      "Validated users: 2850, HR@10: 0.4947, NDCG@10: 0.3073\n",
      "Validated users: 2860, HR@10: 0.4941, NDCG@10: 0.3066\n",
      "Validated users: 2870, HR@10: 0.4941, NDCG@10: 0.3064\n",
      "Validated users: 2880, HR@10: 0.4934, NDCG@10: 0.3057\n",
      "Validated users: 2890, HR@10: 0.4931, NDCG@10: 0.3055\n",
      "Validated users: 2900, HR@10: 0.4931, NDCG@10: 0.3054\n",
      "Validated users: 2910, HR@10: 0.4931, NDCG@10: 0.3053\n",
      "Validated users: 2920, HR@10: 0.4942, NDCG@10: 0.3061\n",
      "Validated users: 2930, HR@10: 0.4945, NDCG@10: 0.3065\n",
      "Validated users: 2940, HR@10: 0.4946, NDCG@10: 0.3064\n",
      "Validated users: 2950, HR@10: 0.4936, NDCG@10: 0.3058\n",
      "Validated users: 2960, HR@10: 0.4932, NDCG@10: 0.3057\n",
      "Validated users: 2970, HR@10: 0.4933, NDCG@10: 0.3058\n",
      "Validated users: 2980, HR@10: 0.4933, NDCG@10: 0.3058\n",
      "Validated users: 2990, HR@10: 0.4933, NDCG@10: 0.3062\n",
      "Validated users: 3000, HR@10: 0.4933, NDCG@10: 0.3063\n",
      "Validated users: 3010, HR@10: 0.4930, NDCG@10: 0.3058\n",
      "Validated users: 3020, HR@10: 0.4927, NDCG@10: 0.3059\n",
      "Validated users: 3030, HR@10: 0.4924, NDCG@10: 0.3056\n",
      "Validated users: 3040, HR@10: 0.4924, NDCG@10: 0.3057\n",
      "Validated users: 3050, HR@10: 0.4925, NDCG@10: 0.3060\n",
      "Validated users: 3060, HR@10: 0.4922, NDCG@10: 0.3061\n",
      "Validated users: 3070, HR@10: 0.4919, NDCG@10: 0.3056\n",
      "Validated users: 3080, HR@10: 0.4925, NDCG@10: 0.3057\n",
      "Validated users: 3090, HR@10: 0.4926, NDCG@10: 0.3055\n",
      "Validated users: 3100, HR@10: 0.4919, NDCG@10: 0.3052\n",
      "Validated users: 3110, HR@10: 0.4910, NDCG@10: 0.3046\n",
      "Validated users: 3120, HR@10: 0.4907, NDCG@10: 0.3047\n",
      "Validated users: 3130, HR@10: 0.4914, NDCG@10: 0.3048\n",
      "Validated users: 3140, HR@10: 0.4917, NDCG@10: 0.3048\n",
      "Validated users: 3150, HR@10: 0.4914, NDCG@10: 0.3047\n",
      "Validated users: 3160, HR@10: 0.4911, NDCG@10: 0.3045\n",
      "Validated users: 3170, HR@10: 0.4905, NDCG@10: 0.3041\n",
      "Validated users: 3180, HR@10: 0.4909, NDCG@10: 0.3045\n",
      "Validated users: 3190, HR@10: 0.4900, NDCG@10: 0.3037\n",
      "Validated users: 3200, HR@10: 0.4900, NDCG@10: 0.3039\n",
      "Validated users: 3210, HR@10: 0.4903, NDCG@10: 0.3041\n",
      "Validated users: 3220, HR@10: 0.4898, NDCG@10: 0.3035\n",
      "Validated users: 3230, HR@10: 0.4898, NDCG@10: 0.3035\n",
      "Validated users: 3240, HR@10: 0.4895, NDCG@10: 0.3032\n",
      "Validated users: 3250, HR@10: 0.4895, NDCG@10: 0.3033\n",
      "Validated users: 3260, HR@10: 0.4899, NDCG@10: 0.3040\n",
      "Validated users: 3270, HR@10: 0.4899, NDCG@10: 0.3040\n",
      "Validated users: 3280, HR@10: 0.4896, NDCG@10: 0.3038\n",
      "Validated users: 3290, HR@10: 0.4894, NDCG@10: 0.3040\n",
      "Validated users: 3300, HR@10: 0.4891, NDCG@10: 0.3041\n",
      "Validated users: 3310, HR@10: 0.4897, NDCG@10: 0.3045\n",
      "Validated users: 3320, HR@10: 0.4898, NDCG@10: 0.3043\n",
      "Validated users: 3330, HR@10: 0.4901, NDCG@10: 0.3045\n",
      "Validated users: 3340, HR@10: 0.4904, NDCG@10: 0.3046\n",
      "Validated users: 3350, HR@10: 0.4901, NDCG@10: 0.3045\n",
      "Validated users: 3360, HR@10: 0.4908, NDCG@10: 0.3053\n",
      "Validated users: 3370, HR@10: 0.4914, NDCG@10: 0.3055\n",
      "Validated users: 3380, HR@10: 0.4908, NDCG@10: 0.3052\n",
      "Validated users: 3390, HR@10: 0.4909, NDCG@10: 0.3054\n",
      "Validated users: 3400, HR@10: 0.4918, NDCG@10: 0.3060\n",
      "Validated users: 3410, HR@10: 0.4915, NDCG@10: 0.3059\n",
      "Validated users: 3420, HR@10: 0.4915, NDCG@10: 0.3061\n",
      "Validated users: 3430, HR@10: 0.4913, NDCG@10: 0.3060\n",
      "Validated users: 3440, HR@10: 0.4910, NDCG@10: 0.3060\n",
      "Validated users: 3450, HR@10: 0.4913, NDCG@10: 0.3060\n",
      "Validated users: 3460, HR@10: 0.4910, NDCG@10: 0.3058\n",
      "Validated users: 3470, HR@10: 0.4911, NDCG@10: 0.3058\n",
      "Validated users: 3480, HR@10: 0.4911, NDCG@10: 0.3061\n",
      "Validated users: 3490, HR@10: 0.4908, NDCG@10: 0.3063\n",
      "Validated users: 3500, HR@10: 0.4917, NDCG@10: 0.3068\n",
      "Validated users: 3510, HR@10: 0.4923, NDCG@10: 0.3070\n",
      "Validated users: 3520, HR@10: 0.4926, NDCG@10: 0.3075\n",
      "Validated users: 3530, HR@10: 0.4932, NDCG@10: 0.3080\n",
      "Validated users: 3540, HR@10: 0.4929, NDCG@10: 0.3076\n",
      "Validated users: 3550, HR@10: 0.4924, NDCG@10: 0.3071\n",
      "Validated users: 3560, HR@10: 0.4924, NDCG@10: 0.3073\n",
      "Validated users: 3570, HR@10: 0.4927, NDCG@10: 0.3075\n",
      "Validated users: 3580, HR@10: 0.4925, NDCG@10: 0.3070\n",
      "Validated users: 3590, HR@10: 0.4933, NDCG@10: 0.3077\n",
      "Validated users: 3600, HR@10: 0.4931, NDCG@10: 0.3075\n",
      "Validated users: 3610, HR@10: 0.4920, NDCG@10: 0.3068\n",
      "Validated users: 3620, HR@10: 0.4923, NDCG@10: 0.3069\n",
      "Validated users: 3630, HR@10: 0.4917, NDCG@10: 0.3068\n",
      "Validated users: 3640, HR@10: 0.4915, NDCG@10: 0.3065\n",
      "Validated users: 3650, HR@10: 0.4915, NDCG@10: 0.3064\n",
      "Validated users: 3660, HR@10: 0.4910, NDCG@10: 0.3061\n",
      "Validated users: 3670, HR@10: 0.4910, NDCG@10: 0.3061\n",
      "Validated users: 3680, HR@10: 0.4908, NDCG@10: 0.3059\n",
      "Validated users: 3690, HR@10: 0.4908, NDCG@10: 0.3058\n",
      "Validated users: 3700, HR@10: 0.4908, NDCG@10: 0.3057\n",
      "Validated users: 3710, HR@10: 0.4906, NDCG@10: 0.3054\n",
      "Validated users: 3720, HR@10: 0.4909, NDCG@10: 0.3058\n",
      "Validated users: 3730, HR@10: 0.4903, NDCG@10: 0.3053\n",
      "Validated users: 3740, HR@10: 0.4906, NDCG@10: 0.3058\n",
      "Validated users: 3750, HR@10: 0.4909, NDCG@10: 0.3059\n",
      "Validated users: 3760, HR@10: 0.4904, NDCG@10: 0.3056\n",
      "Validated users: 3770, HR@10: 0.4907, NDCG@10: 0.3062\n",
      "Validated users: 3780, HR@10: 0.4899, NDCG@10: 0.3058\n",
      "Validated users: 3790, HR@10: 0.4905, NDCG@10: 0.3062\n",
      "Validated users: 3800, HR@10: 0.4905, NDCG@10: 0.3059\n",
      "Validated users: 3810, HR@10: 0.4906, NDCG@10: 0.3056\n",
      "Validated users: 3820, HR@10: 0.4903, NDCG@10: 0.3055\n",
      "Validated users: 3830, HR@10: 0.4898, NDCG@10: 0.3052\n",
      "Validated users: 3840, HR@10: 0.4901, NDCG@10: 0.3052\n",
      "Validated users: 3850, HR@10: 0.4899, NDCG@10: 0.3050\n",
      "Validated users: 3860, HR@10: 0.4904, NDCG@10: 0.3053\n",
      "Validated users: 3870, HR@10: 0.4904, NDCG@10: 0.3054\n",
      "Validated users: 3880, HR@10: 0.4907, NDCG@10: 0.3055\n",
      "Validated users: 3890, HR@10: 0.4913, NDCG@10: 0.3056\n",
      "Validated users: 3900, HR@10: 0.4923, NDCG@10: 0.3060\n",
      "Validated users: 3910, HR@10: 0.4921, NDCG@10: 0.3061\n",
      "Validated users: 3920, HR@10: 0.4921, NDCG@10: 0.3061\n",
      "Validated users: 3930, HR@10: 0.4919, NDCG@10: 0.3062\n",
      "Validated users: 3940, HR@10: 0.4914, NDCG@10: 0.3058\n",
      "Validated users: 3950, HR@10: 0.4911, NDCG@10: 0.3055\n",
      "Validated users: 3960, HR@10: 0.4914, NDCG@10: 0.3057\n",
      "Validated users: 3970, HR@10: 0.4914, NDCG@10: 0.3056\n",
      "Validated users: 3980, HR@10: 0.4912, NDCG@10: 0.3056\n",
      "Validated users: 3990, HR@10: 0.4907, NDCG@10: 0.3053\n",
      "Validated users: 4000, HR@10: 0.4910, NDCG@10: 0.3057\n",
      "Validated users: 4010, HR@10: 0.4910, NDCG@10: 0.3061\n",
      "Validated users: 4020, HR@10: 0.4915, NDCG@10: 0.3065\n",
      "Validated users: 4030, HR@10: 0.4916, NDCG@10: 0.3067\n",
      "Validated users: 4040, HR@10: 0.4911, NDCG@10: 0.3066\n",
      "Validated users: 4050, HR@10: 0.4909, NDCG@10: 0.3063\n",
      "Validated users: 4060, HR@10: 0.4899, NDCG@10: 0.3057\n",
      "Validated users: 4070, HR@10: 0.4897, NDCG@10: 0.3054\n",
      "Validated users: 4080, HR@10: 0.4897, NDCG@10: 0.3055\n",
      "Validated users: 4090, HR@10: 0.4897, NDCG@10: 0.3054\n",
      "Validated users: 4100, HR@10: 0.4895, NDCG@10: 0.3055\n",
      "Validated users: 4110, HR@10: 0.4903, NDCG@10: 0.3059\n",
      "Validated users: 4120, HR@10: 0.4905, NDCG@10: 0.3064\n",
      "Validated users: 4130, HR@10: 0.4903, NDCG@10: 0.3062\n",
      "Validated users: 4140, HR@10: 0.4899, NDCG@10: 0.3058\n",
      "Validated users: 4150, HR@10: 0.4899, NDCG@10: 0.3061\n",
      "Validated users: 4160, HR@10: 0.4904, NDCG@10: 0.3064\n",
      "Validated users: 4170, HR@10: 0.4906, NDCG@10: 0.3063\n",
      "Validated users: 4180, HR@10: 0.4909, NDCG@10: 0.3065\n",
      "Validated users: 4190, HR@10: 0.4905, NDCG@10: 0.3062\n",
      "Validated users: 4200, HR@10: 0.4905, NDCG@10: 0.3064\n",
      "Validated users: 4210, HR@10: 0.4907, NDCG@10: 0.3069\n",
      "Validated users: 4220, HR@10: 0.4905, NDCG@10: 0.3066\n",
      "Validated users: 4230, HR@10: 0.4903, NDCG@10: 0.3065\n",
      "Validated users: 4240, HR@10: 0.4901, NDCG@10: 0.3065\n",
      "Validated users: 4250, HR@10: 0.4911, NDCG@10: 0.3069\n",
      "Validated users: 4260, HR@10: 0.4915, NDCG@10: 0.3075\n",
      "Validated users: 4270, HR@10: 0.4918, NDCG@10: 0.3072\n",
      "Validated users: 4280, HR@10: 0.4923, NDCG@10: 0.3073\n",
      "Validated users: 4290, HR@10: 0.4923, NDCG@10: 0.3072\n",
      "Validated users: 4300, HR@10: 0.4919, NDCG@10: 0.3069\n",
      "Validated users: 4310, HR@10: 0.4916, NDCG@10: 0.3069\n",
      "Validated users: 4320, HR@10: 0.4919, NDCG@10: 0.3067\n",
      "Validated users: 4330, HR@10: 0.4917, NDCG@10: 0.3066\n",
      "Validated users: 4340, HR@10: 0.4917, NDCG@10: 0.3067\n",
      "Validated users: 4350, HR@10: 0.4920, NDCG@10: 0.3069\n",
      "Validated users: 4360, HR@10: 0.4924, NDCG@10: 0.3070\n",
      "Validated users: 4370, HR@10: 0.4929, NDCG@10: 0.3074\n",
      "Validated users: 4380, HR@10: 0.4927, NDCG@10: 0.3073\n",
      "Validated users: 4390, HR@10: 0.4929, NDCG@10: 0.3073\n",
      "Validated users: 4400, HR@10: 0.4932, NDCG@10: 0.3072\n",
      "Validated users: 4410, HR@10: 0.4932, NDCG@10: 0.3073\n",
      "Validated users: 4420, HR@10: 0.4925, NDCG@10: 0.3068\n",
      "Validated users: 4430, HR@10: 0.4932, NDCG@10: 0.3072\n",
      "Validated users: 4440, HR@10: 0.4932, NDCG@10: 0.3073\n",
      "Validated users: 4450, HR@10: 0.4933, NDCG@10: 0.3072\n",
      "Validated users: 4460, HR@10: 0.4935, NDCG@10: 0.3073\n",
      "Validated users: 4470, HR@10: 0.4940, NDCG@10: 0.3076\n",
      "Validated users: 4480, HR@10: 0.4944, NDCG@10: 0.3078\n",
      "Validated users: 4490, HR@10: 0.4947, NDCG@10: 0.3080\n",
      "Validated users: 4500, HR@10: 0.4942, NDCG@10: 0.3078\n",
      "Validated users: 4510, HR@10: 0.4936, NDCG@10: 0.3076\n",
      "Validated users: 4520, HR@10: 0.4936, NDCG@10: 0.3077\n",
      "Validated users: 4530, HR@10: 0.4934, NDCG@10: 0.3077\n",
      "Validated users: 4540, HR@10: 0.4936, NDCG@10: 0.3079\n",
      "Validated users: 4550, HR@10: 0.4938, NDCG@10: 0.3081\n",
      "Validated users: 4560, HR@10: 0.4936, NDCG@10: 0.3079\n",
      "Validated users: 4570, HR@10: 0.4932, NDCG@10: 0.3076\n",
      "Validated users: 4580, HR@10: 0.4932, NDCG@10: 0.3075\n",
      "Validated users: 4590, HR@10: 0.4935, NDCG@10: 0.3077\n",
      "Validated users: 4600, HR@10: 0.4937, NDCG@10: 0.3080\n",
      "Validated users: 4610, HR@10: 0.4931, NDCG@10: 0.3076\n",
      "Validated users: 4620, HR@10: 0.4931, NDCG@10: 0.3075\n",
      "Validated users: 4630, HR@10: 0.4929, NDCG@10: 0.3074\n",
      "Validated users: 4640, HR@10: 0.4933, NDCG@10: 0.3076\n",
      "Validated users: 4650, HR@10: 0.4931, NDCG@10: 0.3074\n",
      "Validated users: 4660, HR@10: 0.4933, NDCG@10: 0.3074\n",
      "Validated users: 4670, HR@10: 0.4929, NDCG@10: 0.3070\n",
      "Validated users: 4680, HR@10: 0.4927, NDCG@10: 0.3069\n",
      "Validated users: 4690, HR@10: 0.4925, NDCG@10: 0.3070\n",
      "Validated users: 4700, HR@10: 0.4923, NDCG@10: 0.3069\n",
      "Validated users: 4710, HR@10: 0.4919, NDCG@10: 0.3067\n",
      "Validated users: 4720, HR@10: 0.4917, NDCG@10: 0.3066\n",
      "Validated users: 4730, HR@10: 0.4924, NDCG@10: 0.3068\n",
      "Validated users: 4740, HR@10: 0.4924, NDCG@10: 0.3067\n",
      "Validated users: 4750, HR@10: 0.4924, NDCG@10: 0.3066\n",
      "Validated users: 4760, HR@10: 0.4922, NDCG@10: 0.3066\n",
      "Validated users: 4770, HR@10: 0.4922, NDCG@10: 0.3066\n",
      "Validated users: 4780, HR@10: 0.4925, NDCG@10: 0.3065\n",
      "Validated users: 4790, HR@10: 0.4923, NDCG@10: 0.3063\n",
      "Validated users: 4800, HR@10: 0.4925, NDCG@10: 0.3064\n",
      "Validated users: 4810, HR@10: 0.4921, NDCG@10: 0.3063\n",
      "Validated users: 4820, HR@10: 0.4919, NDCG@10: 0.3059\n",
      "Validated users: 4830, HR@10: 0.4921, NDCG@10: 0.3060\n",
      "Validated users: 4840, HR@10: 0.4924, NDCG@10: 0.3063\n",
      "Validated users: 4850, HR@10: 0.4922, NDCG@10: 0.3063\n",
      "Validated users: 4860, HR@10: 0.4920, NDCG@10: 0.3062\n",
      "Validated users: 4870, HR@10: 0.4914, NDCG@10: 0.3058\n",
      "Validated users: 4880, HR@10: 0.4912, NDCG@10: 0.3058\n",
      "Validated users: 4890, HR@10: 0.4914, NDCG@10: 0.3061\n",
      "Validated users: 4900, HR@10: 0.4912, NDCG@10: 0.3060\n",
      "Validated users: 4910, HR@10: 0.4914, NDCG@10: 0.3062\n",
      "Validated users: 4920, HR@10: 0.4917, NDCG@10: 0.3064\n",
      "Validated users: 4930, HR@10: 0.4919, NDCG@10: 0.3065\n",
      "Validated users: 4940, HR@10: 0.4919, NDCG@10: 0.3067\n",
      "Validated users: 4950, HR@10: 0.4921, NDCG@10: 0.3070\n",
      "Validated users: 4960, HR@10: 0.4919, NDCG@10: 0.3070\n",
      "Validated users: 4970, HR@10: 0.4915, NDCG@10: 0.3066\n",
      "Validated users: 4980, HR@10: 0.4916, NDCG@10: 0.3064\n",
      "Validated users: 4990, HR@10: 0.4920, NDCG@10: 0.3064\n",
      "Validated users: 5000, HR@10: 0.4920, NDCG@10: 0.3064\n",
      "Validated users: 5010, HR@10: 0.4920, NDCG@10: 0.3062\n",
      "Validated users: 5020, HR@10: 0.4918, NDCG@10: 0.3060\n",
      "Validated users: 5030, HR@10: 0.4918, NDCG@10: 0.3060\n",
      "Validated users: 5040, HR@10: 0.4919, NDCG@10: 0.3058\n",
      "Validated users: 5050, HR@10: 0.4925, NDCG@10: 0.3059\n",
      "Validated users: 5060, HR@10: 0.4921, NDCG@10: 0.3056\n",
      "Validated users: 5070, HR@10: 0.4921, NDCG@10: 0.3056\n",
      "Validated users: 5080, HR@10: 0.4921, NDCG@10: 0.3057\n",
      "Validated users: 5090, HR@10: 0.4917, NDCG@10: 0.3055\n",
      "Validated users: 5100, HR@10: 0.4918, NDCG@10: 0.3058\n",
      "Validated users: 5110, HR@10: 0.4912, NDCG@10: 0.3054\n",
      "Validated users: 5120, HR@10: 0.4910, NDCG@10: 0.3055\n",
      "Validated users: 5130, HR@10: 0.4908, NDCG@10: 0.3052\n",
      "Validated users: 5140, HR@10: 0.4909, NDCG@10: 0.3054\n",
      "Validated users: 5150, HR@10: 0.4905, NDCG@10: 0.3052\n",
      "Validated users: 5160, HR@10: 0.4905, NDCG@10: 0.3051\n",
      "Validated users: 5170, HR@10: 0.4903, NDCG@10: 0.3049\n",
      "Validated users: 5180, HR@10: 0.4903, NDCG@10: 0.3048\n",
      "Validated users: 5190, HR@10: 0.4904, NDCG@10: 0.3046\n",
      "Validated users: 5200, HR@10: 0.4910, NDCG@10: 0.3049\n",
      "Validated users: 5210, HR@10: 0.4906, NDCG@10: 0.3049\n",
      "Validated users: 5220, HR@10: 0.4912, NDCG@10: 0.3052\n",
      "Validated users: 5230, HR@10: 0.4910, NDCG@10: 0.3051\n",
      "Validated users: 5240, HR@10: 0.4908, NDCG@10: 0.3050\n",
      "Validated users: 5250, HR@10: 0.4905, NDCG@10: 0.3048\n",
      "Validated users: 5260, HR@10: 0.4905, NDCG@10: 0.3048\n",
      "Validated users: 5270, HR@10: 0.4903, NDCG@10: 0.3046\n",
      "Validated users: 5280, HR@10: 0.4907, NDCG@10: 0.3050\n",
      "Validated users: 5290, HR@10: 0.4907, NDCG@10: 0.3050\n",
      "Validated users: 5300, HR@10: 0.4911, NDCG@10: 0.3052\n",
      "Validated users: 5310, HR@10: 0.4915, NDCG@10: 0.3053\n",
      "Validated users: 5320, HR@10: 0.4910, NDCG@10: 0.3049\n",
      "Validated users: 5330, HR@10: 0.4910, NDCG@10: 0.3049\n",
      "Validated users: 5340, HR@10: 0.4906, NDCG@10: 0.3047\n",
      "Validated users: 5350, HR@10: 0.4907, NDCG@10: 0.3048\n",
      "Validated users: 5360, HR@10: 0.4907, NDCG@10: 0.3047\n",
      "Validated users: 5370, HR@10: 0.4909, NDCG@10: 0.3047\n",
      "Validated users: 5380, HR@10: 0.4913, NDCG@10: 0.3050\n",
      "Validated users: 5390, HR@10: 0.4915, NDCG@10: 0.3050\n",
      "Validated users: 5400, HR@10: 0.4917, NDCG@10: 0.3052\n",
      "Validated users: 5410, HR@10: 0.4917, NDCG@10: 0.3052\n",
      "Validated users: 5420, HR@10: 0.4919, NDCG@10: 0.3055\n",
      "Validated users: 5430, HR@10: 0.4924, NDCG@10: 0.3059\n",
      "Validated users: 5440, HR@10: 0.4928, NDCG@10: 0.3063\n",
      "Validated users: 5450, HR@10: 0.4932, NDCG@10: 0.3068\n",
      "Validated users: 5460, HR@10: 0.4927, NDCG@10: 0.3063\n",
      "Validated users: 5470, HR@10: 0.4931, NDCG@10: 0.3067\n",
      "Validated users: 5480, HR@10: 0.4929, NDCG@10: 0.3065\n",
      "Validated users: 5490, HR@10: 0.4929, NDCG@10: 0.3064\n",
      "Validated users: 5500, HR@10: 0.4929, NDCG@10: 0.3063\n",
      "Validated users: 5510, HR@10: 0.4927, NDCG@10: 0.3064\n",
      "Validated users: 5520, HR@10: 0.4929, NDCG@10: 0.3064\n",
      "Validated users: 5530, HR@10: 0.4928, NDCG@10: 0.3063\n",
      "Validated users: 5540, HR@10: 0.4930, NDCG@10: 0.3061\n",
      "Validated users: 5550, HR@10: 0.4933, NDCG@10: 0.3061\n",
      "Validated users: 5560, HR@10: 0.4933, NDCG@10: 0.3062\n",
      "Validated users: 5570, HR@10: 0.4935, NDCG@10: 0.3064\n",
      "Validated users: 5580, HR@10: 0.4935, NDCG@10: 0.3064\n",
      "Validated users: 5590, HR@10: 0.4937, NDCG@10: 0.3070\n",
      "Validated users: 5600, HR@10: 0.4936, NDCG@10: 0.3068\n",
      "Validated users: 5610, HR@10: 0.4938, NDCG@10: 0.3070\n",
      "Validated users: 5620, HR@10: 0.4941, NDCG@10: 0.3074\n",
      "Validated users: 5630, HR@10: 0.4941, NDCG@10: 0.3078\n",
      "Validated users: 5640, HR@10: 0.4940, NDCG@10: 0.3078\n",
      "Validated users: 5650, HR@10: 0.4938, NDCG@10: 0.3077\n",
      "Validated users: 5660, HR@10: 0.4943, NDCG@10: 0.3081\n",
      "Validated users: 5670, HR@10: 0.4945, NDCG@10: 0.3080\n",
      "Validated users: 5680, HR@10: 0.4944, NDCG@10: 0.3078\n",
      "Validated users: 5690, HR@10: 0.4947, NDCG@10: 0.3082\n",
      "Validated users: 5700, HR@10: 0.4946, NDCG@10: 0.3082\n",
      "Validated users: 5710, HR@10: 0.4942, NDCG@10: 0.3082\n",
      "Validated users: 5720, HR@10: 0.4939, NDCG@10: 0.3079\n",
      "Validated users: 5730, HR@10: 0.4942, NDCG@10: 0.3084\n",
      "Validated users: 5740, HR@10: 0.4941, NDCG@10: 0.3082\n",
      "Validated users: 5750, HR@10: 0.4941, NDCG@10: 0.3081\n",
      "Validated users: 5760, HR@10: 0.4941, NDCG@10: 0.3080\n",
      "Validated users: 5770, HR@10: 0.4939, NDCG@10: 0.3080\n",
      "Validated users: 5780, HR@10: 0.4939, NDCG@10: 0.3080\n",
      "Validated users: 5790, HR@10: 0.4940, NDCG@10: 0.3080\n",
      "Validated users: 5800, HR@10: 0.4941, NDCG@10: 0.3083\n",
      "Validated users: 5810, HR@10: 0.4945, NDCG@10: 0.3084\n",
      "Validated users: 5820, HR@10: 0.4948, NDCG@10: 0.3086\n",
      "Validated users: 5830, HR@10: 0.4949, NDCG@10: 0.3086\n",
      "Validated users: 5840, HR@10: 0.4949, NDCG@10: 0.3087\n",
      "Validated users: 5850, HR@10: 0.4944, NDCG@10: 0.3083\n",
      "Validated users: 5860, HR@10: 0.4947, NDCG@10: 0.3084\n",
      "Validated users: 5870, HR@10: 0.4942, NDCG@10: 0.3080\n",
      "Validated users: 5880, HR@10: 0.4937, NDCG@10: 0.3077\n",
      "Validated users: 5890, HR@10: 0.4934, NDCG@10: 0.3074\n",
      "Validated users: 5900, HR@10: 0.4931, NDCG@10: 0.3071\n",
      "Validated users: 5910, HR@10: 0.4932, NDCG@10: 0.3074\n",
      "Validated users: 5920, HR@10: 0.4932, NDCG@10: 0.3072\n",
      "Validated users: 5930, HR@10: 0.4933, NDCG@10: 0.3072\n",
      "Validated users: 5940, HR@10: 0.4936, NDCG@10: 0.3075\n",
      "Validated users: 5950, HR@10: 0.4938, NDCG@10: 0.3076\n",
      "Validated users: 5960, HR@10: 0.4943, NDCG@10: 0.3080\n",
      "Validated users: 5970, HR@10: 0.4941, NDCG@10: 0.3079\n",
      "Validated users: 5980, HR@10: 0.4941, NDCG@10: 0.3078\n",
      "Validated users: 5990, HR@10: 0.4942, NDCG@10: 0.3078\n",
      "Validated users: 6000, HR@10: 0.4943, NDCG@10: 0.3078\n",
      "Validated users: 6010, HR@10: 0.4942, NDCG@10: 0.3076\n",
      "Validated users: 6020, HR@10: 0.4942, NDCG@10: 0.3074\n",
      "Validated users: 6030, HR@10: 0.4949, NDCG@10: 0.3080\n",
      "Validated users: 6040, HR@10: 0.4944, NDCG@10: 0.3076\n",
      "HR@10: 0.4944, NDCG@10: 0.3076\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, user_dict, num_items, max_seq_length, segment_length, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
