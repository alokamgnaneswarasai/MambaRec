{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "def load_data(file_path):\n",
    "    user_dict = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            user, item = map(int, line.strip().split())\n",
    "            if user not in user_dict:\n",
    "                user_dict[user] = []\n",
    "            user_dict[user].append(item)\n",
    "    return user_dict\n",
    "\n",
    "class RecDataset(Dataset):\n",
    "    def __init__(self, user_dict, num_items, max_seq_length, segment_length):\n",
    "        self.data = []\n",
    "        for user, items in user_dict.items():\n",
    "            # items = items[:-1] #remove the last item as it can be used during evaluation time\n",
    "            if len(items) < 2:\n",
    "                continue\n",
    "            \n",
    "            if len(items)<max_seq_length:\n",
    "                items = [0]*(max_seq_length+1-len(items)) + items\n",
    "                \n",
    "            \n",
    "            seq = items[:max_seq_length+1]  #first max_seq_length items\n",
    "            for i in range(0, len(seq) - 1, segment_length):\n",
    "                segment = seq[i:i+segment_length]\n",
    "                target = seq[i+1:i+1+segment_length]\n",
    "                if len(segment) == segment_length and len(target) == segment_length:\n",
    "                    self.data.append((segment, target))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx][0], dtype=torch.long), torch.tensor(self.data[idx][1], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embed_dim = 32\n",
    "num_layers = 1\n",
    "num_heads = 8\n",
    "hidden_dim = 32\n",
    "mem_length = 100\n",
    "max_seq_length = 200\n",
    "segment_length = 100\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# Load data\n",
    "\n",
    "file_path = \"data/ml-1m.txt\"\n",
    "user_dict = load_data(file_path)\n",
    "num_items = max(max(items) for items in user_dict.values()) + 1\n",
    "print(\"Number of users: \", len(user_dict))\n",
    "print(\"Number of items: \", num_items)   \n",
    "train_dataset = RecDataset(user_dict, num_items, max_seq_length, segment_length)\n",
    "\n",
    "print(\"Number of training samples: \", len(train_dataset))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "for segment, target in train_loader:\n",
    "    print(segment.shape)\n",
    "    print(target.shape)\n",
    "    break\n",
    "    \n",
    "# if the last batch is smaller than batch_size, the last batch will be dropped \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from labml.logger import inspect\n",
    "from labml_nn.transformers.mha import MultiHeadAttention\n",
    "\n",
    "\n",
    "from typing import List, Optional\n",
    "def shift_right(x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    This method shifts $i^{th}$ row of a matrix by $i$ columns.\n",
    "\n",
    "    If the input is `[[1, 2 ,3], [4, 5 ,6], [7, 8, 9]]`, the shifted\n",
    "    result would be `[[1, 2 ,3], [0, 4, 5], [6, 0, 7]]`.\n",
    "    *Ideally we should mask out the lower triangle but it's ok for our purpose*.\n",
    "    \"\"\"\n",
    "\n",
    "    # Concatenate a column of zeros\n",
    "    zero_pad = x.new_zeros(x.shape[0], 1, *x.shape[2:])\n",
    "    x_padded = torch.cat([x, zero_pad], dim=1)\n",
    "\n",
    "    # Reshape and remove excess elements from the end\n",
    "    x_padded = x_padded.view(x.shape[1] + 1, x.shape[0], *x.shape[2:])\n",
    "    x = x_padded[:-1].view_as(x)\n",
    "\n",
    "    #\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class RelativeMultiHeadAttention(MultiHeadAttention):\n",
    "   \n",
    "\n",
    "    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n",
    "        # The linear transformations do not need a bias since we\n",
    "        # explicitly include it when calculating scores.\n",
    "        # However having a bias for `value` might make sense.\n",
    "        # print(heads, d_model, dropout_prob)\n",
    "        super().__init__(heads, d_model, dropout_prob, bias=False)\n",
    "        \n",
    "        # Number of relative positions\n",
    "        self.P = 2 ** 12\n",
    "\n",
    "        # Relative positional embeddings for key relative to the query.\n",
    "        # We need $2P$ embeddings because the keys can be before or after the query.\n",
    "        self.key_pos_embeddings = nn.Parameter(torch.zeros((self.P * 2, heads, self.d_k)), requires_grad=True)\n",
    "        # Relative positional embedding bias for key relative to the query.\n",
    "        self.key_pos_bias = nn.Parameter(torch.zeros((self.P * 2, heads)), requires_grad=True)\n",
    "        # Positional embeddings for the query is independent of the position of the query\n",
    "        self.query_pos_bias = nn.Parameter(torch.zeros((heads, self.d_k)), requires_grad=True)\n",
    "\n",
    "    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
    "        \n",
    "\n",
    "        # $\\textcolor{orange}{R_k}$\n",
    "        key_pos_emb = self.key_pos_embeddings[self.P - key.shape[0]:self.P + query.shape[0]]\n",
    "        # $\\textcolor{orange}{S_k}$\n",
    "        key_pos_bias = self.key_pos_bias[self.P - key.shape[0]:self.P + query.shape[0]]\n",
    "        # $\\textcolor{orange}{v^\\top}$\n",
    "        query_pos_bias = self.query_pos_bias[None, None, :, :]\n",
    "\n",
    "        # ${(\\textcolor{lightgreen}{\\mathbf{A + C}})}_{i,j} =\n",
    "        # Q_i^\\top K_j +\n",
    "        # \\textcolor{orange}{v^\\top} K_j$\n",
    "        ac = torch.einsum('ibhd,jbhd->ijbh', query + query_pos_bias, key)\n",
    "        # $\\textcolor{lightgreen}{\\mathbf{B'}_{i,k}} = Q_i^\\top \\textcolor{orange}{R_k}$\n",
    "        b = torch.einsum('ibhd,jhd->ijbh', query, key_pos_emb)\n",
    "        # $\\textcolor{lightgreen}{\\mathbf{D'}_{i,k}} = \\textcolor{orange}{S_k}$\n",
    "        d = key_pos_bias[None, :, None, :]\n",
    "        # Shift the rows of $\\textcolor{lightgreen}{\\mathbf{(B' + D')}_{i,k}}$\n",
    "        # to get $$\\textcolor{lightgreen}{\\mathbf{(B + D)}_{i,j} = \\mathbf{(B' + D')}_{i,i - j}}$$\n",
    "        bd = shift_right(b + d)\n",
    "        # Remove extra positions\n",
    "        bd = bd[:, -key.shape[0]:]\n",
    "\n",
    "      \n",
    "        return ac + bd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerXLLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, self_attn: RelativeMultiHeadAttention, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.size = d_model\n",
    "        self.self_attn = self_attn\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.norm_self_attn = nn.LayerNorm(d_model)\n",
    "        self.norm_linear = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mem: Optional[torch.Tensor], mask: torch.Tensor):\n",
    "        z = self.norm_self_attn(x)\n",
    "        if mem is not None:\n",
    "            mem = self.norm_self_attn(mem)\n",
    "            \n",
    "            m_z = torch.cat((mem, z), dim=0)\n",
    "        else:\n",
    "            m_z = z\n",
    "        self_attn = self.self_attn(query=z, key=m_z, value=m_z, mask=mask)\n",
    "        x = x + self.dropout(self_attn)\n",
    "        z = self.norm_linear(x)\n",
    "        linear_out = self.linear(z)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        return x\n",
    "\n",
    "class TransformerXL(nn.Module):\n",
    "    def __init__(self, layer: TransformerXLLayer, n_layers: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([layer for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mem: List[torch.Tensor], mask: torch.Tensor):\n",
    "        new_mem = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            new_mem.append(x.detach())\n",
    "            m = mem[i] if mem else None\n",
    "            x = layer(x=x, mem=m, mask=mask)\n",
    "        return self.norm(x), new_mem\n",
    "\n",
    "class TransformerXLEncoder(nn.Module):\n",
    "    def __init__(self, num_items, embed_dim, num_layers, num_heads, hidden_dim, mem_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_items, embed_dim)\n",
    "        self.mem_length = mem_length\n",
    "        # print(embed_dim)\n",
    "        self.transformer = TransformerXL(\n",
    "            TransformerXLLayer(\n",
    "                d_model=embed_dim,\n",
    "                self_attn=RelativeMultiHeadAttention(num_heads,embed_dim, dropout),\n",
    "                dropout_prob=dropout\n",
    "            ),\n",
    "            n_layers=num_layers\n",
    "        )\n",
    "        self.linear = nn.Linear(embed_dim, num_items)\n",
    "    \n",
    "    def forward(self, x, memory=None):\n",
    "        x = self.embedding(x)  # Shape: (B, S, D)\n",
    "        x = x.permute(1, 0, 2)  # Shape: (S, B, D)\n",
    "        mask = None  # Define mask if needed\n",
    "        output, new_memory = self.transformer(x, memory, mask)\n",
    "        logits = self.linear(output)  # Shape: (S, B, num_items)\n",
    "        return logits.permute(1, 2, 0), new_memory  # Shape: (B, num_items, S)\n",
    "    \n",
    "    \n",
    "#example usage\n",
    "# x = torch.randint(0, num_items, (batch_size, segment_length), dtype=torch.long).to(device)\n",
    "# model = TransformerXLEncoder(num_items, embed_dim, num_layers, num_heads, hidden_dim, mem_length).to(device)\n",
    "# logits, memory = model(x)\n",
    "# print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, device, num_epochs=10):\n",
    "    model.train()\n",
    "    memory = None  # Initialize memory\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, memory = model(inputs, memory)  # Pass both sequence & memory\n",
    "            \n",
    "            # print(logits.shape, targets.shape)\n",
    "            loss = criterion(logits, targets)  # Shape: (B, num_items, S)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, user_dict, num_items, max_seq_length, segment_length, device):\n",
    "    model.eval()\n",
    "    NDCG, HR, valid_users = 0.0, 0.0, 0\n",
    "    \n",
    "    for user, items in user_dict.items():\n",
    "        if len(items) < 2:\n",
    "            continue\n",
    "        \n",
    "        seq = items[:max_seq_length]\n",
    "        input_seq = torch.tensor(seq[:-1], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        target = seq[-1]\n",
    "        candidates = [target] + random.sample(set(range(1, num_items)) - set(items), 99)\n",
    "        \n",
    "        memory = None\n",
    "        for i in range(0, len(input_seq[0]), segment_length):\n",
    "            segment = input_seq[:, i:i+segment_length]\n",
    "            logits, memory = model(segment, memory)\n",
    "            \n",
    "        # logits shape is (1, num_items, segment_length)\n",
    "        scores = logits[0, :, -1]\n",
    "        scores = scores[candidates]\n",
    "        # print(scores)\n",
    "        ranked = torch.argsort(scores, descending=True).cpu().numpy()\n",
    "        rank = np.where(ranked == 0)[0][0] + 1\n",
    "        # print(rank)\n",
    "        valid_users += 1\n",
    "        \n",
    "        \n",
    "        HR += int(rank <= 10)\n",
    "        NDCG += 1 / np.log2(rank + 1) if rank <= 10 else 0\n",
    "        \n",
    "        if valid_users % 10 == 0:\n",
    "            print(f\"Validated users: {valid_users}, HR@10: {HR / valid_users:.4f}, NDCG@10: {NDCG / valid_users:.4f}\")\n",
    "    print(f\"HR@10: {HR / valid_users:.4f}, NDCG@10: {NDCG / valid_users:.4f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerXLEncoder(num_items, embed_dim, num_layers, num_heads, hidden_dim, mem_length).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(model)\n",
    "num_epochs=100\n",
    "train_model(model, train_loader, optimizer, criterion, device, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, user_dict, num_items, max_seq_length, segment_length, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/gnaneswaras/.local/lib/python3.8/site-packages/mamba_ssm/ops/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
      "/raid/home/gnaneswaras/.local/lib/python3.8/site-packages/mamba_ssm/ops/selective_scan_interface.py:240: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout):\n",
      "/raid/home/gnaneswaras/.local/lib/python3.8/site-packages/mamba_ssm/ops/triton/layer_norm.py:986: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/raid/home/gnaneswaras/.local/lib/python3.8/site-packages/mamba_ssm/ops/triton/layer_norm.py:1045: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n",
      "/raid/home/gnaneswaras/.local/lib/python3.8/site-packages/mamba_ssm/distributed/tensor_parallel.py:26: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, x, weight, bias, process_group=None, sequence_parallel=True):\n",
      "/raid/home/gnaneswaras/.local/lib/python3.8/site-packages/mamba_ssm/distributed/tensor_parallel.py:62: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/raid/home/gnaneswaras/.local/lib/python3.8/site-packages/mamba_ssm/ops/triton/ssd_combined.py:758: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n",
      "/raid/home/gnaneswaras/.local/lib/python3.8/site-packages/mamba_ssm/ops/triton/ssd_combined.py:836: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from mamba_ssm import Mamba2\n",
    "import torch\n",
    "\n",
    "batch, length, dim = 128, 208, 128\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "print(x.type())\n",
    "model = Mamba2(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=dim, # Model dimension d_model\n",
    "    d_state=128,  # SSM state expansion factor, typically 64 or 128\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    "    headdim=32,\n",
    ").to(\"cuda\")\n",
    "y = model(x)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: torch.Size([2, 64, 384])\n",
      "Y.shape: torch.Size([2, 64, 384])\n"
     ]
    }
   ],
   "source": [
    "print(f\"X.shape: {x.shape}\")\n",
    "print(f\"Y.shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n"
     ]
    }
   ],
   "source": [
    "expand =2\n",
    "d_model = 32\n",
    "d_state = 16\n",
    "headdim = 8\n",
    "d_inner = (expand *d_model) \n",
    "d_ssm = d_inner\n",
    "nheads = d_ssm // headdim\n",
    "d_in_proj = 2 * d_inner + 2 * d_state + nheads\n",
    "print(d_in_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
