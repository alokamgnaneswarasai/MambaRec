{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users:  6040\n",
      "Number of items:  3417\n",
      "Number of training samples:  12071\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "def load_data(file_path):\n",
    "    user_dict = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            user, item = map(int, line.strip().split())\n",
    "            if user not in user_dict:\n",
    "                user_dict[user] = []\n",
    "            user_dict[user].append(item)\n",
    "    return user_dict\n",
    "\n",
    "class RecDataset(Dataset):\n",
    "    def __init__(self, user_dict, num_items, max_seq_length, segment_length):\n",
    "        self.data = []\n",
    "        for user, items in user_dict.items():\n",
    "            items = items[:-1] #remove the last item as it can be used during evaluation time\n",
    "            if len(items) < 2:\n",
    "                continue\n",
    "            \n",
    "            if len(items)<max_seq_length:\n",
    "                items = [0]*(max_seq_length+1-len(items)) + items\n",
    "                \n",
    "            \n",
    "            seq = items[:max_seq_length+1]  #first max_seq_length items\n",
    "            for i in range(0, len(seq) - 1, segment_length):\n",
    "                segment = seq[i:i+segment_length]\n",
    "                target = seq[i+1:i+1+segment_length]\n",
    "                if len(segment) == segment_length and len(target) == segment_length:\n",
    "                    self.data.append((segment, target))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx][0], dtype=torch.long), torch.tensor(self.data[idx][1], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embed_dim = 32\n",
    "num_layers = 1\n",
    "num_heads = 8\n",
    "hidden_dim = 32\n",
    "mem_length = 100\n",
    "max_seq_length = 200\n",
    "segment_length = 100\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# Load data\n",
    "\n",
    "file_path = \"data/ml-1m.txt\"\n",
    "user_dict = load_data(file_path)\n",
    "num_items = max(max(items) for items in user_dict.values()) + 1\n",
    "print(\"Number of users: \", len(user_dict))\n",
    "print(\"Number of items: \", num_items)   \n",
    "train_dataset = RecDataset(user_dict, num_items, max_seq_length, segment_length)\n",
    "\n",
    "print(\"Number of training samples: \", len(train_dataset))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "for segment, target in train_loader:\n",
    "    print(segment.shape)\n",
    "    print(target.shape)\n",
    "    break\n",
    "    \n",
    "# if the last batch is smaller than batch_size, the last batch will be dropped \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from labml.logger import inspect\n",
    "from labml_nn.transformers.mha import MultiHeadAttention\n",
    "\n",
    "\n",
    "from typing import List, Optional\n",
    "def shift_right(x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    This method shifts $i^{th}$ row of a matrix by $i$ columns.\n",
    "\n",
    "    If the input is `[[1, 2 ,3], [4, 5 ,6], [7, 8, 9]]`, the shifted\n",
    "    result would be `[[1, 2 ,3], [0, 4, 5], [6, 0, 7]]`.\n",
    "    *Ideally we should mask out the lower triangle but it's ok for our purpose*.\n",
    "    \"\"\"\n",
    "\n",
    "    # Concatenate a column of zeros\n",
    "    zero_pad = x.new_zeros(x.shape[0], 1, *x.shape[2:])\n",
    "    x_padded = torch.cat([x, zero_pad], dim=1)\n",
    "\n",
    "    # Reshape and remove excess elements from the end\n",
    "    x_padded = x_padded.view(x.shape[1] + 1, x.shape[0], *x.shape[2:])\n",
    "    x = x_padded[:-1].view_as(x)\n",
    "\n",
    "    #\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class RelativeMultiHeadAttention(MultiHeadAttention):\n",
    "   \n",
    "\n",
    "    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n",
    "        # The linear transformations do not need a bias since we\n",
    "        # explicitly include it when calculating scores.\n",
    "        # However having a bias for `value` might make sense.\n",
    "        # print(heads, d_model, dropout_prob)\n",
    "        super().__init__(heads, d_model, dropout_prob, bias=False)\n",
    "        \n",
    "        # Number of relative positions\n",
    "        self.P = 2 ** 12\n",
    "\n",
    "        # Relative positional embeddings for key relative to the query.\n",
    "        # We need $2P$ embeddings because the keys can be before or after the query.\n",
    "        self.key_pos_embeddings = nn.Parameter(torch.zeros((self.P * 2, heads, self.d_k)), requires_grad=True)\n",
    "        # Relative positional embedding bias for key relative to the query.\n",
    "        self.key_pos_bias = nn.Parameter(torch.zeros((self.P * 2, heads)), requires_grad=True)\n",
    "        # Positional embeddings for the query is independent of the position of the query\n",
    "        self.query_pos_bias = nn.Parameter(torch.zeros((heads, self.d_k)), requires_grad=True)\n",
    "\n",
    "    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
    "        \n",
    "\n",
    "        # $\\textcolor{orange}{R_k}$\n",
    "        key_pos_emb = self.key_pos_embeddings[self.P - key.shape[0]:self.P + query.shape[0]]\n",
    "        # $\\textcolor{orange}{S_k}$\n",
    "        key_pos_bias = self.key_pos_bias[self.P - key.shape[0]:self.P + query.shape[0]]\n",
    "        # $\\textcolor{orange}{v^\\top}$\n",
    "        query_pos_bias = self.query_pos_bias[None, None, :, :]\n",
    "\n",
    "        # ${(\\textcolor{lightgreen}{\\mathbf{A + C}})}_{i,j} =\n",
    "        # Q_i^\\top K_j +\n",
    "        # \\textcolor{orange}{v^\\top} K_j$\n",
    "        ac = torch.einsum('ibhd,jbhd->ijbh', query + query_pos_bias, key)\n",
    "        # $\\textcolor{lightgreen}{\\mathbf{B'}_{i,k}} = Q_i^\\top \\textcolor{orange}{R_k}$\n",
    "        b = torch.einsum('ibhd,jhd->ijbh', query, key_pos_emb)\n",
    "        # $\\textcolor{lightgreen}{\\mathbf{D'}_{i,k}} = \\textcolor{orange}{S_k}$\n",
    "        d = key_pos_bias[None, :, None, :]\n",
    "        # Shift the rows of $\\textcolor{lightgreen}{\\mathbf{(B' + D')}_{i,k}}$\n",
    "        # to get $$\\textcolor{lightgreen}{\\mathbf{(B + D)}_{i,j} = \\mathbf{(B' + D')}_{i,i - j}}$$\n",
    "        bd = shift_right(b + d)\n",
    "        # Remove extra positions\n",
    "        bd = bd[:, -key.shape[0]:]\n",
    "\n",
    "      \n",
    "        return ac + bd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerXLLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, self_attn: RelativeMultiHeadAttention, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.size = d_model\n",
    "        self.self_attn = self_attn\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.norm_self_attn = nn.LayerNorm(d_model)\n",
    "        self.norm_linear = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mem: Optional[torch.Tensor], mask: torch.Tensor):\n",
    "        z = self.norm_self_attn(x)\n",
    "        if mem is not None:\n",
    "            mem = self.norm_self_attn(mem)\n",
    "            \n",
    "            m_z = torch.cat((mem, z), dim=0)\n",
    "        else:\n",
    "            m_z = z\n",
    "        self_attn = self.self_attn(query=z, key=m_z, value=m_z, mask=mask)\n",
    "        x = x + self.dropout(self_attn)\n",
    "        z = self.norm_linear(x)\n",
    "        linear_out = self.linear(z)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        return x\n",
    "\n",
    "class TransformerXL(nn.Module):\n",
    "    def __init__(self, layer: TransformerXLLayer, n_layers: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([layer for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mem: List[torch.Tensor], mask: torch.Tensor):\n",
    "        new_mem = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            new_mem.append(x.detach())\n",
    "            m = mem[i] if mem else None\n",
    "            x = layer(x=x, mem=m, mask=mask)\n",
    "        return self.norm(x), new_mem\n",
    "\n",
    "class TransformerXLEncoder(nn.Module):\n",
    "    def __init__(self, num_items, embed_dim, num_layers, num_heads, hidden_dim, mem_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_items, embed_dim)\n",
    "        self.mem_length = mem_length\n",
    "        # print(embed_dim)\n",
    "        self.transformer = TransformerXL(\n",
    "            TransformerXLLayer(\n",
    "                d_model=embed_dim,\n",
    "                self_attn=RelativeMultiHeadAttention(num_heads,embed_dim, dropout),\n",
    "                dropout_prob=dropout\n",
    "            ),\n",
    "            n_layers=num_layers\n",
    "        )\n",
    "        self.linear = nn.Linear(embed_dim, num_items)\n",
    "    \n",
    "    def forward(self, x, memory=None):\n",
    "        x = self.embedding(x)  # Shape: (B, S, D)\n",
    "        x = x.permute(1, 0, 2)  # Shape: (S, B, D)\n",
    "        mask = None  # Define mask if needed\n",
    "        output, new_memory = self.transformer(x, memory, mask)\n",
    "        logits = self.linear(output)  # Shape: (S, B, num_items)\n",
    "        return logits.permute(1, 2, 0), new_memory  # Shape: (B, num_items, S)\n",
    "    \n",
    "    \n",
    "#example usage\n",
    "# x = torch.randint(0, num_items, (batch_size, segment_length), dtype=torch.long).to(device)\n",
    "# model = TransformerXLEncoder(num_items, embed_dim, num_layers, num_heads, hidden_dim, mem_length).to(device)\n",
    "# logits, memory = model(x)\n",
    "# print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, device, num_epochs=10):\n",
    "    model.train()\n",
    "    memory = None  # Initialize memory\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, memory = model(inputs, memory)  # Pass both sequence & memory\n",
    "            \n",
    "            # print(logits.shape, targets.shape)\n",
    "            loss = criterion(logits, targets)  # Shape: (B, num_items, S)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, user_dict, num_items, max_seq_length, segment_length, device):\n",
    "    model.eval()\n",
    "    NDCG, HR, valid_users = 0.0, 0.0, 0\n",
    "    \n",
    "    for user, items in user_dict.items():\n",
    "        if len(items) < 2:\n",
    "            continue\n",
    "        \n",
    "        seq = items[:max_seq_length]\n",
    "        input_seq = torch.tensor(seq[:-1], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        target = seq[-1]\n",
    "        candidates = [target] + random.sample(set(range(1, num_items)) - set(items), 99)\n",
    "        \n",
    "        memory = None\n",
    "        for i in range(0, len(input_seq[0]), segment_length):\n",
    "            segment = input_seq[:, i:i+segment_length]\n",
    "            logits, memory = model(segment, memory)\n",
    "            \n",
    "        # logits shape is (1, num_items, segment_length)\n",
    "        scores = logits[0, :, -1]\n",
    "        scores = scores[candidates]\n",
    "        # print(scores)\n",
    "        ranked = torch.argsort(scores, descending=True).cpu().numpy()\n",
    "        rank = np.where(ranked == 0)[0][0] + 1\n",
    "        # print(rank)\n",
    "        valid_users += 1\n",
    "        \n",
    "        \n",
    "        HR += int(rank <= 10)\n",
    "        NDCG += 1 / np.log2(rank + 1) if rank <= 10 else 0\n",
    "        \n",
    "        if valid_users % 10 == 0:\n",
    "            print(f\"Validated users: {valid_users}, HR@10: {HR / valid_users:.4f}, NDCG@10: {NDCG / valid_users:.4f}\")\n",
    "    print(f\"HR@10: {HR / valid_users:.4f}, NDCG@10: {NDCG / valid_users:.4f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerXLEncoder(\n",
      "  (embedding): Embedding(3417, 32)\n",
      "  (transformer): TransformerXL(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerXLLayer(\n",
      "        (self_attn): RelativeMultiHeadAttention(\n",
      "          (query): PrepareForMultiHeadAttention(\n",
      "            (linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          )\n",
      "          (key): PrepareForMultiHeadAttention(\n",
      "            (linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          )\n",
      "          (value): PrepareForMultiHeadAttention(\n",
      "            (linear): Linear(in_features=32, out_features=32, bias=True)\n",
      "          )\n",
      "          (softmax): Softmax(dim=1)\n",
      "          (output): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (linear): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (norm_self_attn): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_linear): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (linear): Linear(in_features=32, out_features=3417, bias=True)\n",
      ")\n",
      "Epoch 1, Loss: 4.5599\n",
      "Epoch 2, Loss: 3.9840\n",
      "Epoch 3, Loss: 3.7583\n",
      "Epoch 4, Loss: 2.0216\n",
      "Epoch 5, Loss: 0.5983\n",
      "Epoch 6, Loss: 0.2569\n",
      "Epoch 7, Loss: 0.1619\n",
      "Epoch 8, Loss: 0.1231\n",
      "Epoch 9, Loss: 0.1027\n",
      "Epoch 10, Loss: 0.0904\n",
      "Epoch 11, Loss: 0.0818\n",
      "Epoch 12, Loss: 0.0762\n",
      "Epoch 13, Loss: 0.0718\n",
      "Epoch 14, Loss: 0.0682\n",
      "Epoch 15, Loss: 0.0659\n",
      "Epoch 16, Loss: 0.0636\n",
      "Epoch 17, Loss: 0.0618\n",
      "Epoch 18, Loss: 0.0602\n",
      "Epoch 19, Loss: 0.0593\n",
      "Epoch 20, Loss: 0.0578\n",
      "Epoch 21, Loss: 0.0568\n",
      "Epoch 22, Loss: 0.0557\n",
      "Epoch 23, Loss: 0.0551\n",
      "Epoch 24, Loss: 0.0544\n",
      "Epoch 25, Loss: 0.0534\n",
      "Epoch 26, Loss: 0.0527\n",
      "Epoch 27, Loss: 0.0524\n",
      "Epoch 28, Loss: 0.0518\n",
      "Epoch 29, Loss: 0.0511\n",
      "Epoch 30, Loss: 0.0505\n",
      "Epoch 31, Loss: 0.0499\n",
      "Epoch 32, Loss: 0.0494\n",
      "Epoch 33, Loss: 0.0488\n",
      "Epoch 34, Loss: 0.0485\n",
      "Epoch 35, Loss: 0.0477\n",
      "Epoch 36, Loss: 0.0474\n",
      "Epoch 37, Loss: 0.0467\n",
      "Epoch 38, Loss: 0.0464\n",
      "Epoch 39, Loss: 0.0459\n",
      "Epoch 40, Loss: 0.0455\n",
      "Epoch 41, Loss: 0.0449\n",
      "Epoch 42, Loss: 0.0446\n",
      "Epoch 43, Loss: 0.0440\n",
      "Epoch 44, Loss: 0.0433\n",
      "Epoch 45, Loss: 0.0430\n",
      "Epoch 46, Loss: 0.0425\n",
      "Epoch 47, Loss: 0.0421\n",
      "Epoch 48, Loss: 0.0417\n",
      "Epoch 49, Loss: 0.0411\n",
      "Epoch 50, Loss: 0.0407\n",
      "Epoch 51, Loss: 0.0404\n",
      "Epoch 52, Loss: 0.0401\n",
      "Epoch 53, Loss: 0.0393\n",
      "Epoch 54, Loss: 0.0388\n",
      "Epoch 55, Loss: 0.0384\n",
      "Epoch 56, Loss: 0.0380\n",
      "Epoch 57, Loss: 0.0376\n",
      "Epoch 58, Loss: 0.0371\n",
      "Epoch 59, Loss: 0.0368\n",
      "Epoch 60, Loss: 0.0363\n",
      "Epoch 61, Loss: 0.0359\n",
      "Epoch 62, Loss: 0.0353\n",
      "Epoch 63, Loss: 0.0348\n",
      "Epoch 64, Loss: 0.0344\n",
      "Epoch 65, Loss: 0.0343\n",
      "Epoch 66, Loss: 0.0338\n",
      "Epoch 67, Loss: 0.0333\n",
      "Epoch 68, Loss: 0.0332\n",
      "Epoch 69, Loss: 0.0327\n",
      "Epoch 70, Loss: 0.0320\n",
      "Epoch 71, Loss: 0.0317\n",
      "Epoch 72, Loss: 0.0314\n",
      "Epoch 73, Loss: 0.0310\n",
      "Epoch 74, Loss: 0.0308\n",
      "Epoch 75, Loss: 0.0305\n",
      "Epoch 76, Loss: 0.0301\n",
      "Epoch 77, Loss: 0.0296\n",
      "Epoch 78, Loss: 0.0298\n",
      "Epoch 79, Loss: 0.0289\n",
      "Epoch 80, Loss: 0.0289\n",
      "Epoch 81, Loss: 0.0286\n",
      "Epoch 82, Loss: 0.0279\n",
      "Epoch 83, Loss: 0.0277\n",
      "Epoch 84, Loss: 0.0274\n",
      "Epoch 85, Loss: 0.0273\n",
      "Epoch 86, Loss: 0.0270\n",
      "Epoch 87, Loss: 0.0270\n",
      "Epoch 88, Loss: 0.0264\n",
      "Epoch 89, Loss: 0.0263\n",
      "Epoch 90, Loss: 0.0262\n",
      "Epoch 91, Loss: 0.0258\n",
      "Epoch 92, Loss: 0.0255\n",
      "Epoch 93, Loss: 0.0254\n",
      "Epoch 94, Loss: 0.0249\n",
      "Epoch 95, Loss: 0.0245\n",
      "Epoch 96, Loss: 0.0244\n",
      "Epoch 97, Loss: 0.0243\n",
      "Epoch 98, Loss: 0.0242\n",
      "Epoch 99, Loss: 0.0236\n",
      "Epoch 100, Loss: 0.0235\n"
     ]
    }
   ],
   "source": [
    "model = TransformerXLEncoder(num_items, embed_dim, num_layers, num_heads, hidden_dim, mem_length).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(model)\n",
    "num_epochs=100\n",
    "train_model(model, train_loader, optimizer, criterion, device, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated users: 10, HR@10: 0.8000, NDCG@10: 0.7262\n",
      "Validated users: 20, HR@10: 0.8500, NDCG@10: 0.7824\n",
      "Validated users: 30, HR@10: 0.7667, NDCG@10: 0.6865\n",
      "Validated users: 40, HR@10: 0.7500, NDCG@10: 0.6899\n",
      "Validated users: 50, HR@10: 0.7400, NDCG@10: 0.6723\n",
      "Validated users: 60, HR@10: 0.7500, NDCG@10: 0.6812\n",
      "Validated users: 70, HR@10: 0.7714, NDCG@10: 0.6974\n",
      "Validated users: 80, HR@10: 0.7875, NDCG@10: 0.7042\n",
      "Validated users: 90, HR@10: 0.7889, NDCG@10: 0.7067\n",
      "Validated users: 100, HR@10: 0.8100, NDCG@10: 0.7299\n",
      "Validated users: 110, HR@10: 0.8091, NDCG@10: 0.7273\n",
      "Validated users: 120, HR@10: 0.8250, NDCG@10: 0.7274\n",
      "Validated users: 130, HR@10: 0.8154, NDCG@10: 0.7123\n",
      "Validated users: 140, HR@10: 0.8214, NDCG@10: 0.7231\n",
      "Validated users: 150, HR@10: 0.8333, NDCG@10: 0.7382\n",
      "Validated users: 160, HR@10: 0.8375, NDCG@10: 0.7483\n",
      "Validated users: 170, HR@10: 0.8353, NDCG@10: 0.7475\n",
      "Validated users: 180, HR@10: 0.8333, NDCG@10: 0.7436\n",
      "Validated users: 190, HR@10: 0.8368, NDCG@10: 0.7483\n",
      "Validated users: 200, HR@10: 0.8450, NDCG@10: 0.7547\n",
      "Validated users: 210, HR@10: 0.8381, NDCG@10: 0.7421\n",
      "Validated users: 220, HR@10: 0.8455, NDCG@10: 0.7476\n",
      "Validated users: 230, HR@10: 0.8522, NDCG@10: 0.7548\n",
      "Validated users: 240, HR@10: 0.8583, NDCG@10: 0.7596\n",
      "Validated users: 250, HR@10: 0.8520, NDCG@10: 0.7552\n",
      "Validated users: 260, HR@10: 0.8538, NDCG@10: 0.7594\n",
      "Validated users: 270, HR@10: 0.8593, NDCG@10: 0.7669\n",
      "Validated users: 280, HR@10: 0.8571, NDCG@10: 0.7668\n",
      "Validated users: 290, HR@10: 0.8483, NDCG@10: 0.7610\n",
      "Validated users: 300, HR@10: 0.8467, NDCG@10: 0.7563\n",
      "Validated users: 310, HR@10: 0.8484, NDCG@10: 0.7610\n",
      "Validated users: 320, HR@10: 0.8531, NDCG@10: 0.7661\n",
      "Validated users: 330, HR@10: 0.8545, NDCG@10: 0.7680\n",
      "Validated users: 340, HR@10: 0.8529, NDCG@10: 0.7614\n",
      "Validated users: 350, HR@10: 0.8543, NDCG@10: 0.7619\n",
      "Validated users: 360, HR@10: 0.8583, NDCG@10: 0.7657\n",
      "Validated users: 370, HR@10: 0.8595, NDCG@10: 0.7683\n",
      "Validated users: 380, HR@10: 0.8605, NDCG@10: 0.7672\n",
      "Validated users: 390, HR@10: 0.8641, NDCG@10: 0.7703\n",
      "Validated users: 400, HR@10: 0.8650, NDCG@10: 0.7702\n",
      "Validated users: 410, HR@10: 0.8659, NDCG@10: 0.7708\n",
      "Validated users: 420, HR@10: 0.8619, NDCG@10: 0.7682\n",
      "Validated users: 430, HR@10: 0.8628, NDCG@10: 0.7683\n",
      "Validated users: 440, HR@10: 0.8614, NDCG@10: 0.7668\n",
      "Validated users: 450, HR@10: 0.8600, NDCG@10: 0.7667\n",
      "Validated users: 460, HR@10: 0.8565, NDCG@10: 0.7653\n",
      "Validated users: 470, HR@10: 0.8553, NDCG@10: 0.7642\n",
      "Validated users: 480, HR@10: 0.8521, NDCG@10: 0.7621\n",
      "Validated users: 490, HR@10: 0.8531, NDCG@10: 0.7631\n",
      "Validated users: 500, HR@10: 0.8540, NDCG@10: 0.7651\n",
      "Validated users: 510, HR@10: 0.8569, NDCG@10: 0.7687\n",
      "Validated users: 520, HR@10: 0.8481, NDCG@10: 0.7607\n",
      "Validated users: 530, HR@10: 0.8453, NDCG@10: 0.7595\n",
      "Validated users: 540, HR@10: 0.8444, NDCG@10: 0.7580\n",
      "Validated users: 550, HR@10: 0.8455, NDCG@10: 0.7586\n",
      "Validated users: 560, HR@10: 0.8446, NDCG@10: 0.7576\n",
      "Validated users: 570, HR@10: 0.8439, NDCG@10: 0.7560\n",
      "Validated users: 580, HR@10: 0.8414, NDCG@10: 0.7533\n",
      "Validated users: 590, HR@10: 0.8390, NDCG@10: 0.7509\n",
      "Validated users: 600, HR@10: 0.8417, NDCG@10: 0.7534\n",
      "Validated users: 610, HR@10: 0.8410, NDCG@10: 0.7515\n",
      "Validated users: 620, HR@10: 0.8435, NDCG@10: 0.7540\n",
      "Validated users: 630, HR@10: 0.8397, NDCG@10: 0.7515\n",
      "Validated users: 640, HR@10: 0.8391, NDCG@10: 0.7523\n",
      "Validated users: 650, HR@10: 0.8385, NDCG@10: 0.7524\n",
      "Validated users: 660, HR@10: 0.8394, NDCG@10: 0.7526\n",
      "Validated users: 670, HR@10: 0.8373, NDCG@10: 0.7508\n",
      "Validated users: 680, HR@10: 0.8382, NDCG@10: 0.7501\n",
      "Validated users: 690, HR@10: 0.8362, NDCG@10: 0.7478\n",
      "Validated users: 700, HR@10: 0.8371, NDCG@10: 0.7500\n",
      "Validated users: 710, HR@10: 0.8352, NDCG@10: 0.7483\n",
      "Validated users: 720, HR@10: 0.8361, NDCG@10: 0.7490\n",
      "Validated users: 730, HR@10: 0.8342, NDCG@10: 0.7474\n",
      "Validated users: 740, HR@10: 0.8365, NDCG@10: 0.7484\n",
      "Validated users: 750, HR@10: 0.8347, NDCG@10: 0.7470\n",
      "Validated users: 760, HR@10: 0.8342, NDCG@10: 0.7461\n",
      "Validated users: 770, HR@10: 0.8325, NDCG@10: 0.7442\n",
      "Validated users: 780, HR@10: 0.8346, NDCG@10: 0.7456\n",
      "Validated users: 790, HR@10: 0.8367, NDCG@10: 0.7461\n",
      "Validated users: 800, HR@10: 0.8350, NDCG@10: 0.7444\n",
      "Validated users: 810, HR@10: 0.8346, NDCG@10: 0.7443\n",
      "Validated users: 820, HR@10: 0.8354, NDCG@10: 0.7453\n",
      "Validated users: 830, HR@10: 0.8349, NDCG@10: 0.7455\n",
      "Validated users: 840, HR@10: 0.8357, NDCG@10: 0.7462\n",
      "Validated users: 850, HR@10: 0.8365, NDCG@10: 0.7453\n",
      "Validated users: 860, HR@10: 0.8372, NDCG@10: 0.7440\n",
      "Validated users: 870, HR@10: 0.8356, NDCG@10: 0.7429\n",
      "Validated users: 880, HR@10: 0.8364, NDCG@10: 0.7437\n",
      "Validated users: 890, HR@10: 0.8382, NDCG@10: 0.7450\n",
      "Validated users: 900, HR@10: 0.8389, NDCG@10: 0.7463\n",
      "Validated users: 910, HR@10: 0.8396, NDCG@10: 0.7465\n",
      "Validated users: 920, HR@10: 0.8380, NDCG@10: 0.7450\n",
      "Validated users: 930, HR@10: 0.8366, NDCG@10: 0.7429\n",
      "Validated users: 940, HR@10: 0.8372, NDCG@10: 0.7442\n",
      "Validated users: 950, HR@10: 0.8379, NDCG@10: 0.7446\n",
      "Validated users: 960, HR@10: 0.8385, NDCG@10: 0.7456\n",
      "Validated users: 970, HR@10: 0.8381, NDCG@10: 0.7447\n",
      "Validated users: 980, HR@10: 0.8388, NDCG@10: 0.7447\n",
      "Validated users: 990, HR@10: 0.8394, NDCG@10: 0.7459\n",
      "Validated users: 1000, HR@10: 0.8390, NDCG@10: 0.7461\n",
      "Validated users: 1010, HR@10: 0.8396, NDCG@10: 0.7470\n",
      "Validated users: 1020, HR@10: 0.8373, NDCG@10: 0.7452\n",
      "Validated users: 1030, HR@10: 0.8379, NDCG@10: 0.7456\n",
      "Validated users: 1040, HR@10: 0.8356, NDCG@10: 0.7428\n",
      "Validated users: 1050, HR@10: 0.8343, NDCG@10: 0.7418\n",
      "Validated users: 1060, HR@10: 0.8349, NDCG@10: 0.7422\n",
      "Validated users: 1070, HR@10: 0.8355, NDCG@10: 0.7432\n",
      "Validated users: 1080, HR@10: 0.8361, NDCG@10: 0.7443\n",
      "Validated users: 1090, HR@10: 0.8349, NDCG@10: 0.7434\n",
      "Validated users: 1100, HR@10: 0.8364, NDCG@10: 0.7441\n",
      "Validated users: 1110, HR@10: 0.8351, NDCG@10: 0.7429\n",
      "Validated users: 1120, HR@10: 0.8366, NDCG@10: 0.7446\n",
      "Validated users: 1130, HR@10: 0.8372, NDCG@10: 0.7452\n",
      "Validated users: 1140, HR@10: 0.8351, NDCG@10: 0.7436\n",
      "Validated users: 1150, HR@10: 0.8357, NDCG@10: 0.7441\n",
      "Validated users: 1160, HR@10: 0.8353, NDCG@10: 0.7434\n",
      "Validated users: 1170, HR@10: 0.8368, NDCG@10: 0.7452\n",
      "Validated users: 1180, HR@10: 0.8364, NDCG@10: 0.7451\n",
      "Validated users: 1190, HR@10: 0.8361, NDCG@10: 0.7441\n",
      "Validated users: 1200, HR@10: 0.8342, NDCG@10: 0.7426\n",
      "Validated users: 1210, HR@10: 0.8331, NDCG@10: 0.7412\n",
      "Validated users: 1220, HR@10: 0.8328, NDCG@10: 0.7408\n",
      "Validated users: 1230, HR@10: 0.8317, NDCG@10: 0.7387\n",
      "Validated users: 1240, HR@10: 0.8315, NDCG@10: 0.7386\n",
      "Validated users: 1250, HR@10: 0.8296, NDCG@10: 0.7375\n",
      "Validated users: 1260, HR@10: 0.8294, NDCG@10: 0.7371\n",
      "Validated users: 1270, HR@10: 0.8283, NDCG@10: 0.7368\n",
      "Validated users: 1280, HR@10: 0.8281, NDCG@10: 0.7363\n",
      "Validated users: 1290, HR@10: 0.8279, NDCG@10: 0.7364\n",
      "Validated users: 1300, HR@10: 0.8285, NDCG@10: 0.7374\n",
      "Validated users: 1310, HR@10: 0.8282, NDCG@10: 0.7375\n",
      "Validated users: 1320, HR@10: 0.8288, NDCG@10: 0.7381\n",
      "Validated users: 1330, HR@10: 0.8286, NDCG@10: 0.7378\n",
      "Validated users: 1340, HR@10: 0.8276, NDCG@10: 0.7370\n",
      "Validated users: 1350, HR@10: 0.8281, NDCG@10: 0.7368\n",
      "Validated users: 1360, HR@10: 0.8287, NDCG@10: 0.7374\n",
      "Validated users: 1370, HR@10: 0.8292, NDCG@10: 0.7376\n",
      "Validated users: 1380, HR@10: 0.8283, NDCG@10: 0.7365\n",
      "Validated users: 1390, HR@10: 0.8266, NDCG@10: 0.7348\n",
      "Validated users: 1400, HR@10: 0.8257, NDCG@10: 0.7343\n",
      "Validated users: 1410, HR@10: 0.8255, NDCG@10: 0.7340\n",
      "Validated users: 1420, HR@10: 0.8254, NDCG@10: 0.7335\n",
      "Validated users: 1430, HR@10: 0.8259, NDCG@10: 0.7334\n",
      "Validated users: 1440, HR@10: 0.8271, NDCG@10: 0.7345\n",
      "Validated users: 1450, HR@10: 0.8262, NDCG@10: 0.7336\n",
      "Validated users: 1460, HR@10: 0.8253, NDCG@10: 0.7327\n",
      "Validated users: 1470, HR@10: 0.8259, NDCG@10: 0.7339\n",
      "Validated users: 1480, HR@10: 0.8264, NDCG@10: 0.7341\n",
      "Validated users: 1490, HR@10: 0.8248, NDCG@10: 0.7327\n",
      "Validated users: 1500, HR@10: 0.8253, NDCG@10: 0.7322\n",
      "Validated users: 1510, HR@10: 0.8258, NDCG@10: 0.7316\n",
      "Validated users: 1520, HR@10: 0.8270, NDCG@10: 0.7322\n",
      "Validated users: 1530, HR@10: 0.8268, NDCG@10: 0.7319\n",
      "Validated users: 1540, HR@10: 0.8266, NDCG@10: 0.7324\n",
      "Validated users: 1550, HR@10: 0.8258, NDCG@10: 0.7322\n",
      "Validated users: 1560, HR@10: 0.8256, NDCG@10: 0.7324\n",
      "Validated users: 1570, HR@10: 0.8261, NDCG@10: 0.7326\n",
      "Validated users: 1580, HR@10: 0.8259, NDCG@10: 0.7328\n",
      "Validated users: 1590, HR@10: 0.8264, NDCG@10: 0.7333\n",
      "Validated users: 1600, HR@10: 0.8275, NDCG@10: 0.7347\n",
      "Validated users: 1610, HR@10: 0.8267, NDCG@10: 0.7340\n",
      "Validated users: 1620, HR@10: 0.8259, NDCG@10: 0.7324\n",
      "Validated users: 1630, HR@10: 0.8258, NDCG@10: 0.7323\n",
      "Validated users: 1640, HR@10: 0.8256, NDCG@10: 0.7322\n",
      "Validated users: 1650, HR@10: 0.8248, NDCG@10: 0.7317\n",
      "Validated users: 1660, HR@10: 0.8253, NDCG@10: 0.7318\n",
      "Validated users: 1670, HR@10: 0.8257, NDCG@10: 0.7326\n",
      "Validated users: 1680, HR@10: 0.8250, NDCG@10: 0.7324\n",
      "Validated users: 1690, HR@10: 0.8243, NDCG@10: 0.7322\n",
      "Validated users: 1700, HR@10: 0.8247, NDCG@10: 0.7329\n",
      "Validated users: 1710, HR@10: 0.8240, NDCG@10: 0.7325\n",
      "Validated users: 1720, HR@10: 0.8238, NDCG@10: 0.7323\n",
      "Validated users: 1730, HR@10: 0.8237, NDCG@10: 0.7322\n",
      "Validated users: 1740, HR@10: 0.8236, NDCG@10: 0.7323\n",
      "Validated users: 1750, HR@10: 0.8234, NDCG@10: 0.7321\n",
      "Validated users: 1760, HR@10: 0.8239, NDCG@10: 0.7325\n",
      "Validated users: 1770, HR@10: 0.8237, NDCG@10: 0.7322\n",
      "Validated users: 1780, HR@10: 0.8236, NDCG@10: 0.7323\n",
      "Validated users: 1790, HR@10: 0.8235, NDCG@10: 0.7316\n",
      "Validated users: 1800, HR@10: 0.8244, NDCG@10: 0.7326\n",
      "Validated users: 1810, HR@10: 0.8243, NDCG@10: 0.7323\n",
      "Validated users: 1820, HR@10: 0.8236, NDCG@10: 0.7319\n",
      "Validated users: 1830, HR@10: 0.8246, NDCG@10: 0.7330\n",
      "Validated users: 1840, HR@10: 0.8255, NDCG@10: 0.7336\n",
      "Validated users: 1850, HR@10: 0.8259, NDCG@10: 0.7343\n",
      "Validated users: 1860, HR@10: 0.8263, NDCG@10: 0.7347\n",
      "Validated users: 1870, HR@10: 0.8251, NDCG@10: 0.7339\n",
      "Validated users: 1880, HR@10: 0.8255, NDCG@10: 0.7346\n",
      "Validated users: 1890, HR@10: 0.8265, NDCG@10: 0.7356\n",
      "Validated users: 1900, HR@10: 0.8268, NDCG@10: 0.7355\n",
      "Validated users: 1910, HR@10: 0.8272, NDCG@10: 0.7362\n",
      "Validated users: 1920, HR@10: 0.8266, NDCG@10: 0.7354\n",
      "Validated users: 1930, HR@10: 0.8275, NDCG@10: 0.7359\n",
      "Validated users: 1940, HR@10: 0.8273, NDCG@10: 0.7354\n",
      "Validated users: 1950, HR@10: 0.8277, NDCG@10: 0.7362\n",
      "Validated users: 1960, HR@10: 0.8281, NDCG@10: 0.7358\n",
      "Validated users: 1970, HR@10: 0.8284, NDCG@10: 0.7362\n",
      "Validated users: 1980, HR@10: 0.8283, NDCG@10: 0.7365\n",
      "Validated users: 1990, HR@10: 0.8281, NDCG@10: 0.7363\n",
      "Validated users: 2000, HR@10: 0.8290, NDCG@10: 0.7371\n",
      "Validated users: 2010, HR@10: 0.8289, NDCG@10: 0.7371\n",
      "Validated users: 2020, HR@10: 0.8282, NDCG@10: 0.7369\n",
      "Validated users: 2030, HR@10: 0.8286, NDCG@10: 0.7371\n",
      "Validated users: 2040, HR@10: 0.8279, NDCG@10: 0.7370\n",
      "Validated users: 2050, HR@10: 0.8278, NDCG@10: 0.7371\n",
      "Validated users: 2060, HR@10: 0.8257, NDCG@10: 0.7351\n",
      "Validated users: 2070, HR@10: 0.8261, NDCG@10: 0.7352\n",
      "Validated users: 2080, HR@10: 0.8269, NDCG@10: 0.7363\n",
      "Validated users: 2090, HR@10: 0.8263, NDCG@10: 0.7357\n",
      "Validated users: 2100, HR@10: 0.8271, NDCG@10: 0.7368\n",
      "Validated users: 2110, HR@10: 0.8270, NDCG@10: 0.7369\n",
      "Validated users: 2120, HR@10: 0.8269, NDCG@10: 0.7369\n",
      "Validated users: 2130, HR@10: 0.8272, NDCG@10: 0.7376\n",
      "Validated users: 2140, HR@10: 0.8280, NDCG@10: 0.7382\n",
      "Validated users: 2150, HR@10: 0.8284, NDCG@10: 0.7383\n",
      "Validated users: 2160, HR@10: 0.8282, NDCG@10: 0.7384\n",
      "Validated users: 2170, HR@10: 0.8286, NDCG@10: 0.7386\n",
      "Validated users: 2180, HR@10: 0.8289, NDCG@10: 0.7386\n",
      "Validated users: 2190, HR@10: 0.8279, NDCG@10: 0.7376\n",
      "Validated users: 2200, HR@10: 0.8286, NDCG@10: 0.7385\n",
      "Validated users: 2210, HR@10: 0.8285, NDCG@10: 0.7385\n",
      "Validated users: 2220, HR@10: 0.8288, NDCG@10: 0.7386\n",
      "Validated users: 2230, HR@10: 0.8287, NDCG@10: 0.7389\n",
      "Validated users: 2240, HR@10: 0.8290, NDCG@10: 0.7386\n",
      "Validated users: 2250, HR@10: 0.8293, NDCG@10: 0.7381\n",
      "Validated users: 2260, HR@10: 0.8296, NDCG@10: 0.7387\n",
      "Validated users: 2270, HR@10: 0.8295, NDCG@10: 0.7385\n",
      "Validated users: 2280, HR@10: 0.8289, NDCG@10: 0.7382\n",
      "Validated users: 2290, HR@10: 0.8284, NDCG@10: 0.7374\n",
      "Validated users: 2300, HR@10: 0.8287, NDCG@10: 0.7382\n",
      "Validated users: 2310, HR@10: 0.8294, NDCG@10: 0.7387\n",
      "Validated users: 2320, HR@10: 0.8297, NDCG@10: 0.7386\n",
      "Validated users: 2330, HR@10: 0.8296, NDCG@10: 0.7389\n",
      "Validated users: 2340, HR@10: 0.8295, NDCG@10: 0.7387\n",
      "Validated users: 2350, HR@10: 0.8294, NDCG@10: 0.7390\n",
      "Validated users: 2360, HR@10: 0.8284, NDCG@10: 0.7382\n",
      "Validated users: 2370, HR@10: 0.8274, NDCG@10: 0.7375\n",
      "Validated users: 2380, HR@10: 0.8269, NDCG@10: 0.7372\n",
      "Validated users: 2390, HR@10: 0.8268, NDCG@10: 0.7369\n",
      "Validated users: 2400, HR@10: 0.8267, NDCG@10: 0.7372\n",
      "Validated users: 2410, HR@10: 0.8270, NDCG@10: 0.7374\n",
      "Validated users: 2420, HR@10: 0.8264, NDCG@10: 0.7369\n",
      "Validated users: 2430, HR@10: 0.8267, NDCG@10: 0.7369\n",
      "Validated users: 2440, HR@10: 0.8262, NDCG@10: 0.7366\n",
      "Validated users: 2450, HR@10: 0.8265, NDCG@10: 0.7366\n",
      "Validated users: 2460, HR@10: 0.8268, NDCG@10: 0.7373\n",
      "Validated users: 2470, HR@10: 0.8267, NDCG@10: 0.7374\n",
      "Validated users: 2480, HR@10: 0.8266, NDCG@10: 0.7376\n",
      "Validated users: 2490, HR@10: 0.8269, NDCG@10: 0.7383\n",
      "Validated users: 2500, HR@10: 0.8268, NDCG@10: 0.7379\n",
      "Validated users: 2510, HR@10: 0.8271, NDCG@10: 0.7382\n",
      "Validated users: 2520, HR@10: 0.8270, NDCG@10: 0.7382\n",
      "Validated users: 2530, HR@10: 0.8277, NDCG@10: 0.7382\n",
      "Validated users: 2540, HR@10: 0.8276, NDCG@10: 0.7378\n",
      "Validated users: 2550, HR@10: 0.8278, NDCG@10: 0.7383\n",
      "Validated users: 2560, HR@10: 0.8281, NDCG@10: 0.7383\n",
      "Validated users: 2570, HR@10: 0.8284, NDCG@10: 0.7385\n",
      "Validated users: 2580, HR@10: 0.8287, NDCG@10: 0.7383\n",
      "Validated users: 2590, HR@10: 0.8282, NDCG@10: 0.7380\n",
      "Validated users: 2600, HR@10: 0.8285, NDCG@10: 0.7380\n",
      "Validated users: 2610, HR@10: 0.8291, NDCG@10: 0.7388\n",
      "Validated users: 2620, HR@10: 0.8294, NDCG@10: 0.7390\n",
      "Validated users: 2630, HR@10: 0.8297, NDCG@10: 0.7391\n",
      "Validated users: 2640, HR@10: 0.8299, NDCG@10: 0.7389\n",
      "Validated users: 2650, HR@10: 0.8298, NDCG@10: 0.7383\n",
      "Validated users: 2660, HR@10: 0.8305, NDCG@10: 0.7386\n",
      "Validated users: 2670, HR@10: 0.8303, NDCG@10: 0.7387\n",
      "Validated users: 2680, HR@10: 0.8306, NDCG@10: 0.7389\n",
      "Validated users: 2690, HR@10: 0.8301, NDCG@10: 0.7383\n",
      "Validated users: 2700, HR@10: 0.8307, NDCG@10: 0.7387\n",
      "Validated users: 2710, HR@10: 0.8310, NDCG@10: 0.7383\n",
      "Validated users: 2720, HR@10: 0.8313, NDCG@10: 0.7385\n",
      "Validated users: 2730, HR@10: 0.8311, NDCG@10: 0.7382\n",
      "Validated users: 2740, HR@10: 0.8310, NDCG@10: 0.7382\n",
      "Validated users: 2750, HR@10: 0.8313, NDCG@10: 0.7388\n",
      "Validated users: 2760, HR@10: 0.8308, NDCG@10: 0.7382\n",
      "Validated users: 2770, HR@10: 0.8307, NDCG@10: 0.7385\n",
      "Validated users: 2780, HR@10: 0.8309, NDCG@10: 0.7386\n",
      "Validated users: 2790, HR@10: 0.8308, NDCG@10: 0.7382\n",
      "Validated users: 2800, HR@10: 0.8314, NDCG@10: 0.7388\n",
      "Validated users: 2810, HR@10: 0.8313, NDCG@10: 0.7388\n",
      "Validated users: 2820, HR@10: 0.8312, NDCG@10: 0.7388\n",
      "Validated users: 2830, HR@10: 0.8311, NDCG@10: 0.7388\n",
      "Validated users: 2840, HR@10: 0.8313, NDCG@10: 0.7388\n",
      "Validated users: 2850, HR@10: 0.8316, NDCG@10: 0.7392\n",
      "Validated users: 2860, HR@10: 0.8308, NDCG@10: 0.7380\n",
      "Validated users: 2870, HR@10: 0.8307, NDCG@10: 0.7375\n",
      "Validated users: 2880, HR@10: 0.8302, NDCG@10: 0.7374\n",
      "Validated users: 2890, HR@10: 0.8298, NDCG@10: 0.7371\n",
      "Validated users: 2900, HR@10: 0.8303, NDCG@10: 0.7376\n",
      "Validated users: 2910, HR@10: 0.8306, NDCG@10: 0.7379\n",
      "Validated users: 2920, HR@10: 0.8301, NDCG@10: 0.7374\n",
      "Validated users: 2930, HR@10: 0.8304, NDCG@10: 0.7378\n",
      "Validated users: 2940, HR@10: 0.8299, NDCG@10: 0.7371\n",
      "Validated users: 2950, HR@10: 0.8302, NDCG@10: 0.7375\n",
      "Validated users: 2960, HR@10: 0.8301, NDCG@10: 0.7372\n",
      "Validated users: 2970, HR@10: 0.8300, NDCG@10: 0.7369\n",
      "Validated users: 2980, HR@10: 0.8305, NDCG@10: 0.7373\n",
      "Validated users: 2990, HR@10: 0.8301, NDCG@10: 0.7372\n",
      "Validated users: 3000, HR@10: 0.8297, NDCG@10: 0.7366\n",
      "Validated users: 3010, HR@10: 0.8289, NDCG@10: 0.7360\n",
      "Validated users: 3020, HR@10: 0.8288, NDCG@10: 0.7362\n",
      "Validated users: 3030, HR@10: 0.8277, NDCG@10: 0.7353\n",
      "Validated users: 3040, HR@10: 0.8283, NDCG@10: 0.7354\n",
      "Validated users: 3050, HR@10: 0.8282, NDCG@10: 0.7354\n",
      "Validated users: 3060, HR@10: 0.8288, NDCG@10: 0.7358\n",
      "Validated users: 3070, HR@10: 0.8283, NDCG@10: 0.7352\n",
      "Validated users: 3080, HR@10: 0.8279, NDCG@10: 0.7348\n",
      "Validated users: 3090, HR@10: 0.8282, NDCG@10: 0.7348\n",
      "Validated users: 3100, HR@10: 0.8277, NDCG@10: 0.7343\n",
      "Validated users: 3110, HR@10: 0.8277, NDCG@10: 0.7342\n",
      "Validated users: 3120, HR@10: 0.8279, NDCG@10: 0.7346\n",
      "Validated users: 3130, HR@10: 0.8278, NDCG@10: 0.7344\n",
      "Validated users: 3140, HR@10: 0.8274, NDCG@10: 0.7338\n",
      "Validated users: 3150, HR@10: 0.8273, NDCG@10: 0.7340\n",
      "Validated users: 3160, HR@10: 0.8272, NDCG@10: 0.7340\n",
      "Validated users: 3170, HR@10: 0.8274, NDCG@10: 0.7343\n",
      "Validated users: 3180, HR@10: 0.8277, NDCG@10: 0.7344\n",
      "Validated users: 3190, HR@10: 0.8270, NDCG@10: 0.7336\n",
      "Validated users: 3200, HR@10: 0.8266, NDCG@10: 0.7333\n",
      "Validated users: 3210, HR@10: 0.8268, NDCG@10: 0.7337\n",
      "Validated users: 3220, HR@10: 0.8267, NDCG@10: 0.7336\n",
      "Validated users: 3230, HR@10: 0.8269, NDCG@10: 0.7338\n",
      "Validated users: 3240, HR@10: 0.8269, NDCG@10: 0.7336\n",
      "Validated users: 3250, HR@10: 0.8268, NDCG@10: 0.7337\n",
      "Validated users: 3260, HR@10: 0.8264, NDCG@10: 0.7336\n",
      "Validated users: 3270, HR@10: 0.8263, NDCG@10: 0.7335\n",
      "Validated users: 3280, HR@10: 0.8259, NDCG@10: 0.7331\n",
      "Validated users: 3290, HR@10: 0.8255, NDCG@10: 0.7327\n",
      "Validated users: 3300, HR@10: 0.8248, NDCG@10: 0.7319\n",
      "Validated users: 3310, HR@10: 0.8251, NDCG@10: 0.7323\n",
      "Validated users: 3320, HR@10: 0.8256, NDCG@10: 0.7328\n",
      "Validated users: 3330, HR@10: 0.8258, NDCG@10: 0.7329\n",
      "Validated users: 3340, HR@10: 0.8254, NDCG@10: 0.7321\n",
      "Validated users: 3350, HR@10: 0.8260, NDCG@10: 0.7329\n",
      "Validated users: 3360, HR@10: 0.8253, NDCG@10: 0.7324\n",
      "Validated users: 3370, HR@10: 0.8255, NDCG@10: 0.7322\n",
      "Validated users: 3380, HR@10: 0.8257, NDCG@10: 0.7325\n",
      "Validated users: 3390, HR@10: 0.8254, NDCG@10: 0.7321\n",
      "Validated users: 3400, HR@10: 0.8259, NDCG@10: 0.7324\n",
      "Validated users: 3410, HR@10: 0.8261, NDCG@10: 0.7328\n",
      "Validated users: 3420, HR@10: 0.8260, NDCG@10: 0.7329\n",
      "Validated users: 3430, HR@10: 0.8262, NDCG@10: 0.7333\n",
      "Validated users: 3440, HR@10: 0.8259, NDCG@10: 0.7331\n",
      "Validated users: 3450, HR@10: 0.8258, NDCG@10: 0.7333\n",
      "Validated users: 3460, HR@10: 0.8254, NDCG@10: 0.7329\n",
      "Validated users: 3470, HR@10: 0.8251, NDCG@10: 0.7323\n",
      "Validated users: 3480, HR@10: 0.8253, NDCG@10: 0.7325\n",
      "Validated users: 3490, HR@10: 0.8252, NDCG@10: 0.7324\n",
      "Validated users: 3500, HR@10: 0.8249, NDCG@10: 0.7323\n",
      "Validated users: 3510, HR@10: 0.8248, NDCG@10: 0.7324\n",
      "Validated users: 3520, HR@10: 0.8247, NDCG@10: 0.7324\n",
      "Validated users: 3530, HR@10: 0.8249, NDCG@10: 0.7326\n",
      "Validated users: 3540, HR@10: 0.8246, NDCG@10: 0.7323\n",
      "Validated users: 3550, HR@10: 0.8245, NDCG@10: 0.7322\n",
      "Validated users: 3560, HR@10: 0.8244, NDCG@10: 0.7321\n",
      "Validated users: 3570, HR@10: 0.8244, NDCG@10: 0.7321\n",
      "Validated users: 3580, HR@10: 0.8240, NDCG@10: 0.7314\n",
      "Validated users: 3590, HR@10: 0.8242, NDCG@10: 0.7317\n",
      "Validated users: 3600, HR@10: 0.8242, NDCG@10: 0.7317\n",
      "Validated users: 3610, HR@10: 0.8241, NDCG@10: 0.7313\n",
      "Validated users: 3620, HR@10: 0.8240, NDCG@10: 0.7315\n",
      "Validated users: 3630, HR@10: 0.8242, NDCG@10: 0.7313\n",
      "Validated users: 3640, HR@10: 0.8239, NDCG@10: 0.7309\n",
      "Validated users: 3650, HR@10: 0.8236, NDCG@10: 0.7303\n",
      "Validated users: 3660, HR@10: 0.8238, NDCG@10: 0.7308\n",
      "Validated users: 3670, HR@10: 0.8243, NDCG@10: 0.7311\n",
      "Validated users: 3680, HR@10: 0.8239, NDCG@10: 0.7306\n",
      "Validated users: 3690, HR@10: 0.8230, NDCG@10: 0.7298\n",
      "Validated users: 3700, HR@10: 0.8232, NDCG@10: 0.7297\n",
      "Validated users: 3710, HR@10: 0.8235, NDCG@10: 0.7297\n",
      "Validated users: 3720, HR@10: 0.8239, NDCG@10: 0.7303\n",
      "Validated users: 3730, HR@10: 0.8236, NDCG@10: 0.7300\n",
      "Validated users: 3740, HR@10: 0.8233, NDCG@10: 0.7297\n",
      "Validated users: 3750, HR@10: 0.8229, NDCG@10: 0.7292\n",
      "Validated users: 3760, HR@10: 0.8231, NDCG@10: 0.7293\n",
      "Validated users: 3770, HR@10: 0.8231, NDCG@10: 0.7293\n",
      "Validated users: 3780, HR@10: 0.8228, NDCG@10: 0.7290\n",
      "Validated users: 3790, HR@10: 0.8227, NDCG@10: 0.7285\n",
      "Validated users: 3800, HR@10: 0.8226, NDCG@10: 0.7287\n",
      "Validated users: 3810, HR@10: 0.8226, NDCG@10: 0.7287\n",
      "Validated users: 3820, HR@10: 0.8223, NDCG@10: 0.7285\n",
      "Validated users: 3830, HR@10: 0.8225, NDCG@10: 0.7286\n",
      "Validated users: 3840, HR@10: 0.8229, NDCG@10: 0.7290\n",
      "Validated users: 3850, HR@10: 0.8229, NDCG@10: 0.7291\n",
      "Validated users: 3860, HR@10: 0.8233, NDCG@10: 0.7290\n",
      "Validated users: 3870, HR@10: 0.8230, NDCG@10: 0.7290\n",
      "Validated users: 3880, HR@10: 0.8232, NDCG@10: 0.7288\n",
      "Validated users: 3890, HR@10: 0.8229, NDCG@10: 0.7285\n",
      "Validated users: 3900, HR@10: 0.8233, NDCG@10: 0.7289\n",
      "Validated users: 3910, HR@10: 0.8233, NDCG@10: 0.7287\n",
      "Validated users: 3920, HR@10: 0.8232, NDCG@10: 0.7286\n",
      "Validated users: 3930, HR@10: 0.8234, NDCG@10: 0.7287\n",
      "Validated users: 3940, HR@10: 0.8226, NDCG@10: 0.7280\n",
      "Validated users: 3950, HR@10: 0.8223, NDCG@10: 0.7276\n",
      "Validated users: 3960, HR@10: 0.8225, NDCG@10: 0.7277\n",
      "Validated users: 3970, HR@10: 0.8222, NDCG@10: 0.7274\n",
      "Validated users: 3980, HR@10: 0.8224, NDCG@10: 0.7274\n",
      "Validated users: 3990, HR@10: 0.8226, NDCG@10: 0.7276\n",
      "Validated users: 4000, HR@10: 0.8230, NDCG@10: 0.7281\n",
      "Validated users: 4010, HR@10: 0.8232, NDCG@10: 0.7284\n",
      "Validated users: 4020, HR@10: 0.8234, NDCG@10: 0.7283\n",
      "Validated users: 4030, HR@10: 0.8233, NDCG@10: 0.7281\n",
      "Validated users: 4040, HR@10: 0.8238, NDCG@10: 0.7286\n",
      "Validated users: 4050, HR@10: 0.8240, NDCG@10: 0.7287\n",
      "Validated users: 4060, HR@10: 0.8234, NDCG@10: 0.7281\n",
      "Validated users: 4070, HR@10: 0.8229, NDCG@10: 0.7274\n",
      "Validated users: 4080, HR@10: 0.8233, NDCG@10: 0.7278\n",
      "Validated users: 4090, HR@10: 0.8230, NDCG@10: 0.7276\n",
      "Validated users: 4100, HR@10: 0.8234, NDCG@10: 0.7281\n",
      "Validated users: 4110, HR@10: 0.8231, NDCG@10: 0.7277\n",
      "Validated users: 4120, HR@10: 0.8231, NDCG@10: 0.7275\n",
      "Validated users: 4130, HR@10: 0.8235, NDCG@10: 0.7278\n",
      "Validated users: 4140, HR@10: 0.8237, NDCG@10: 0.7279\n",
      "Validated users: 4150, HR@10: 0.8239, NDCG@10: 0.7281\n",
      "Validated users: 4160, HR@10: 0.8233, NDCG@10: 0.7275\n",
      "Validated users: 4170, HR@10: 0.8225, NDCG@10: 0.7268\n",
      "Validated users: 4180, HR@10: 0.8227, NDCG@10: 0.7271\n",
      "Validated users: 4190, HR@10: 0.8229, NDCG@10: 0.7273\n",
      "Validated users: 4200, HR@10: 0.8229, NDCG@10: 0.7274\n",
      "Validated users: 4210, HR@10: 0.8230, NDCG@10: 0.7275\n",
      "Validated users: 4220, HR@10: 0.8235, NDCG@10: 0.7280\n",
      "Validated users: 4230, HR@10: 0.8234, NDCG@10: 0.7279\n",
      "Validated users: 4240, HR@10: 0.8233, NDCG@10: 0.7278\n",
      "Validated users: 4250, HR@10: 0.8238, NDCG@10: 0.7282\n",
      "Validated users: 4260, HR@10: 0.8232, NDCG@10: 0.7277\n",
      "Validated users: 4270, HR@10: 0.8237, NDCG@10: 0.7282\n",
      "Validated users: 4280, HR@10: 0.8238, NDCG@10: 0.7282\n",
      "Validated users: 4290, HR@10: 0.8240, NDCG@10: 0.7284\n",
      "Validated users: 4300, HR@10: 0.8242, NDCG@10: 0.7284\n",
      "Validated users: 4310, HR@10: 0.8239, NDCG@10: 0.7281\n",
      "Validated users: 4320, HR@10: 0.8241, NDCG@10: 0.7280\n",
      "Validated users: 4330, HR@10: 0.8236, NDCG@10: 0.7277\n",
      "Validated users: 4340, HR@10: 0.8235, NDCG@10: 0.7275\n",
      "Validated users: 4350, HR@10: 0.8239, NDCG@10: 0.7278\n",
      "Validated users: 4360, HR@10: 0.8243, NDCG@10: 0.7283\n",
      "Validated users: 4370, HR@10: 0.8247, NDCG@10: 0.7289\n",
      "Validated users: 4380, HR@10: 0.8247, NDCG@10: 0.7288\n",
      "Validated users: 4390, HR@10: 0.8246, NDCG@10: 0.7288\n",
      "Validated users: 4400, HR@10: 0.8245, NDCG@10: 0.7288\n",
      "Validated users: 4410, HR@10: 0.8243, NDCG@10: 0.7285\n",
      "Validated users: 4420, HR@10: 0.8244, NDCG@10: 0.7286\n",
      "Validated users: 4430, HR@10: 0.8246, NDCG@10: 0.7287\n",
      "Validated users: 4440, HR@10: 0.8245, NDCG@10: 0.7286\n",
      "Validated users: 4450, HR@10: 0.8245, NDCG@10: 0.7284\n",
      "Validated users: 4460, HR@10: 0.8244, NDCG@10: 0.7285\n",
      "Validated users: 4470, HR@10: 0.8248, NDCG@10: 0.7287\n",
      "Validated users: 4480, HR@10: 0.8248, NDCG@10: 0.7283\n",
      "Validated users: 4490, HR@10: 0.8249, NDCG@10: 0.7285\n",
      "Validated users: 4500, HR@10: 0.8251, NDCG@10: 0.7288\n",
      "Validated users: 4510, HR@10: 0.8253, NDCG@10: 0.7286\n",
      "Validated users: 4520, HR@10: 0.8254, NDCG@10: 0.7285\n",
      "Validated users: 4530, HR@10: 0.8256, NDCG@10: 0.7286\n",
      "Validated users: 4540, HR@10: 0.8249, NDCG@10: 0.7281\n",
      "Validated users: 4550, HR@10: 0.8244, NDCG@10: 0.7273\n",
      "Validated users: 4560, HR@10: 0.8243, NDCG@10: 0.7272\n",
      "Validated users: 4570, HR@10: 0.8243, NDCG@10: 0.7271\n",
      "Validated users: 4580, HR@10: 0.8242, NDCG@10: 0.7271\n",
      "Validated users: 4590, HR@10: 0.8244, NDCG@10: 0.7272\n",
      "Validated users: 4600, HR@10: 0.8248, NDCG@10: 0.7277\n",
      "Validated users: 4610, HR@10: 0.8247, NDCG@10: 0.7277\n",
      "Validated users: 4620, HR@10: 0.8249, NDCG@10: 0.7277\n",
      "Validated users: 4630, HR@10: 0.8246, NDCG@10: 0.7272\n",
      "Validated users: 4640, HR@10: 0.8246, NDCG@10: 0.7272\n",
      "Validated users: 4650, HR@10: 0.8247, NDCG@10: 0.7273\n",
      "Validated users: 4660, HR@10: 0.8251, NDCG@10: 0.7277\n",
      "Validated users: 4670, HR@10: 0.8248, NDCG@10: 0.7272\n",
      "Validated users: 4680, HR@10: 0.8252, NDCG@10: 0.7275\n",
      "Validated users: 4690, HR@10: 0.8252, NDCG@10: 0.7273\n",
      "Validated users: 4700, HR@10: 0.8253, NDCG@10: 0.7273\n",
      "Validated users: 4710, HR@10: 0.8248, NDCG@10: 0.7269\n",
      "Validated users: 4720, HR@10: 0.8244, NDCG@10: 0.7264\n",
      "Validated users: 4730, HR@10: 0.8243, NDCG@10: 0.7265\n",
      "Validated users: 4740, HR@10: 0.8243, NDCG@10: 0.7265\n",
      "Validated users: 4750, HR@10: 0.8242, NDCG@10: 0.7265\n",
      "Validated users: 4760, HR@10: 0.8244, NDCG@10: 0.7265\n",
      "Validated users: 4770, HR@10: 0.8247, NDCG@10: 0.7268\n",
      "Validated users: 4780, HR@10: 0.8249, NDCG@10: 0.7267\n",
      "Validated users: 4790, HR@10: 0.8251, NDCG@10: 0.7269\n",
      "Validated users: 4800, HR@10: 0.8250, NDCG@10: 0.7265\n",
      "Validated users: 4810, HR@10: 0.8249, NDCG@10: 0.7264\n",
      "Validated users: 4820, HR@10: 0.8251, NDCG@10: 0.7265\n",
      "Validated users: 4830, HR@10: 0.8253, NDCG@10: 0.7266\n",
      "Validated users: 4840, HR@10: 0.8254, NDCG@10: 0.7268\n",
      "Validated users: 4850, HR@10: 0.8256, NDCG@10: 0.7271\n",
      "Validated users: 4860, HR@10: 0.8255, NDCG@10: 0.7269\n",
      "Validated users: 4870, HR@10: 0.8253, NDCG@10: 0.7268\n",
      "Validated users: 4880, HR@10: 0.8252, NDCG@10: 0.7268\n",
      "Validated users: 4890, HR@10: 0.8256, NDCG@10: 0.7272\n",
      "Validated users: 4900, HR@10: 0.8251, NDCG@10: 0.7269\n",
      "Validated users: 4910, HR@10: 0.8251, NDCG@10: 0.7269\n",
      "Validated users: 4920, HR@10: 0.8254, NDCG@10: 0.7272\n",
      "Validated users: 4930, HR@10: 0.8249, NDCG@10: 0.7268\n",
      "Validated users: 4940, HR@10: 0.8251, NDCG@10: 0.7271\n",
      "Validated users: 4950, HR@10: 0.8255, NDCG@10: 0.7277\n",
      "Validated users: 4960, HR@10: 0.8254, NDCG@10: 0.7276\n",
      "Validated users: 4970, HR@10: 0.8256, NDCG@10: 0.7278\n",
      "Validated users: 4980, HR@10: 0.8255, NDCG@10: 0.7275\n",
      "Validated users: 4990, HR@10: 0.8257, NDCG@10: 0.7276\n",
      "Validated users: 5000, HR@10: 0.8254, NDCG@10: 0.7273\n",
      "Validated users: 5010, HR@10: 0.8250, NDCG@10: 0.7269\n",
      "Validated users: 5020, HR@10: 0.8251, NDCG@10: 0.7270\n",
      "Validated users: 5030, HR@10: 0.8252, NDCG@10: 0.7270\n",
      "Validated users: 5040, HR@10: 0.8252, NDCG@10: 0.7269\n",
      "Validated users: 5050, HR@10: 0.8253, NDCG@10: 0.7270\n",
      "Validated users: 5060, HR@10: 0.8253, NDCG@10: 0.7271\n",
      "Validated users: 5070, HR@10: 0.8249, NDCG@10: 0.7267\n",
      "Validated users: 5080, HR@10: 0.8248, NDCG@10: 0.7269\n",
      "Validated users: 5090, HR@10: 0.8242, NDCG@10: 0.7261\n",
      "Validated users: 5100, HR@10: 0.8241, NDCG@10: 0.7261\n",
      "Validated users: 5110, HR@10: 0.8243, NDCG@10: 0.7264\n",
      "Validated users: 5120, HR@10: 0.8242, NDCG@10: 0.7265\n",
      "Validated users: 5130, HR@10: 0.8238, NDCG@10: 0.7261\n",
      "Validated users: 5140, HR@10: 0.8239, NDCG@10: 0.7261\n",
      "Validated users: 5150, HR@10: 0.8239, NDCG@10: 0.7262\n",
      "Validated users: 5160, HR@10: 0.8242, NDCG@10: 0.7264\n",
      "Validated users: 5170, HR@10: 0.8238, NDCG@10: 0.7260\n",
      "Validated users: 5180, HR@10: 0.8239, NDCG@10: 0.7260\n",
      "Validated users: 5190, HR@10: 0.8239, NDCG@10: 0.7259\n",
      "Validated users: 5200, HR@10: 0.8240, NDCG@10: 0.7260\n",
      "Validated users: 5210, HR@10: 0.8244, NDCG@10: 0.7264\n",
      "Validated users: 5220, HR@10: 0.8243, NDCG@10: 0.7263\n",
      "Validated users: 5230, HR@10: 0.8247, NDCG@10: 0.7266\n",
      "Validated users: 5240, HR@10: 0.8248, NDCG@10: 0.7267\n",
      "Validated users: 5250, HR@10: 0.8251, NDCG@10: 0.7271\n",
      "Validated users: 5260, HR@10: 0.8249, NDCG@10: 0.7267\n",
      "Validated users: 5270, HR@10: 0.8247, NDCG@10: 0.7265\n",
      "Validated users: 5280, HR@10: 0.8246, NDCG@10: 0.7265\n",
      "Validated users: 5290, HR@10: 0.8242, NDCG@10: 0.7261\n",
      "Validated users: 5300, HR@10: 0.8240, NDCG@10: 0.7257\n",
      "Validated users: 5310, HR@10: 0.8241, NDCG@10: 0.7258\n",
      "Validated users: 5320, HR@10: 0.8241, NDCG@10: 0.7259\n",
      "Validated users: 5330, HR@10: 0.8240, NDCG@10: 0.7260\n",
      "Validated users: 5340, HR@10: 0.8242, NDCG@10: 0.7260\n",
      "Validated users: 5350, HR@10: 0.8241, NDCG@10: 0.7260\n",
      "Validated users: 5360, HR@10: 0.8243, NDCG@10: 0.7261\n",
      "Validated users: 5370, HR@10: 0.8240, NDCG@10: 0.7259\n",
      "Validated users: 5380, HR@10: 0.8242, NDCG@10: 0.7260\n",
      "Validated users: 5390, HR@10: 0.8245, NDCG@10: 0.7258\n",
      "Validated users: 5400, HR@10: 0.8243, NDCG@10: 0.7255\n",
      "Validated users: 5410, HR@10: 0.8237, NDCG@10: 0.7249\n",
      "Validated users: 5420, HR@10: 0.8240, NDCG@10: 0.7251\n",
      "Validated users: 5430, HR@10: 0.8243, NDCG@10: 0.7256\n",
      "Validated users: 5440, HR@10: 0.8244, NDCG@10: 0.7258\n",
      "Validated users: 5450, HR@10: 0.8246, NDCG@10: 0.7260\n",
      "Validated users: 5460, HR@10: 0.8245, NDCG@10: 0.7258\n",
      "Validated users: 5470, HR@10: 0.8245, NDCG@10: 0.7259\n",
      "Validated users: 5480, HR@10: 0.8243, NDCG@10: 0.7257\n",
      "Validated users: 5490, HR@10: 0.8244, NDCG@10: 0.7260\n",
      "Validated users: 5500, HR@10: 0.8245, NDCG@10: 0.7260\n",
      "Validated users: 5510, HR@10: 0.8247, NDCG@10: 0.7260\n",
      "Validated users: 5520, HR@10: 0.8248, NDCG@10: 0.7259\n",
      "Validated users: 5530, HR@10: 0.8244, NDCG@10: 0.7254\n",
      "Validated users: 5540, HR@10: 0.8244, NDCG@10: 0.7255\n",
      "Validated users: 5550, HR@10: 0.8243, NDCG@10: 0.7256\n",
      "Validated users: 5560, HR@10: 0.8243, NDCG@10: 0.7256\n",
      "Validated users: 5570, HR@10: 0.8241, NDCG@10: 0.7252\n",
      "Validated users: 5580, HR@10: 0.8242, NDCG@10: 0.7254\n",
      "Validated users: 5590, HR@10: 0.8245, NDCG@10: 0.7257\n",
      "Validated users: 5600, HR@10: 0.8245, NDCG@10: 0.7255\n",
      "Validated users: 5610, HR@10: 0.8242, NDCG@10: 0.7252\n",
      "Validated users: 5620, HR@10: 0.8244, NDCG@10: 0.7254\n",
      "Validated users: 5630, HR@10: 0.8247, NDCG@10: 0.7256\n",
      "Validated users: 5640, HR@10: 0.8248, NDCG@10: 0.7258\n",
      "Validated users: 5650, HR@10: 0.8250, NDCG@10: 0.7259\n",
      "Validated users: 5660, HR@10: 0.8249, NDCG@10: 0.7256\n",
      "Validated users: 5670, HR@10: 0.8247, NDCG@10: 0.7254\n",
      "Validated users: 5680, HR@10: 0.8250, NDCG@10: 0.7254\n",
      "Validated users: 5690, HR@10: 0.8250, NDCG@10: 0.7254\n",
      "Validated users: 5700, HR@10: 0.8247, NDCG@10: 0.7253\n",
      "Validated users: 5710, HR@10: 0.8247, NDCG@10: 0.7252\n",
      "Validated users: 5720, HR@10: 0.8248, NDCG@10: 0.7254\n",
      "Validated users: 5730, HR@10: 0.8250, NDCG@10: 0.7256\n",
      "Validated users: 5740, HR@10: 0.8249, NDCG@10: 0.7254\n",
      "Validated users: 5750, HR@10: 0.8249, NDCG@10: 0.7255\n",
      "Validated users: 5760, HR@10: 0.8248, NDCG@10: 0.7253\n",
      "Validated users: 5770, HR@10: 0.8243, NDCG@10: 0.7248\n",
      "Validated users: 5780, HR@10: 0.8244, NDCG@10: 0.7250\n",
      "Validated users: 5790, HR@10: 0.8242, NDCG@10: 0.7248\n",
      "Validated users: 5800, HR@10: 0.8241, NDCG@10: 0.7247\n",
      "Validated users: 5810, HR@10: 0.8244, NDCG@10: 0.7250\n",
      "Validated users: 5820, HR@10: 0.8242, NDCG@10: 0.7249\n",
      "Validated users: 5830, HR@10: 0.8242, NDCG@10: 0.7248\n",
      "Validated users: 5840, HR@10: 0.8238, NDCG@10: 0.7245\n",
      "Validated users: 5850, HR@10: 0.8232, NDCG@10: 0.7239\n",
      "Validated users: 5860, HR@10: 0.8234, NDCG@10: 0.7241\n",
      "Validated users: 5870, HR@10: 0.8232, NDCG@10: 0.7240\n",
      "Validated users: 5880, HR@10: 0.8233, NDCG@10: 0.7242\n",
      "Validated users: 5890, HR@10: 0.8233, NDCG@10: 0.7242\n",
      "Validated users: 5900, HR@10: 0.8234, NDCG@10: 0.7244\n",
      "Validated users: 5910, HR@10: 0.8235, NDCG@10: 0.7243\n",
      "Validated users: 5920, HR@10: 0.8238, NDCG@10: 0.7243\n",
      "Validated users: 5930, HR@10: 0.8239, NDCG@10: 0.7243\n",
      "Validated users: 5940, HR@10: 0.8242, NDCG@10: 0.7244\n",
      "Validated users: 5950, HR@10: 0.8242, NDCG@10: 0.7244\n",
      "Validated users: 5960, HR@10: 0.8242, NDCG@10: 0.7242\n",
      "Validated users: 5970, HR@10: 0.8243, NDCG@10: 0.7241\n",
      "Validated users: 5980, HR@10: 0.8241, NDCG@10: 0.7241\n",
      "Validated users: 5990, HR@10: 0.8240, NDCG@10: 0.7241\n",
      "Validated users: 6000, HR@10: 0.8240, NDCG@10: 0.7242\n",
      "Validated users: 6010, HR@10: 0.8236, NDCG@10: 0.7238\n",
      "Validated users: 6020, HR@10: 0.8234, NDCG@10: 0.7236\n",
      "Validated users: 6030, HR@10: 0.8234, NDCG@10: 0.7237\n",
      "Validated users: 6040, HR@10: 0.8237, NDCG@10: 0.7235\n",
      "HR@10: 0.8237, NDCG@10: 0.7235\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, user_dict, num_items, max_seq_length, segment_length, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
