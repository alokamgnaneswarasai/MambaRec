{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decc5906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 6040, Number of items: 3416\n",
      "Length of dataloader: 24\n",
      "Epoch 1, Loss: 3.6467\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/gnaneswaras/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py:409: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, HR@10: 0.3978, NDCG@10: 0.2055\n",
      "Epoch 2, Loss: 3.7897\n",
      "Epoch 3, Loss: 4.3596\n",
      "Epoch 4, Loss: 3.1085\n",
      "Epoch 5, Loss: 4.0256\n",
      "Epoch 6, Loss: 3.7150\n",
      "Epoch 7, Loss: 2.4765\n",
      "Epoch 8, Loss: 2.1553\n",
      "Epoch 9, Loss: 4.0136\n",
      "Epoch 10, Loss: 1.5421\n",
      "Epoch 11, Loss: 3.7231\n",
      "Evaluating...\n",
      "Epoch 11, HR@10: 0.4579, NDCG@10: 0.2494\n",
      "Epoch 12, Loss: 3.3866\n",
      "Epoch 13, Loss: 4.0010\n",
      "Epoch 14, Loss: 4.6089\n",
      "Epoch 15, Loss: 5.5257\n",
      "Epoch 16, Loss: 5.2468\n",
      "Epoch 17, Loss: 4.2982\n",
      "Epoch 18, Loss: 4.6237\n",
      "Epoch 19, Loss: 3.7032\n",
      "Epoch 20, Loss: 3.7152\n",
      "Epoch 21, Loss: 4.6244\n",
      "Evaluating...\n",
      "Epoch 21, HR@10: 0.4576, NDCG@10: 0.2528\n",
      "Epoch 22, Loss: 3.6947\n",
      "Epoch 23, Loss: 4.3166\n",
      "Epoch 24, Loss: 3.9880\n",
      "Epoch 25, Loss: 3.0766\n",
      "Epoch 26, Loss: 3.3759\n",
      "Epoch 27, Loss: 4.9337\n",
      "Epoch 28, Loss: 1.5473\n",
      "Epoch 29, Loss: 2.1576\n",
      "Epoch 30, Loss: 3.3889\n",
      "Epoch 31, Loss: 3.0728\n",
      "Evaluating...\n",
      "Epoch 31, HR@10: 0.4679, NDCG@10: 0.2583\n",
      "Epoch 32, Loss: 3.6999\n",
      "Epoch 33, Loss: 4.3043\n",
      "Epoch 34, Loss: 3.6822\n",
      "Epoch 35, Loss: 3.7041\n",
      "Epoch 36, Loss: 3.9961\n",
      "Epoch 37, Loss: 3.0728\n",
      "Epoch 38, Loss: 3.0635\n",
      "Epoch 39, Loss: 4.3038\n",
      "Epoch 40, Loss: 3.3794\n",
      "Epoch 41, Loss: 3.3752\n",
      "Evaluating...\n",
      "Epoch 41, HR@10: 0.4609, NDCG@10: 0.2525\n",
      "Epoch 42, Loss: 2.7713\n",
      "Epoch 43, Loss: 3.6957\n",
      "Epoch 44, Loss: 3.3902\n",
      "Epoch 45, Loss: 4.0090\n",
      "Epoch 46, Loss: 4.0107\n",
      "Epoch 47, Loss: 3.6851\n",
      "Epoch 48, Loss: 3.3796\n",
      "Epoch 49, Loss: 2.4499\n",
      "Epoch 50, Loss: 2.7619\n",
      "Epoch 51, Loss: 5.8426\n",
      "Evaluating...\n",
      "Epoch 51, HR@10: 0.4629, NDCG@10: 0.2575\n",
      "Epoch 52, Loss: 2.4505\n",
      "Epoch 53, Loss: 3.6816\n",
      "Epoch 54, Loss: 4.6083\n",
      "Epoch 55, Loss: 3.6835\n",
      "Epoch 56, Loss: 3.6843\n",
      "Epoch 57, Loss: 3.0765\n",
      "Epoch 58, Loss: 3.9958\n",
      "Epoch 59, Loss: 3.6769\n",
      "Epoch 60, Loss: 3.6952\n",
      "Epoch 61, Loss: 2.7888\n",
      "Evaluating...\n",
      "Epoch 61, HR@10: 0.4594, NDCG@10: 0.2527\n",
      "Epoch 62, Loss: 3.6833\n",
      "Epoch 63, Loss: 3.0713\n",
      "Epoch 64, Loss: 2.7667\n",
      "Epoch 65, Loss: 5.5488\n",
      "Epoch 66, Loss: 2.4535\n",
      "Epoch 67, Loss: 2.4682\n",
      "Epoch 68, Loss: 4.9365\n",
      "Epoch 69, Loss: 3.7013\n",
      "Epoch 70, Loss: 3.0573\n",
      "Epoch 71, Loss: 2.7711\n",
      "Evaluating...\n",
      "Epoch 71, HR@10: 0.4671, NDCG@10: 0.2568\n",
      "Epoch 72, Loss: 3.7215\n",
      "Epoch 73, Loss: 3.7001\n",
      "Epoch 74, Loss: 2.4490\n",
      "Epoch 75, Loss: 3.6864\n",
      "Epoch 76, Loss: 4.6285\n",
      "Epoch 77, Loss: 3.0791\n",
      "Epoch 78, Loss: 3.6997\n",
      "Epoch 79, Loss: 3.6779\n",
      "Epoch 80, Loss: 2.7779\n",
      "Epoch 81, Loss: 3.0845\n",
      "Evaluating...\n",
      "Epoch 81, HR@10: 0.4659, NDCG@10: 0.2576\n",
      "Epoch 82, Loss: 4.3186\n",
      "Epoch 83, Loss: 4.2984\n",
      "Epoch 84, Loss: 4.0011\n",
      "Epoch 85, Loss: 5.5360\n",
      "Epoch 86, Loss: 2.4791\n",
      "Epoch 87, Loss: 4.3092\n",
      "Epoch 88, Loss: 3.6717\n",
      "Epoch 89, Loss: 4.5893\n",
      "Epoch 90, Loss: 4.3111\n",
      "Epoch 91, Loss: 4.2845\n",
      "Evaluating...\n",
      "Epoch 91, HR@10: 0.4614, NDCG@10: 0.2545\n",
      "Epoch 92, Loss: 2.7623\n",
      "Epoch 93, Loss: 2.4495\n",
      "Epoch 94, Loss: 2.1451\n",
      "Epoch 95, Loss: 3.3832\n",
      "Epoch 96, Loss: 2.1398\n",
      "Epoch 97, Loss: 4.9351\n",
      "Epoch 98, Loss: 4.3128\n",
      "Epoch 99, Loss: 3.6657\n",
      "Epoch 100, Loss: 3.6898\n",
      "Epoch 101, Loss: 3.3444\n",
      "Evaluating...\n",
      "Epoch 101, HR@10: 0.4619, NDCG@10: 0.2543\n",
      "Epoch 102, Loss: 2.7502\n",
      "Epoch 103, Loss: 3.0868\n",
      "Epoch 104, Loss: 4.3119\n",
      "Epoch 105, Loss: 3.9852\n",
      "Epoch 106, Loss: 4.3154\n",
      "Epoch 107, Loss: 4.2991\n",
      "Epoch 108, Loss: 3.7126\n",
      "Epoch 109, Loss: 4.8896\n",
      "Epoch 110, Loss: 3.3780\n",
      "Epoch 111, Loss: 3.0721\n",
      "Evaluating...\n",
      "Epoch 111, HR@10: 0.4699, NDCG@10: 0.2565\n",
      "Epoch 112, Loss: 3.9972\n",
      "Epoch 113, Loss: 3.6882\n",
      "Epoch 114, Loss: 3.6830\n",
      "Epoch 115, Loss: 3.3772\n",
      "Epoch 116, Loss: 2.1590\n",
      "Epoch 117, Loss: 2.1301\n",
      "Epoch 118, Loss: 2.1672\n",
      "Epoch 119, Loss: 2.7454\n",
      "Epoch 120, Loss: 3.3776\n",
      "Epoch 121, Loss: 2.7620\n",
      "Evaluating...\n",
      "Epoch 121, HR@10: 0.4619, NDCG@10: 0.2525\n",
      "Epoch 122, Loss: 4.0008\n",
      "Epoch 123, Loss: 4.2964\n",
      "Epoch 124, Loss: 3.0712\n",
      "Epoch 125, Loss: 5.2241\n",
      "Epoch 126, Loss: 4.5888\n",
      "Epoch 127, Loss: 4.3039\n",
      "Epoch 128, Loss: 4.0030\n",
      "Epoch 129, Loss: 3.3495\n",
      "Epoch 130, Loss: 2.4700\n",
      "Epoch 131, Loss: 3.3704\n",
      "Evaluating...\n",
      "Epoch 131, HR@10: 0.4656, NDCG@10: 0.2574\n",
      "Epoch 132, Loss: 4.6140\n",
      "Epoch 133, Loss: 3.6884\n",
      "Epoch 134, Loss: 3.6558\n",
      "Epoch 135, Loss: 4.0204\n",
      "Epoch 136, Loss: 2.4593\n",
      "Epoch 137, Loss: 3.3811\n",
      "Epoch 138, Loss: 1.5435\n",
      "Epoch 139, Loss: 2.4658\n",
      "Epoch 140, Loss: 4.6306\n",
      "Epoch 141, Loss: 3.0765\n",
      "Evaluating...\n",
      "Epoch 141, HR@10: 0.4687, NDCG@10: 0.2581\n",
      "Epoch 142, Loss: 3.6779\n",
      "Epoch 143, Loss: 3.9897\n",
      "Epoch 144, Loss: 4.2881\n",
      "Epoch 145, Loss: 3.3868\n",
      "Epoch 146, Loss: 3.0704\n",
      "Epoch 147, Loss: 3.6817\n",
      "Epoch 148, Loss: 2.7602\n",
      "Epoch 149, Loss: 3.9794\n",
      "Epoch 150, Loss: 3.3976\n",
      "Epoch 151, Loss: 4.2936\n",
      "Evaluating...\n",
      "Epoch 151, HR@10: 0.4675, NDCG@10: 0.2551\n",
      "Epoch 152, Loss: 2.7743\n",
      "Epoch 153, Loss: 3.3797\n",
      "Epoch 154, Loss: 4.2856\n",
      "Epoch 155, Loss: 4.3048\n",
      "Epoch 156, Loss: 3.3876\n",
      "Epoch 157, Loss: 4.5993\n",
      "Epoch 158, Loss: 3.0738\n",
      "Epoch 159, Loss: 3.0709\n",
      "Epoch 160, Loss: 2.4678\n",
      "Epoch 161, Loss: 2.4700\n",
      "Evaluating...\n",
      "Epoch 161, HR@10: 0.4694, NDCG@10: 0.2591\n",
      "Epoch 162, Loss: 4.3144\n",
      "Epoch 163, Loss: 3.3819\n",
      "Epoch 164, Loss: 2.7674\n",
      "Epoch 165, Loss: 3.0815\n",
      "Epoch 166, Loss: 3.9879\n",
      "Epoch 167, Loss: 2.1643\n",
      "Epoch 168, Loss: 2.7716\n",
      "Epoch 169, Loss: 3.0690\n",
      "Epoch 170, Loss: 1.8445\n",
      "Epoch 171, Loss: 3.3563\n",
      "Evaluating...\n",
      "Epoch 171, HR@10: 0.4707, NDCG@10: 0.2584\n",
      "Epoch 172, Loss: 4.2983\n",
      "Epoch 173, Loss: 2.1569\n",
      "Epoch 174, Loss: 4.9081\n",
      "Epoch 175, Loss: 4.9285\n",
      "Epoch 176, Loss: 4.3155\n",
      "Epoch 177, Loss: 3.3954\n",
      "Epoch 178, Loss: 3.3750\n",
      "Epoch 179, Loss: 2.4494\n",
      "Epoch 180, Loss: 4.9112\n",
      "Epoch 181, Loss: 2.4587\n",
      "Evaluating...\n",
      "Epoch 181, HR@10: 0.4654, NDCG@10: 0.2593\n",
      "Epoch 182, Loss: 5.2181\n",
      "Epoch 183, Loss: 4.3033\n",
      "Epoch 184, Loss: 3.0915\n",
      "Epoch 185, Loss: 3.9789\n",
      "Epoch 186, Loss: 3.6942\n",
      "Epoch 187, Loss: 3.9840\n",
      "Epoch 188, Loss: 2.4524\n",
      "Epoch 189, Loss: 3.3790\n",
      "Epoch 190, Loss: 4.3171\n",
      "Epoch 191, Loss: 3.3774\n",
      "Evaluating...\n",
      "Epoch 191, HR@10: 0.4675, NDCG@10: 0.2586\n",
      "Epoch 192, Loss: 6.4132\n",
      "Epoch 193, Loss: 3.6812\n",
      "Epoch 194, Loss: 2.4580\n",
      "Epoch 195, Loss: 3.6640\n",
      "Epoch 196, Loss: 3.9898\n",
      "Epoch 197, Loss: 3.0578\n",
      "Epoch 198, Loss: 3.3627\n",
      "Epoch 199, Loss: 3.6699\n",
      "Epoch 200, Loss: 2.7667\n",
      "Epoch 201, Loss: 3.3751\n",
      "Evaluating...\n",
      "Epoch 201, HR@10: 0.4694, NDCG@10: 0.2582\n",
      "Epoch 202, Loss: 3.6805\n",
      "Epoch 203, Loss: 3.3559\n",
      "Epoch 204, Loss: 3.6649\n",
      "Epoch 205, Loss: 3.9951\n",
      "Epoch 206, Loss: 4.9117\n",
      "Epoch 207, Loss: 4.5929\n",
      "Epoch 208, Loss: 3.3782\n",
      "Epoch 209, Loss: 3.6992\n",
      "Epoch 210, Loss: 3.3706\n",
      "Epoch 211, Loss: 3.0658\n",
      "Evaluating...\n",
      "Epoch 211, HR@10: 0.4674, NDCG@10: 0.2547\n",
      "Epoch 212, Loss: 4.2813\n",
      "Epoch 213, Loss: 4.0264\n",
      "Epoch 214, Loss: 3.0659\n",
      "Epoch 215, Loss: 3.9761\n",
      "Epoch 216, Loss: 3.0722\n",
      "Epoch 217, Loss: 2.1488\n",
      "Epoch 218, Loss: 3.3991\n",
      "Epoch 219, Loss: 4.3012\n",
      "Epoch 220, Loss: 3.3881\n",
      "Epoch 221, Loss: 3.3718\n",
      "Evaluating...\n",
      "Epoch 221, HR@10: 0.4647, NDCG@10: 0.2538\n",
      "Epoch 222, Loss: 3.6955\n",
      "Epoch 223, Loss: 3.9804\n",
      "Epoch 224, Loss: 3.9925\n",
      "Epoch 225, Loss: 3.6973\n",
      "Epoch 226, Loss: 3.7065\n",
      "Epoch 227, Loss: 4.6089\n",
      "Epoch 228, Loss: 4.3027\n",
      "Epoch 229, Loss: 5.2161\n",
      "Epoch 230, Loss: 2.4533\n",
      "Epoch 231, Loss: 3.9787\n",
      "Evaluating...\n",
      "Epoch 231, HR@10: 0.4689, NDCG@10: 0.2575\n",
      "Epoch 232, Loss: 2.7649\n",
      "Epoch 233, Loss: 3.6864\n",
      "Epoch 234, Loss: 2.4603\n",
      "Epoch 235, Loss: 3.3779\n",
      "Epoch 236, Loss: 3.6889\n",
      "Epoch 237, Loss: 3.3737\n",
      "Epoch 238, Loss: 2.7609\n",
      "Epoch 239, Loss: 2.7757\n",
      "Epoch 240, Loss: 3.6878\n",
      "Epoch 241, Loss: 2.4600\n",
      "Evaluating...\n",
      "Epoch 241, HR@10: 0.4685, NDCG@10: 0.2590\n",
      "Epoch 242, Loss: 3.6906\n",
      "Epoch 243, Loss: 4.6080\n",
      "Epoch 244, Loss: 2.4499\n",
      "Epoch 245, Loss: 4.9034\n",
      "Epoch 246, Loss: 3.0660\n",
      "Epoch 247, Loss: 4.3093\n",
      "Epoch 248, Loss: 3.0559\n",
      "Epoch 249, Loss: 3.0601\n",
      "Epoch 250, Loss: 3.7101\n",
      "Epoch 251, Loss: 4.0150\n",
      "Evaluating...\n",
      "Epoch 251, HR@10: 0.4667, NDCG@10: 0.2611\n",
      "Epoch 252, Loss: 3.3890\n",
      "Epoch 253, Loss: 2.7692\n",
      "Epoch 254, Loss: 3.3567\n",
      "Epoch 255, Loss: 4.2779\n",
      "Epoch 256, Loss: 3.3723\n",
      "Epoch 257, Loss: 3.6811\n",
      "Epoch 258, Loss: 3.3480\n",
      "Epoch 259, Loss: 4.3017\n",
      "Epoch 260, Loss: 3.9810\n",
      "Epoch 261, Loss: 3.6852\n",
      "Evaluating...\n",
      "Epoch 261, HR@10: 0.4652, NDCG@10: 0.2551\n",
      "Epoch 262, Loss: 3.0714\n",
      "Epoch 263, Loss: 3.6838\n",
      "Epoch 264, Loss: 4.3110\n",
      "Epoch 265, Loss: 2.1591\n",
      "Epoch 266, Loss: 3.9828\n",
      "Epoch 267, Loss: 3.3678\n",
      "Epoch 268, Loss: 3.0428\n",
      "Epoch 269, Loss: 4.3097\n",
      "Epoch 270, Loss: 3.6893\n",
      "Epoch 271, Loss: 3.0933\n",
      "Evaluating...\n",
      "Epoch 271, HR@10: 0.4624, NDCG@10: 0.2568\n",
      "Epoch 272, Loss: 2.4607\n",
      "Epoch 273, Loss: 3.3691\n",
      "Epoch 274, Loss: 2.7506\n",
      "Epoch 275, Loss: 4.3038\n",
      "Epoch 276, Loss: 3.0621\n",
      "Epoch 277, Loss: 2.7777\n",
      "Epoch 278, Loss: 3.6953\n",
      "Epoch 279, Loss: 2.4523\n",
      "Epoch 280, Loss: 2.4395\n",
      "Epoch 281, Loss: 3.3962\n",
      "Evaluating...\n",
      "Epoch 281, HR@10: 0.4753, NDCG@10: 0.2569\n",
      "Epoch 282, Loss: 4.3094\n",
      "Epoch 283, Loss: 2.7362\n",
      "Epoch 284, Loss: 4.0107\n",
      "Epoch 285, Loss: 3.0738\n",
      "Epoch 286, Loss: 2.7637\n",
      "Epoch 287, Loss: 4.9027\n",
      "Epoch 288, Loss: 3.0788\n",
      "Epoch 289, Loss: 2.7488\n",
      "Epoch 290, Loss: 2.7584\n",
      "Epoch 291, Loss: 3.6913\n",
      "Evaluating...\n",
      "Epoch 291, HR@10: 0.4641, NDCG@10: 0.2535\n",
      "Epoch 292, Loss: 4.2978\n",
      "Epoch 293, Loss: 4.0070\n",
      "Epoch 294, Loss: 3.6610\n",
      "Epoch 295, Loss: 3.3779\n",
      "Epoch 296, Loss: 3.6753\n",
      "Epoch 297, Loss: 3.0743\n",
      "Epoch 298, Loss: 3.6689\n",
      "Epoch 299, Loss: 3.6942\n",
      "Epoch 300, Loss: 2.7762\n",
      "Epoch 301, Loss: 2.7573\n",
      "Evaluating...\n",
      "Epoch 301, HR@10: 0.4695, NDCG@10: 0.2602\n",
      "Epoch 302, Loss: 3.3760\n",
      "Epoch 303, Loss: 2.7747\n",
      "Epoch 304, Loss: 5.2298\n",
      "Epoch 305, Loss: 3.3695\n",
      "Epoch 306, Loss: 3.9788\n",
      "Epoch 307, Loss: 2.4653\n",
      "Epoch 308, Loss: 3.9963\n",
      "Epoch 309, Loss: 4.2966\n",
      "Epoch 310, Loss: 3.6912\n",
      "Epoch 311, Loss: 3.7066\n",
      "Evaluating...\n",
      "Epoch 311, HR@10: 0.4664, NDCG@10: 0.2575\n",
      "Epoch 312, Loss: 3.3697\n",
      "Epoch 313, Loss: 3.6715\n",
      "Epoch 314, Loss: 4.3000\n",
      "Epoch 315, Loss: 4.6097\n",
      "Epoch 316, Loss: 3.3900\n",
      "Epoch 317, Loss: 3.7140\n",
      "Epoch 318, Loss: 2.1687\n",
      "Epoch 319, Loss: 2.4413\n",
      "Epoch 320, Loss: 2.1263\n",
      "Epoch 321, Loss: 3.6807\n",
      "Evaluating...\n",
      "Epoch 321, HR@10: 0.4697, NDCG@10: 0.2601\n",
      "Epoch 322, Loss: 3.6937\n",
      "Epoch 323, Loss: 3.3863\n",
      "Epoch 324, Loss: 3.6987\n",
      "Epoch 325, Loss: 3.9872\n",
      "Epoch 326, Loss: 3.9907\n",
      "Epoch 327, Loss: 4.2728\n",
      "Epoch 328, Loss: 4.6180\n",
      "Epoch 329, Loss: 4.6035\n",
      "Epoch 330, Loss: 2.7555\n",
      "Epoch 331, Loss: 4.2772\n",
      "Evaluating...\n",
      "Epoch 331, HR@10: 0.4672, NDCG@10: 0.2574\n",
      "Epoch 332, Loss: 3.6754\n",
      "Epoch 333, Loss: 3.0611\n",
      "Epoch 334, Loss: 3.9805\n",
      "Epoch 335, Loss: 3.6865\n",
      "Epoch 336, Loss: 3.3764\n",
      "Epoch 337, Loss: 3.6666\n",
      "Epoch 338, Loss: 3.0569\n",
      "Epoch 339, Loss: 4.8998\n",
      "Epoch 340, Loss: 2.7576\n",
      "Epoch 341, Loss: 3.6727\n",
      "Evaluating...\n",
      "Epoch 341, HR@10: 0.4699, NDCG@10: 0.2593\n",
      "Epoch 342, Loss: 4.8830\n",
      "Epoch 343, Loss: 3.0669\n",
      "Epoch 344, Loss: 3.9903\n",
      "Epoch 345, Loss: 3.6884\n",
      "Epoch 346, Loss: 2.1474\n",
      "Epoch 347, Loss: 3.3652\n",
      "Epoch 348, Loss: 2.7738\n",
      "Epoch 349, Loss: 2.7655\n",
      "Epoch 350, Loss: 3.0573\n",
      "Epoch 351, Loss: 3.0737\n",
      "Evaluating...\n",
      "Epoch 351, HR@10: 0.4705, NDCG@10: 0.2583\n",
      "Epoch 352, Loss: 3.6914\n",
      "Epoch 353, Loss: 4.9274\n",
      "Epoch 354, Loss: 3.6695\n",
      "Epoch 355, Loss: 5.2226\n",
      "Epoch 356, Loss: 3.6976\n",
      "Epoch 357, Loss: 2.4542\n",
      "Epoch 358, Loss: 3.3833\n",
      "Epoch 359, Loss: 3.9897\n",
      "Epoch 360, Loss: 3.6915\n",
      "Epoch 361, Loss: 4.3067\n",
      "Evaluating...\n",
      "Epoch 361, HR@10: 0.4682, NDCG@10: 0.2583\n",
      "Epoch 362, Loss: 3.3897\n",
      "Epoch 363, Loss: 3.6925\n",
      "Epoch 364, Loss: 3.6753\n",
      "Epoch 365, Loss: 4.9247\n",
      "Epoch 366, Loss: 3.3735\n",
      "Epoch 367, Loss: 3.9853\n",
      "Epoch 368, Loss: 4.0116\n",
      "Epoch 369, Loss: 3.6790\n",
      "Epoch 370, Loss: 3.0705\n",
      "Epoch 371, Loss: 3.3624\n",
      "Evaluating...\n",
      "Epoch 371, HR@10: 0.4634, NDCG@10: 0.2578\n",
      "Epoch 372, Loss: 1.5336\n",
      "Epoch 373, Loss: 2.4624\n",
      "Epoch 374, Loss: 2.7601\n",
      "Epoch 375, Loss: 3.6789\n",
      "Epoch 376, Loss: 3.0767\n",
      "Epoch 377, Loss: 3.7003\n",
      "Epoch 378, Loss: 3.0780\n",
      "Epoch 379, Loss: 3.9712\n",
      "Epoch 380, Loss: 3.6594\n",
      "Epoch 381, Loss: 3.3757\n",
      "Evaluating...\n",
      "Epoch 381, HR@10: 0.4656, NDCG@10: 0.2584\n",
      "Epoch 382, Loss: 4.0025\n",
      "Epoch 383, Loss: 3.3770\n",
      "Epoch 384, Loss: 4.3149\n",
      "Epoch 385, Loss: 4.0010\n",
      "Epoch 386, Loss: 3.0591\n",
      "Epoch 387, Loss: 4.3029\n",
      "Epoch 388, Loss: 4.9197\n",
      "Epoch 389, Loss: 4.2885\n",
      "Epoch 390, Loss: 2.7496\n",
      "Epoch 391, Loss: 2.4521\n",
      "Evaluating...\n",
      "Epoch 391, HR@10: 0.4656, NDCG@10: 0.2586\n",
      "Epoch 392, Loss: 4.3073\n",
      "Epoch 393, Loss: 2.4542\n",
      "Epoch 394, Loss: 3.6944\n",
      "Epoch 395, Loss: 4.9110\n",
      "Epoch 396, Loss: 4.3101\n",
      "Epoch 397, Loss: 4.5639\n",
      "Epoch 398, Loss: 4.6389\n",
      "Epoch 399, Loss: 3.6755\n",
      "Epoch 400, Loss: 3.3737\n",
      "Epoch 401, Loss: 3.3651\n",
      "Evaluating...\n",
      "Epoch 401, HR@10: 0.4623, NDCG@10: 0.2584\n",
      "Epoch 402, Loss: 4.3132\n",
      "Epoch 403, Loss: 3.0783\n",
      "Epoch 404, Loss: 4.0046\n",
      "Epoch 405, Loss: 2.4436\n",
      "Epoch 406, Loss: 3.0799\n",
      "Epoch 407, Loss: 3.9788\n",
      "Epoch 408, Loss: 3.9996\n",
      "Epoch 409, Loss: 3.3872\n",
      "Epoch 410, Loss: 2.4466\n",
      "Epoch 411, Loss: 2.4489\n",
      "Evaluating...\n",
      "Epoch 411, HR@10: 0.4689, NDCG@10: 0.2548\n",
      "Epoch 412, Loss: 3.9740\n",
      "Epoch 413, Loss: 4.5856\n",
      "Epoch 414, Loss: 3.9949\n",
      "Epoch 415, Loss: 2.7531\n",
      "Epoch 416, Loss: 4.2841\n",
      "Epoch 417, Loss: 2.1692\n",
      "Epoch 418, Loss: 2.7701\n",
      "Epoch 419, Loss: 3.9999\n",
      "Epoch 420, Loss: 3.6775\n",
      "Epoch 421, Loss: 2.1568\n",
      "Evaluating...\n",
      "Epoch 421, HR@10: 0.4704, NDCG@10: 0.2617\n",
      "Epoch 422, Loss: 3.9661\n",
      "Epoch 423, Loss: 2.7437\n",
      "Epoch 424, Loss: 4.6390\n",
      "Epoch 425, Loss: 5.2313\n",
      "Epoch 426, Loss: 4.6326\n",
      "Epoch 427, Loss: 2.7715\n",
      "Epoch 428, Loss: 3.6736\n",
      "Epoch 429, Loss: 4.0016\n",
      "Epoch 430, Loss: 4.3095\n",
      "Epoch 431, Loss: 2.7564\n",
      "Evaluating...\n",
      "Epoch 431, HR@10: 0.4679, NDCG@10: 0.2601\n",
      "Epoch 432, Loss: 3.6847\n",
      "Epoch 433, Loss: 4.6087\n",
      "Epoch 434, Loss: 3.3937\n",
      "Epoch 435, Loss: 3.3553\n",
      "Epoch 436, Loss: 3.0873\n",
      "Epoch 437, Loss: 2.4590\n",
      "Epoch 438, Loss: 2.4589\n",
      "Epoch 439, Loss: 3.6999\n",
      "Epoch 440, Loss: 2.4510\n",
      "Epoch 441, Loss: 3.3656\n",
      "Evaluating...\n",
      "Epoch 441, HR@10: 0.4651, NDCG@10: 0.2553\n",
      "Epoch 442, Loss: 1.5444\n",
      "Epoch 443, Loss: 3.3751\n",
      "Epoch 444, Loss: 3.6940\n",
      "Epoch 445, Loss: 3.3679\n",
      "Epoch 446, Loss: 4.6302\n",
      "Epoch 447, Loss: 3.6986\n",
      "Epoch 448, Loss: 3.6887\n",
      "Epoch 449, Loss: 3.6793\n",
      "Epoch 450, Loss: 3.6571\n",
      "Epoch 451, Loss: 4.6348\n",
      "Evaluating...\n",
      "Epoch 451, HR@10: 0.4692, NDCG@10: 0.2576\n",
      "Epoch 452, Loss: 3.3720\n",
      "Epoch 453, Loss: 3.6827\n",
      "Epoch 454, Loss: 2.4445\n",
      "Epoch 455, Loss: 3.9910\n",
      "Epoch 456, Loss: 4.6135\n",
      "Epoch 457, Loss: 2.4512\n",
      "Epoch 458, Loss: 2.7672\n",
      "Epoch 459, Loss: 3.3616\n",
      "Epoch 460, Loss: 2.4460\n",
      "Epoch 461, Loss: 3.0974\n",
      "Evaluating...\n",
      "Epoch 461, HR@10: 0.4695, NDCG@10: 0.2588\n",
      "Epoch 462, Loss: 4.0160\n",
      "Epoch 463, Loss: 3.9866\n",
      "Epoch 464, Loss: 3.6794\n",
      "Epoch 465, Loss: 3.3666\n",
      "Epoch 466, Loss: 3.0676\n",
      "Epoch 467, Loss: 2.7622\n",
      "Epoch 468, Loss: 4.6193\n",
      "Epoch 469, Loss: 3.3973\n",
      "Epoch 470, Loss: 4.2785\n",
      "Epoch 471, Loss: 3.9732\n",
      "Evaluating...\n",
      "Epoch 471, HR@10: 0.4631, NDCG@10: 0.2570\n",
      "Epoch 472, Loss: 3.0567\n",
      "Epoch 473, Loss: 3.3857\n",
      "Epoch 474, Loss: 2.1613\n",
      "Epoch 475, Loss: 4.3096\n",
      "Epoch 476, Loss: 3.9789\n",
      "Epoch 477, Loss: 3.3825\n",
      "Epoch 478, Loss: 4.2733\n",
      "Epoch 479, Loss: 3.3545\n",
      "Epoch 480, Loss: 3.3929\n",
      "Epoch 481, Loss: 2.4439\n",
      "Evaluating...\n",
      "Epoch 481, HR@10: 0.4695, NDCG@10: 0.2582\n",
      "Epoch 482, Loss: 3.0665\n",
      "Epoch 483, Loss: 3.0714\n",
      "Epoch 484, Loss: 4.6121\n",
      "Epoch 485, Loss: 2.4431\n",
      "Epoch 486, Loss: 3.9850\n",
      "Epoch 487, Loss: 3.0526\n",
      "Epoch 488, Loss: 2.4462\n",
      "Epoch 489, Loss: 4.5971\n",
      "Epoch 490, Loss: 3.9786\n",
      "Epoch 491, Loss: 3.3700\n",
      "Evaluating...\n",
      "Epoch 491, HR@10: 0.4642, NDCG@10: 0.2564\n",
      "Epoch 492, Loss: 3.6817\n",
      "Epoch 493, Loss: 4.0021\n",
      "Epoch 494, Loss: 4.2940\n",
      "Epoch 495, Loss: 3.3622\n",
      "Epoch 496, Loss: 4.6117\n",
      "Epoch 497, Loss: 4.5925\n",
      "Epoch 498, Loss: 2.7572\n",
      "Epoch 499, Loss: 2.4505\n",
      "Epoch 500, Loss: 3.3826\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "max_seq_length = 200\n",
    "window_size = 10\n",
    "chunk_size = 10\n",
    "batch_size = 256\n",
    "num_epochs = 500\n",
    "learning_rate = 0.0025\n",
    "mask_prob = 0.15\n",
    "num_masks_per_batch = 5\n",
    "num_negatives = 99\n",
    "\n",
    "# Custom collation function\n",
    "def custom_collate_fn(batch):\n",
    "    seqs = torch.stack([item[\"seq\"] for item in batch])\n",
    "    return {\"seq\": seqs}\n",
    "\n",
    "# Dataset class\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, user_dict, num_items, max_seq_length):\n",
    "        self.user_dict = user_dict\n",
    "        self.num_items = num_items\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.users = list(user_dict.keys())\n",
    "        self.precomputed = self.precompute_sequences()\n",
    "\n",
    "    def precompute_sequences(self):\n",
    "        precomputed = {}\n",
    "        for user in self.users:\n",
    "            seq = self.user_dict[user][:self.max_seq_length]\n",
    "            if len(seq) < 2:\n",
    "                seq = [0] * self.max_seq_length\n",
    "            else:\n",
    "                seq = seq + [0] * (self.max_seq_length - len(seq)) if len(seq) < self.max_seq_length else seq\n",
    "            precomputed[user] = torch.tensor(seq, dtype=torch.long)\n",
    "        return precomputed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.users[idx]\n",
    "        seq = self.precomputed[user].clone()\n",
    "        return {\"seq\": seq}\n",
    "\n",
    "# Model\n",
    "class SequentialRecommender(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim, num_heads, num_layers, dropout, window_size):\n",
    "        super(SequentialRecommender, self).__init__()\n",
    "        self.num_items = num_items\n",
    "        self.window_size = window_size\n",
    "        self.embedding = nn.Embedding(num_items + 10000, embedding_dim, padding_idx=0)\n",
    "        self.pos_encoding = nn.Parameter(self.create_pos_encoding(5000, embedding_dim))\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=embedding_dim * 4,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embedding_dim, num_items)\n",
    "\n",
    "    def create_pos_encoding(self, max_len, dim):\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "    def compress_sequence(self, seq, mask_positions):\n",
    "        batch_size, seq_len = seq.shape\n",
    "        windows = []\n",
    "        for pos in mask_positions:\n",
    "            start = max(0, pos - self.window_size)\n",
    "            end = min(seq_len, pos + self.window_size + 1)\n",
    "            windows.append((start, end))\n",
    "\n",
    "        merged = []\n",
    "        if windows:\n",
    "            current_start, current_end = windows[0]\n",
    "            for start, end in windows[1:]:\n",
    "                if start <= current_end:\n",
    "                    current_end = max(current_end, end)\n",
    "                else:\n",
    "                    merged.append((current_start, current_end))\n",
    "                    current_start, current_end = start, end\n",
    "            merged.append((current_start, current_end))\n",
    "\n",
    "        total_len = 0\n",
    "        last_end = 0\n",
    "        for start, end in merged:\n",
    "            if last_end < start:\n",
    "                total_len += 1\n",
    "            total_len += end - start\n",
    "            last_end = end\n",
    "        if last_end < seq_len:\n",
    "            total_len += 1\n",
    "\n",
    "        compressed_seq = torch.zeros(batch_size, total_len, dtype=torch.long, device=seq.device)\n",
    "        chunk_map = [[] for _ in range(batch_size)]\n",
    "        mask_indices = []\n",
    "        chunk_id = self.num_items + 1\n",
    "        pos = 0\n",
    "\n",
    "        last_end = 0\n",
    "        for start, end in merged:\n",
    "            if last_end < start:\n",
    "                compressed_seq[:, pos] = chunk_id\n",
    "                for b in range(batch_size):\n",
    "                    chunk_map[b].append((last_end, start))\n",
    "                chunk_id += 1\n",
    "                pos += 1\n",
    "            window_len = end - start\n",
    "            compressed_seq[:, pos:pos + window_len] = seq[:, start:end].clone()\n",
    "            for mp in mask_positions:\n",
    "                if start <= mp < end and pos + (mp - start) not in mask_indices:\n",
    "                    mask_indices.append(pos + (mp - start))\n",
    "            for b in range(batch_size):\n",
    "                chunk_map[b].extend(list(range(start, end)))\n",
    "            pos += window_len\n",
    "            last_end = end\n",
    "\n",
    "        if last_end < seq_len:\n",
    "            compressed_seq[:, pos] = chunk_id\n",
    "            for b in range(batch_size):\n",
    "                chunk_map[b].append((last_end, seq_len))\n",
    "\n",
    "        return compressed_seq, chunk_map, torch.tensor(mask_indices, device=seq.device)\n",
    "\n",
    "    def forward(self, seq, mask_positions, is_predict=False):\n",
    "        batch_size, seq_len = seq.shape\n",
    "        masked_seq = seq.clone()\n",
    "        masked_seq[torch.arange(batch_size).unsqueeze(1), mask_positions] = self.num_items\n",
    "\n",
    "        compressed_seq, chunk_map, mask_indices = self.compress_sequence(masked_seq, mask_positions)\n",
    "        batch_size, comp_len = compressed_seq.shape\n",
    "        embeddings = self.embedding(compressed_seq)\n",
    "\n",
    "        is_chunk = (compressed_seq > self.num_items)\n",
    "        chunk_indices = torch.where(is_chunk)\n",
    "        if chunk_indices[0].numel() > 0:\n",
    "            chunk_embeddings = []\n",
    "            for b, i in zip(chunk_indices[0], chunk_indices[1]):\n",
    "                chunk_start, chunk_end = chunk_map[b][i]\n",
    "                chunk = seq[b, chunk_start:chunk_end].unsqueeze(0)\n",
    "                chunk_emb = self.embedding(chunk)\n",
    "                chunk_mask = (chunk != 0).float().unsqueeze(-1)\n",
    "                chunk_sum = (chunk_emb * chunk_mask).sum(dim=1)\n",
    "                chunk_count = chunk_mask.sum(dim=1).clamp(min=1)\n",
    "                chunk_embeddings.append(chunk_sum / chunk_count)\n",
    "            embeddings[chunk_indices] = torch.cat(chunk_embeddings)\n",
    "\n",
    "        embeddings = embeddings + self.pos_encoding[:comp_len].unsqueeze(0)\n",
    "        mask = (compressed_seq == 0).to(device)\n",
    "        output = self.transformer(embeddings, src_key_padding_mask=mask)\n",
    "\n",
    "        mask_output = output[torch.arange(batch_size).unsqueeze(1), mask_indices]\n",
    "        logits = self.fc(mask_output)  # Shape: (batch_size, num_masks, num_items)\n",
    "        \n",
    "        # For predict, ensure only the last mask is used\n",
    "        if is_predict and len(mask_indices) > 1:\n",
    "            mask_output = mask_output[:, -1:, :]  # Take only the last mask\n",
    "            logits = self.fc(mask_output)         # Shape: (batch_size, 1, num_items)\n",
    "        \n",
    "        return logits, mask_indices\n",
    "\n",
    "    def predict(self, input_seq):\n",
    "        seq = input_seq[:, :max_seq_length]\n",
    "        seq = torch.where(seq >= self.num_items, torch.zeros_like(seq), seq)\n",
    "        next_pos = seq.shape[1]\n",
    "        seq = torch.cat([seq, torch.full((seq.shape[0], 1), self.num_items, device=seq.device)], dim=1)\n",
    "        mask_positions = torch.tensor([next_pos], device=seq.device)\n",
    "        logits, _ = self.forward(seq, mask_positions, is_predict=True)\n",
    "        return logits\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, user_dict, num_items, max_seq_length, device):\n",
    "    model.eval()\n",
    "    NDCG, HR, valid_users = 0.0, 0.0, 0\n",
    "\n",
    "    for user, items in user_dict.items():\n",
    "        if len(items) < 2:\n",
    "            continue\n",
    "\n",
    "        seq = items[:max_seq_length]\n",
    "        input_seq = torch.tensor(seq[:-1], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        target = seq[-1]\n",
    "        candidates = [target] + random.sample(list(set(range(1, num_items)) - set(items)), num_negatives)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model.predict(input_seq)\n",
    "            scores = logits[0, 0, candidates]  # Single mask in predict\n",
    "            \n",
    "            ranked = torch.argsort(scores, descending=True).cpu().numpy()\n",
    "            rank = np.where(ranked == 0)[0][0] + 1\n",
    "\n",
    "        valid_users += 1\n",
    "        HR += int(rank <= 10)\n",
    "        NDCG += 1 / np.log2(rank + 1) if rank <= 10 else 0\n",
    "\n",
    "        # if valid_users % 100 == 0:\n",
    "        #     print(f\"Validated users: {valid_users}, HR@10: {HR / valid_users:.4f}, NDCG@10: {NDCG / valid_users:.4f}\")\n",
    "\n",
    "    # print(f\"Final HR@10: {HR / valid_users:.4f}, NDCG@10: {NDCG / valid_users:.4f}\")\n",
    "    return HR / valid_users, NDCG / valid_users\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "def load_movielens(file_path):\n",
    "    user_dict = defaultdict(list)\n",
    "    item_set = set()\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            user_id, item_id = map(int, line.strip().split())\n",
    "            user_dict[user_id].append(item_id)\n",
    "            item_set.add(item_id)\n",
    "    num_items = max(item_set)\n",
    "    return user_dict, num_items\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"data/ml-1m.txt\"\n",
    "    user_dict, num_items = load_movielens(file_path)\n",
    "    print(f\"Number of users: {len(user_dict)}, Number of items: {num_items}\")\n",
    "\n",
    "    dataset = MovieLensDataset(user_dict, num_items, max_seq_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "    model = SequentialRecommender(num_items, embedding_dim, num_heads, num_layers, dropout, window_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    print(f\"Length of dataloader: {len(dataloader)}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            seq = batch[\"seq\"].to(device)\n",
    "            batch_size = seq.shape[0]\n",
    "            valid_positions = (seq != 0).sum(dim=0).nonzero(as_tuple=True)[0]\n",
    "            if len(valid_positions) < num_masks_per_batch:\n",
    "                continue\n",
    "            mask_positions = random.sample(valid_positions.tolist(), num_masks_per_batch)\n",
    "            mask_positions = torch.tensor(mask_positions, device=device)\n",
    "\n",
    "            logits, mask_indices = model(seq, mask_positions)  # Shape: (batch_size, num_masks, num_items)\n",
    "            if logits.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            targets = seq[torch.arange(batch_size).unsqueeze(1), mask_positions]  # Shape: (batch_size, num_masks_per_batch)\n",
    "            valid_mask = (targets != num_items) & (targets != 0)  # Shape: (batch_size, num_masks_per_batch)\n",
    "\n",
    "            num_masks = mask_indices.shape[0]\n",
    "            logits = logits.view(batch_size * num_masks, num_items)  # Shape: (batch_size * num_masks, num_items)\n",
    "            targets = targets.view(batch_size * num_masks_per_batch)  # Shape: (batch_size * num_masks_per_batch)\n",
    "            valid_mask = valid_mask.view(batch_size * num_masks_per_batch)\n",
    "\n",
    "            targets = targets[valid_mask]\n",
    "            logits = logits[:targets.shape[0]]  # Align with filtered targets\n",
    "\n",
    "            if logits.shape[0] != targets.shape[0] or targets.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            loss = criterion(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # if batch_idx % 10 == 0:\n",
    "            #     print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "        if epoch % 10 == 0:\n",
    "            # Evaluate every 10 epochs\n",
    "            print(\"Evaluating...\")\n",
    "            HR, NDCG = evaluate(model, user_dict, num_items, max_seq_length, device)\n",
    "            print(f\"Epoch {epoch+1}, HR@10: {HR:.4f}, NDCG@10: {NDCG:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580298b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
