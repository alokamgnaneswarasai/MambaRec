{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/gnaneswaras/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486\n",
      "Epoch 1, Loss: 7.3614\n",
      "HR@10: 0.5469, NDCG@10: 0.3464\n",
      "Epoch 2, Loss: 6.6449\n",
      "HR@10: 0.6531, NDCG@10: 0.4320\n",
      "Epoch 3, Loss: 6.4286\n",
      "HR@10: 0.6649, NDCG@10: 0.4446\n",
      "Epoch 4, Loss: 6.3404\n",
      "HR@10: 0.6702, NDCG@10: 0.4424\n",
      "Epoch 5, Loss: 6.2867\n",
      "HR@10: 0.6791, NDCG@10: 0.4563\n",
      "Epoch 6, Loss: 6.2499\n",
      "HR@10: 0.6896, NDCG@10: 0.4600\n",
      "Epoch 7, Loss: 6.2215\n",
      "HR@10: 0.6747, NDCG@10: 0.4551\n",
      "Epoch 8, Loss: 6.1997\n",
      "HR@10: 0.6863, NDCG@10: 0.4592\n",
      "Epoch 9, Loss: 6.1819\n",
      "HR@10: 0.6887, NDCG@10: 0.4611\n",
      "Epoch 10, Loss: 6.1665\n",
      "HR@10: 0.6982, NDCG@10: 0.4745\n",
      "Epoch 11, Loss: 6.1539\n",
      "HR@10: 0.6798, NDCG@10: 0.4537\n",
      "Epoch 12, Loss: 6.1423\n",
      "HR@10: 0.6927, NDCG@10: 0.4657\n",
      "Epoch 13, Loss: 6.1323\n",
      "HR@10: 0.6929, NDCG@10: 0.4650\n",
      "Epoch 14, Loss: 6.1234\n",
      "HR@10: 0.6935, NDCG@10: 0.4673\n",
      "Epoch 15, Loss: 6.1156\n",
      "HR@10: 0.6859, NDCG@10: 0.4574\n",
      "Epoch 16, Loss: 6.1083\n",
      "HR@10: 0.6980, NDCG@10: 0.4616\n",
      "Epoch 17, Loss: 6.1021\n",
      "HR@10: 0.6993, NDCG@10: 0.4638\n",
      "Epoch 18, Loss: 6.0965\n",
      "HR@10: 0.6965, NDCG@10: 0.4632\n",
      "Epoch 19, Loss: 6.0915\n",
      "HR@10: 0.6932, NDCG@10: 0.4595\n",
      "Epoch 20, Loss: 6.0860\n",
      "HR@10: 0.6982, NDCG@10: 0.4637\n",
      "Epoch 21, Loss: 6.0818\n",
      "HR@10: 0.7018, NDCG@10: 0.4702\n",
      "Epoch 22, Loss: 6.0770\n",
      "HR@10: 0.6869, NDCG@10: 0.4506\n",
      "Epoch 23, Loss: 6.0733\n",
      "HR@10: 0.6927, NDCG@10: 0.4557\n",
      "Epoch 24, Loss: 6.0699\n",
      "HR@10: 0.6886, NDCG@10: 0.4569\n",
      "Epoch 25, Loss: 6.0665\n",
      "HR@10: 0.6964, NDCG@10: 0.4604\n",
      "Epoch 26, Loss: 6.0632\n",
      "HR@10: 0.6965, NDCG@10: 0.4689\n",
      "Epoch 27, Loss: 6.0600\n",
      "HR@10: 0.6932, NDCG@10: 0.4547\n",
      "Epoch 28, Loss: 6.0568\n",
      "HR@10: 0.6954, NDCG@10: 0.4624\n",
      "Epoch 29, Loss: 6.0544\n",
      "HR@10: 0.6846, NDCG@10: 0.4504\n",
      "Epoch 30, Loss: 6.0517\n",
      "HR@10: 0.6904, NDCG@10: 0.4550\n",
      "Epoch 31, Loss: 6.0499\n",
      "HR@10: 0.6838, NDCG@10: 0.4480\n",
      "Epoch 32, Loss: 6.0467\n",
      "HR@10: 0.6755, NDCG@10: 0.4408\n",
      "Epoch 33, Loss: 6.0450\n",
      "HR@10: 0.6881, NDCG@10: 0.4506\n",
      "Epoch 34, Loss: 6.0428\n",
      "HR@10: 0.6810, NDCG@10: 0.4452\n",
      "Epoch 35, Loss: 6.0404\n",
      "HR@10: 0.6672, NDCG@10: 0.4289\n",
      "Epoch 36, Loss: 6.0390\n",
      "HR@10: 0.6710, NDCG@10: 0.4359\n",
      "Epoch 37, Loss: 6.0370\n",
      "HR@10: 0.6770, NDCG@10: 0.4449\n",
      "Epoch 38, Loss: 6.0349\n",
      "HR@10: 0.6702, NDCG@10: 0.4313\n",
      "Epoch 39, Loss: 6.0332\n",
      "HR@10: 0.6742, NDCG@10: 0.4332\n",
      "Epoch 40, Loss: 6.0317\n",
      "HR@10: 0.6573, NDCG@10: 0.4197\n",
      "Epoch 41, Loss: 6.0294\n",
      "HR@10: 0.6647, NDCG@10: 0.4280\n",
      "Epoch 42, Loss: 6.0283\n",
      "HR@10: 0.6649, NDCG@10: 0.4337\n",
      "Epoch 43, Loss: 6.0263\n",
      "HR@10: 0.6526, NDCG@10: 0.4192\n",
      "Epoch 44, Loss: 6.0249\n",
      "HR@10: 0.6632, NDCG@10: 0.4258\n",
      "Epoch 45, Loss: 6.0242\n",
      "HR@10: 0.6409, NDCG@10: 0.4066\n",
      "Epoch 46, Loss: 6.0223\n",
      "HR@10: 0.6589, NDCG@10: 0.4227\n",
      "Epoch 47, Loss: 6.0212\n",
      "HR@10: 0.6818, NDCG@10: 0.4404\n",
      "Epoch 48, Loss: 6.0199\n",
      "HR@10: 0.6667, NDCG@10: 0.4313\n",
      "Epoch 49, Loss: 6.0182\n",
      "HR@10: 0.6791, NDCG@10: 0.4396\n",
      "Epoch 50, Loss: 6.0166\n",
      "HR@10: 0.6884, NDCG@10: 0.4492\n",
      "Epoch 51, Loss: 6.0162\n",
      "HR@10: 0.6781, NDCG@10: 0.4422\n",
      "Epoch 52, Loss: 6.0143\n",
      "HR@10: 0.6750, NDCG@10: 0.4367\n",
      "Epoch 53, Loss: 6.0135\n",
      "HR@10: 0.6730, NDCG@10: 0.4397\n",
      "Epoch 54, Loss: 6.0126\n",
      "HR@10: 0.6690, NDCG@10: 0.4301\n",
      "Epoch 55, Loss: 6.0116\n",
      "HR@10: 0.6702, NDCG@10: 0.4372\n",
      "Epoch 56, Loss: 6.0102\n",
      "HR@10: 0.6550, NDCG@10: 0.4177\n",
      "Epoch 57, Loss: 6.0092\n",
      "HR@10: 0.6772, NDCG@10: 0.4396\n",
      "Epoch 58, Loss: 6.0079\n",
      "HR@10: 0.6914, NDCG@10: 0.4504\n",
      "Epoch 59, Loss: 6.0071\n",
      "HR@10: 0.6712, NDCG@10: 0.4327\n",
      "Epoch 60, Loss: 6.0066\n",
      "HR@10: 0.6921, NDCG@10: 0.4465\n",
      "Epoch 61, Loss: 6.0051\n",
      "HR@10: 0.6916, NDCG@10: 0.4469\n",
      "Epoch 62, Loss: 6.0047\n",
      "HR@10: 0.6992, NDCG@10: 0.4605\n",
      "Epoch 63, Loss: 6.0034\n",
      "HR@10: 0.6856, NDCG@10: 0.4453\n",
      "Epoch 64, Loss: 6.0026\n",
      "HR@10: 0.6917, NDCG@10: 0.4489\n",
      "Epoch 65, Loss: 6.0016\n",
      "HR@10: 0.6912, NDCG@10: 0.4489\n",
      "Epoch 66, Loss: 6.0006\n",
      "HR@10: 0.6854, NDCG@10: 0.4458\n",
      "Epoch 67, Loss: 6.0003\n",
      "HR@10: 0.6591, NDCG@10: 0.4264\n",
      "Epoch 68, Loss: 5.9994\n",
      "HR@10: 0.6791, NDCG@10: 0.4428\n",
      "Epoch 69, Loss: 5.9987\n",
      "HR@10: 0.6790, NDCG@10: 0.4417\n",
      "Epoch 70, Loss: 5.9973\n",
      "HR@10: 0.6750, NDCG@10: 0.4402\n",
      "Epoch 71, Loss: 5.9967\n",
      "HR@10: 0.6960, NDCG@10: 0.4546\n",
      "Epoch 72, Loss: 5.9960\n",
      "HR@10: 0.6954, NDCG@10: 0.4550\n",
      "Epoch 73, Loss: 5.9952\n",
      "HR@10: 0.6947, NDCG@10: 0.4532\n",
      "Epoch 74, Loss: 5.9945\n",
      "HR@10: 0.6818, NDCG@10: 0.4438\n",
      "Epoch 75, Loss: 5.9938\n",
      "HR@10: 0.6906, NDCG@10: 0.4513\n",
      "Epoch 76, Loss: 5.9933\n",
      "HR@10: 0.6982, NDCG@10: 0.4556\n",
      "Epoch 77, Loss: 5.9926\n",
      "HR@10: 0.7028, NDCG@10: 0.4643\n",
      "Epoch 78, Loss: 5.9918\n",
      "HR@10: 0.6772, NDCG@10: 0.4403\n",
      "Epoch 79, Loss: 5.9910\n",
      "HR@10: 0.6972, NDCG@10: 0.4542\n",
      "Epoch 80, Loss: 5.9900\n",
      "HR@10: 0.6947, NDCG@10: 0.4521\n",
      "Epoch 81, Loss: 5.9901\n",
      "HR@10: 0.6892, NDCG@10: 0.4467\n",
      "Epoch 82, Loss: 5.9894\n",
      "HR@10: 0.6608, NDCG@10: 0.4268\n",
      "Epoch 83, Loss: 5.9889\n",
      "HR@10: 0.6886, NDCG@10: 0.4486\n",
      "Epoch 84, Loss: 5.9881\n",
      "HR@10: 0.6921, NDCG@10: 0.4528\n",
      "Epoch 85, Loss: 5.9875\n",
      "HR@10: 0.6839, NDCG@10: 0.4487\n",
      "Epoch 86, Loss: 5.9866\n",
      "HR@10: 0.7033, NDCG@10: 0.4694\n",
      "Epoch 87, Loss: 5.9858\n",
      "HR@10: 0.7106, NDCG@10: 0.4714\n",
      "Epoch 88, Loss: 5.9852\n",
      "HR@10: 0.7224, NDCG@10: 0.4798\n",
      "Epoch 89, Loss: 5.9852\n",
      "HR@10: 0.6975, NDCG@10: 0.4649\n",
      "Epoch 90, Loss: 5.9842\n",
      "HR@10: 0.7041, NDCG@10: 0.4659\n",
      "Epoch 91, Loss: 5.9838\n",
      "HR@10: 0.7111, NDCG@10: 0.4693\n",
      "Epoch 92, Loss: 5.9831\n",
      "HR@10: 0.7096, NDCG@10: 0.4632\n",
      "Epoch 93, Loss: 5.9827\n",
      "HR@10: 0.7098, NDCG@10: 0.4713\n",
      "Epoch 94, Loss: 5.9821\n",
      "HR@10: 0.7056, NDCG@10: 0.4700\n",
      "Epoch 95, Loss: 5.9817\n",
      "HR@10: 0.7194, NDCG@10: 0.4747\n",
      "Epoch 96, Loss: 5.9814\n",
      "HR@10: 0.7200, NDCG@10: 0.4776\n",
      "Epoch 97, Loss: 5.9805\n",
      "HR@10: 0.7028, NDCG@10: 0.4625\n",
      "Epoch 98, Loss: 5.9801\n",
      "HR@10: 0.7276, NDCG@10: 0.4852\n",
      "Epoch 99, Loss: 5.9796\n",
      "HR@10: 0.7116, NDCG@10: 0.4693\n",
      "Epoch 100, Loss: 5.9793\n",
      "HR@10: 0.7084, NDCG@10: 0.4716\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_SEQ_LENGTH = 200\n",
    "EMBEDDING_DIM = 64\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2\n",
    "BATCH_SIZE = 2048\n",
    "LR = 0.001\n",
    "EPOCHS = 100\n",
    "NEG_SAMPLES = 99\n",
    "\n",
    "# Load data\n",
    "def load_data(file_path):\n",
    "    user_dict = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            user, item = map(int, line.strip().split())\n",
    "            if user not in user_dict:\n",
    "                user_dict[user] = []\n",
    "            user_dict[user].append(item)\n",
    "    return user_dict\n",
    "\n",
    "def prepare_sequences(user_dict, max_seq_length):\n",
    "    sequences, targets = [], []\n",
    "    for user, items in user_dict.items():\n",
    "        for i in range(1, len(items)):\n",
    "            seq = items[:i][-max_seq_length:]\n",
    "            pad_length = max_seq_length - len(seq)\n",
    "            sequences.append([0] * pad_length + seq)  # Left padding\n",
    "            targets.append(items[i])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "class RecDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "class TransformerRec(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim, num_heads, num_layers, max_seq_length):\n",
    "        super().__init__()\n",
    "        self.item_embedding = nn.Embedding(num_items + 1, embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_length, embedding_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim, num_items)\n",
    "    \n",
    "    def forward(self, sequences):\n",
    "        seq_len = sequences.size(1)\n",
    "        item_embeds = self.item_embedding(sequences)\n",
    "        pos_ids = torch.arange(seq_len, device=sequences.device).unsqueeze(0)\n",
    "        pos_embeds = self.pos_embedding(pos_ids)\n",
    "        \n",
    "        encoded = self.encoder(item_embeds + pos_embeds)\n",
    "        output = self.fc(encoded[:, -1, :])  # Predict next item\n",
    "        return output\n",
    "\n",
    "def train_model(model, train_loader, num_items, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    print(len(train_loader))\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        for sequences, targets in train_loader:\n",
    "            sequences, targets = sequences.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(sequences)\n",
    "            # print(logits.shape, targets.shape)\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        evaluate(model, user_dict, num_items, MAX_SEQ_LENGTH, device)\n",
    "\n",
    "def evaluate(model, user_dict, num_items, max_seq_length, device):\n",
    "    model.eval()\n",
    "    NDCG, HR = 0.0, 0.0\n",
    "    valid_users = 0\n",
    "    \n",
    "    for user, items in user_dict.items():\n",
    "        if len(items) < 2:\n",
    "            continue\n",
    "        \n",
    "        seq = items[:-1][-max_seq_length:]\n",
    "        pad_length = max_seq_length - len(seq)\n",
    "        # print(pad_length)\n",
    "        # print(min(seq))\n",
    "        input_seq = torch.tensor([0] * pad_length + seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        target = items[-1]\n",
    "        negative_samples = random.sample(set(range(1, num_items)) - set(items), NEG_SAMPLES)\n",
    "        candidates = [target] + negative_samples\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_seq)\n",
    "            scores = logits.squeeze(0)[candidates]\n",
    "        \n",
    "        # print(scores.shape)\n",
    "        ranked = np.argsort(-scores.cpu().numpy())\n",
    "        rank = np.where(ranked == 0)[0][0] + 1  # Rank of positive sample\n",
    "        \n",
    "        # print(rank)\n",
    "        valid_users += 1\n",
    "        HR += int(rank <= 10)\n",
    "        NDCG += 1 / np.log2(rank + 1) if rank <= 10 else 0\n",
    "        \n",
    "    \n",
    "    print(f\"HR@10: {HR / valid_users:.4f}, NDCG@10: {NDCG / valid_users:.4f}\")\n",
    "    \n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda:3\" if  torch.cuda.is_available() else \"cpu\")\n",
    "    user_dict = load_data(\"data/ml-1m.txt\")\n",
    "    num_items = max(max(items) for items in user_dict.values())+1\n",
    "    \n",
    "    sequences, targets = prepare_sequences(user_dict, MAX_SEQ_LENGTH)\n",
    "    train_dataset = RecDataset(sequences, targets)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    model = TransformerRec(num_items, EMBEDDING_DIM, NUM_HEADS, NUM_LAYERS, MAX_SEQ_LENGTH).to(device)\n",
    "    train_model(model, train_loader, num_items, device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@10: 0.7149, NDCG@10: 0.4689\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, user_dict, num_items, max_seq_length, device):\n",
    "    model.eval()\n",
    "    NDCG, HR = 0.0, 0.0\n",
    "    valid_users = 0\n",
    "    \n",
    "    for user, items in user_dict.items():\n",
    "        if len(items) < 2:\n",
    "            continue\n",
    "        \n",
    "        seq = items[:-1][-max_seq_length:]\n",
    "        pad_length = max_seq_length - len(seq)\n",
    "        # print(pad_length)\n",
    "        # print(min(seq))\n",
    "        input_seq = torch.tensor([0] * pad_length + seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        target = items[-1]\n",
    "        negative_samples = random.sample(set(range(1, num_items)) - set(items), NEG_SAMPLES)\n",
    "        candidates = [target] + negative_samples\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_seq)\n",
    "            scores = logits.squeeze(0)[candidates]\n",
    "        \n",
    "        # print(scores.shape)\n",
    "        ranked = np.argsort(-scores.cpu().numpy())\n",
    "        rank = np.where(ranked == 0)[0][0] + 1  # Rank of positive sample\n",
    "        \n",
    "        # print(rank)\n",
    "        valid_users += 1\n",
    "        HR += int(rank <= 10)\n",
    "        NDCG += 1 / np.log2(rank + 1) if rank <= 10 else 0\n",
    "    \n",
    "    print(f\"HR@10: {HR / valid_users:.4f}, NDCG@10: {NDCG / valid_users:.4f}\")\n",
    "\n",
    "evaluate(model, user_dict, num_items, MAX_SEQ_LENGTH, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n"
     ]
    }
   ],
   "source": [
    "print(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
